{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15945d26",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b6eab989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tqdm.notebook as tq\n",
    "import utils\n",
    "from tkinter import Tcl # file sorting by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7763c",
   "metadata": {},
   "source": [
    "# Creation of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8ded2",
   "metadata": {},
   "source": [
    "### Dictionary creation for the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c4013",
   "metadata": {},
   "source": [
    "We want a dictionary indicating a numbeer for each genre:\n",
    "\n",
    "{0: 'Hip-Hop', 1: 'Pop', 2: 'Folk', 3: 'Rock', 4: 'Experimental', 5: 'International', 6: 'Electronic', 7: 'Instrumental'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e985c",
   "metadata": {},
   "source": [
    "### Creation of the labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be9951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_folder: data/fma_small_stft_transposed/train\n",
      "validation_folder: data/fma_small_stft_transposed/validation\n",
      "test_folder: data/fma_small_stft_transposed/test \n",
      "\n",
      "audio directory:  ./data/fma_small/\n",
      "Loading tracks.csv...\n",
      "small dataset shape: (8000, 52)\n",
      "Track.csv: 6400 training samples, 800 validation samples, 800 test samples\n",
      "\n",
      "there are 8 unique genres\n",
      "Dictionary of genres created: {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Rock': 3, 'Experimental': 4, 'International': 5, 'Electronic': 6, 'Instrumental': 7}\n",
      "labels length: 63970\n",
      "labels length: 8000\n",
      "labels length: 8000\n"
     ]
    }
   ],
   "source": [
    "def create_single_dataset(folder_path, tracks_dataframe, genre_dictionary):    \n",
    "    labels = []\n",
    "   \n",
    "    _, file_list = get_sorted_file_paths(folder_path)\n",
    "    \n",
    "    for i,file in enumerate(file_list):\n",
    "        #print(\"considering file:\",file, \"({}/{})\".format(i,len(file_list)))\n",
    "        track_id_clip_id = file.split('.')[0]\n",
    "        track_id = track_id_clip_id.split('_')[0]\n",
    "        #print(\"track id with clip: {}, track id: {}\".format(track_id_clip_id, track_id))\n",
    "        genre = tracks_dataframe.loc[int(track_id)]\n",
    "        #print(\"genre from dataframe: \", genre)\n",
    "        label = genre_dictionary[genre]\n",
    "        #print(\"label from dictionary:\",label)\n",
    "        labels.append(label)\n",
    "    print(\"labels length: {}\".format(len(labels)))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "#create the train,validation and test vectors using the files in the train/validation/test folders\n",
    "def create_dataset_splitted(folder_path):\n",
    "    train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "    validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "    test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "    \n",
    "    print(\"train_folder:\",train_folder)\n",
    "    print(\"validation_folder:\",validation_folder)\n",
    "    print(\"test_folder:\",test_folder,\"\\n\")\n",
    "    \n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "    \n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "\n",
    "    small_training_top_genres = small_training['genre_top']\n",
    "    small_validation_top_genres = small_validation['genre_top']\n",
    "    small_test_top_genres = small_test['genre_top']\n",
    "    \n",
    "    #create dictionary of genre classes:\n",
    "    unique_genres = small_training_top_genres.unique()\n",
    "    unique_genres = np.array(unique_genres)\n",
    "    print(\"there are {} unique genres\".format(len(unique_genres)))\n",
    "    genre_dictionary = {}\n",
    "    for i,genre in enumerate(unique_genres):\n",
    "        genre_dictionary[genre] = i\n",
    "    print(\"Dictionary of genres created:\",genre_dictionary)\n",
    "    \n",
    "    \n",
    "    Y_train = create_single_dataset(train_folder, small_training_top_genres, genre_dictionary)\n",
    "    Y_validation = create_single_dataset(validation_folder, small_validation_top_genres, genre_dictionary)\n",
    "    Y_test = create_single_dataset(test_folder, small_test_top_genres, genre_dictionary)\n",
    "    \n",
    "    return Y_train, Y_validation, Y_test\n",
    " \n",
    "def get_sorted_file_paths(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #sort the dataset files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_list = Tcl().call('lsort', '-dict', file_list) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...\n",
    "    file_paths = [os.path.join(folder_path, file_name) for file_name in file_list] #join filename with folder path\n",
    "    #print(\"There are {} in the folder: {}\".format(len(file_list),file_list))\n",
    "    return file_paths, file_list\n",
    "    \n",
    "    \n",
    "folder_path=\"data/fma_small_stft_transposed\"\n",
    "Y_train, Y_validation, Y_test = create_dataset_splitted(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20175a8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "187ddbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list, labels):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae70891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  63970\n",
      "len of validation dataset:  8000\n",
      "len of test dataset:  8000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_stft_transposed\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDataset(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef67f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_dimension_errors(filepaths):\n",
    "    error_indexes = []\n",
    "    progress = 0\n",
    "    for file in filepaths:\n",
    "        progress+=1\n",
    "        print(\"checked {}/{} files\".format(progress,len(filepaths)))\n",
    "        x = np.load(file)\n",
    "        if(x.shape != (128,513)):\n",
    "            error_indexes.append(x)\n",
    "            print(\"error\")\n",
    "    print(\"{} errors found in files: {}\".format(len(error_indexes),error_indexes))\n",
    "    for idx,error in enumerate(error_indexes):\n",
    "        print(\"index: {}, shape: {}\".format(idx,error.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a52dc",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f745b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513), stride=(1,513))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1), stride=(1,513))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1), stride=(1,513))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.dense2 = nn.Linear(300, 150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x=self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845a53e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d034d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=512\n",
    "EPOCHS=3\n",
    "LEARNING_RATE=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd576a3",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "223742e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, validation_dataset, Y_validation):\n",
    "    #Stop parameters learning\n",
    "    model.eval()\n",
    "    \n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            \n",
    "            inputs=inputs.unsqueeze(1)\n",
    "            #predict label\n",
    "            outputs = model(inputs)\n",
    "            #compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "\n",
    "    accuracy = correct / total\n",
    "    average_loss = total_loss / len(validation_loader)\n",
    "\n",
    "    return accuracy, average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a0ca4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, num_epochs, learning_rate, verbose = False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", content: \",inputs)\n",
    "                print(\"\\nlabels shape:\",labels.size(),\", content: \",labels)\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                if(labels[index].item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "            \n",
    "        val_acc, val_loss = test(model, validation_dataset, Y_validation)\n",
    "        loss_list.append(val_loss)\n",
    "        acc_list.append(val_acc)\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}. Accuracy: {average_accuracy}\")\n",
    "        progress_bar.close()\n",
    "    return [acc_list,loss_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc7adc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef544d9caf554b2f94db1a83d28b24b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.9132. Accuracy: 34.6453125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9631234cca6b4bb8b9ce4a118337ca9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NNet1()\n",
    "#summary(model, (1, 128, 513))\n",
    "performance=train(model, train_dataset, batch_size=BATCH_SIZE, num_epochs=EPOCHS, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d928b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,EPOCHS+1), performance[1][:], label='Loss') \n",
    "plt.plot(np.arange(1,EPOCHS+1), performance[0][:], label='Accuracy')\n",
    "plt.legend()  # Display the legend showing the labels\n",
    "plt.show()\n",
    "print(performance[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc08332",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom model class\n",
    "class NNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=(4, 513),padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=(4, 1),padding=\"same\")\n",
    "        self.conv3 = nn.Conv2d(256, 256, kernel_size=(4, 1),padding=\"same\")\n",
    "        self.dense1 = nn.Linear(256*125*1, 300)\n",
    "        self.dense2 = nn.Linear(300, 150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x1 = x  # Save the output of the first convolutional layer for later\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.ReLU()(self.conv3(x))\n",
    "        x += x1  # Sum the output of the first convolutional layer with the third convolutional layer\n",
    "        \n",
    "        x = torch.cat((nn.AvgPool2d(kernel_size=(125, 1))(x), nn.MaxPool2d(kernel_size=(125, 1))(x)), dim=1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.ReLU()(self.dense1(x))\n",
    "        x = nn.ReLU()(self.dense2(x))\n",
    "        x = nn.Softmax(dim=1)(self.dense3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda60597",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = NNet2()\n",
    "#summary(model2, (1, 128, 513))\n",
    "train(model2, train_dataset, batch_size=32, num_epochs=50, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c443ec",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGenreClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(AudioGenreClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Time-distributed layer\n",
    "        self.time_distributed = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        self.bilstm = nn.LSTM(256, hidden_size, num_layers = num_layers, bidirectional=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Time-distributed layer\n",
    "        x = self.time_distributed(x)\n",
    "\n",
    "        # Bidirectional LSTM layers\n",
    "        output, _ = self.bilstm(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.softmax(self.attention(output), dim=1)\n",
    "        attended_output = torch.sum(attention_weights * output, dim=1)\n",
    "\n",
    "        # Pooling layer\n",
    "        pooled_output = self.pooling(attended_output.permute(0, 1).unsqueeze(2))\n",
    "        \n",
    "        # Reshape and pass through the output layer\n",
    "        pooled_output = pooled_output.squeeze(2)\n",
    "        output = self.fc(pooled_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRNN(model, dataset, batch_size, num_epochs, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \",device)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            #print(\"inputs shape:\",inputs.shape,\", content: \",inputs)\n",
    "            #print(\"labels shape:\",labels.shape,\", content: \",labels)\n",
    "            #inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            #print(\"\\noutputs type:\",type(outputs),\"content:\",outputs)\n",
    "            #print(\"\\nlabels type:\",type(labels),\"content:\",labels)\n",
    "\n",
    "            loss = criterion(outputs, labels.float()) #labels need to be a vector of float, not Long\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #print(\"outputs shape:\",outputs.size(),\"content:\",outputs)\n",
    "            #print(\"labels shape:\",labels.size(),\"content:\",labels)\n",
    "\n",
    "            #calculate train accuracy\n",
    "            for idx, predicted_label in enumerate(outputs):\n",
    "                #print(\"predicted_label size:\",predicted_label.size(),\"content:\",predicted_label)\n",
    "                #print(\"labels[idx] size:\",labels[idx].size(),\"content:\",labels[idx])\n",
    "                max_idx = torch.argmax(predicted_label).item() #index with max argument in the one hot predicted label vector\n",
    "                #print(\"max_idx content:\",max_idx)\n",
    "\n",
    "                if(labels[idx][max_idx].item() == 1):\n",
    "                    correct += 1\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            #print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"Batch accuracy\": accuracy, \"Average accuracy\": average_accuracy})\n",
    "\n",
    "\n",
    "\n",
    "            #bps = (batch_idx + 1) / (time.time() - start_time)\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "       \n",
    "\n",
    "\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}. Accuracy: {average_accuracy}\")\n",
    "        progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRNN = AudioGenreClassifier(input_size=513, hidden_size=256, num_layers=5, num_classes=8)\n",
    "trainRNN(modelRNN, train_dataset, batch_size=32, num_epochs=8, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59461c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
