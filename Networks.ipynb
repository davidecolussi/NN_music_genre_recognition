{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15945d26",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6eab989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tqdm.notebook as tq\n",
    "import utils\n",
    "from pydub import AudioSegment\n",
    "from tkinter import Tcl # file sorting by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c3e14",
   "metadata": {},
   "source": [
    "# Load STFT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8ded2",
   "metadata": {},
   "source": [
    "### Dictionary creation for the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c4013",
   "metadata": {},
   "source": [
    "We want a dictionary indicating a numbeer for each genre:\n",
    "\n",
    "{0: 'Hip-Hop', 1: 'Pop', 2: 'Folk', 3: 'Rock', 4: 'Experimental', 5: 'International', 6: 'Electronic', 7: 'Instrumental'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e985c",
   "metadata": {},
   "source": [
    "### Creation of the labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be9951a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_folder: data/fma_small_stft_transposed_22050_overlapped/train\n",
      "validation_folder: data/fma_small_stft_transposed_22050_overlapped/validation\n",
      "test_folder: data/fma_small_stft_transposed_22050_overlapped/test \n",
      "\n",
      "audio directory:  ./data/fma_small/\n",
      "Loading tracks.csv...\n",
      "small dataset shape: (8000, 52)\n",
      "Track.csv: 6400 training samples, 800 validation samples, 800 test samples\n",
      "\n",
      "there are 8 unique genres\n",
      "Dictionary of genres created: {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Rock': 3, 'Experimental': 4, 'International': 5, 'Electronic': 6, 'Instrumental': 7}\n",
      "labels length: 127940\n",
      "labels length: 16000\n",
      "labels length: 16000\n"
     ]
    }
   ],
   "source": [
    "def create_single_dataset(folder_path, tracks_dataframe, genre_dictionary):    \n",
    "    labels = []\n",
    "   \n",
    "    _, file_list = get_sorted_file_paths(folder_path)\n",
    "    \n",
    "    for i,file in enumerate(file_list):\n",
    "        #print(\"considering file:\",file, \"({}/{})\".format(i,len(file_list)))\n",
    "        track_id_clip_id = file.split('.')[0]\n",
    "        track_id = track_id_clip_id.split('_')[0]\n",
    "        #print(\"track id with clip: {}, track id: {}\".format(track_id_clip_id, track_id))\n",
    "        genre = tracks_dataframe.loc[int(track_id)]\n",
    "        #print(\"genre from dataframe: \", genre)\n",
    "        label = genre_dictionary[genre]\n",
    "        #print(\"label from dictionary:\",label)\n",
    "        labels.append(label)\n",
    "    print(\"labels length: {}\".format(len(labels)))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "#create the train,validation and test vectors using the files in the train/validation/test folders\n",
    "def create_dataset_splitted(folder_path):\n",
    "    train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "    validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "    test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "    \n",
    "    print(\"train_folder:\",train_folder)\n",
    "    print(\"validation_folder:\",validation_folder)\n",
    "    print(\"test_folder:\",test_folder,\"\\n\")\n",
    "    \n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "    \n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "\n",
    "    small_training_top_genres = small_training['genre_top']\n",
    "    small_validation_top_genres = small_validation['genre_top']\n",
    "    small_test_top_genres = small_test['genre_top']\n",
    "    \n",
    "    #create dictionary of genre classes:\n",
    "    unique_genres = small_training_top_genres.unique()\n",
    "    unique_genres = np.array(unique_genres)\n",
    "    print(\"there are {} unique genres\".format(len(unique_genres)))\n",
    "    genre_dictionary = {}\n",
    "    for i,genre in enumerate(unique_genres):\n",
    "        genre_dictionary[genre] = i\n",
    "    print(\"Dictionary of genres created:\",genre_dictionary)\n",
    "    \n",
    "    \n",
    "    Y_train = create_single_dataset(train_folder, small_training_top_genres, genre_dictionary)\n",
    "    Y_validation = create_single_dataset(validation_folder, small_validation_top_genres, genre_dictionary)\n",
    "    Y_test = create_single_dataset(test_folder, small_test_top_genres, genre_dictionary)\n",
    "    \n",
    "    return Y_train, Y_validation, Y_test\n",
    " \n",
    "def get_sorted_file_paths(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #sort the dataset files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_list = Tcl().call('lsort', '-dict', file_list) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...\n",
    "    file_paths = [os.path.join(folder_path, file_name) for file_name in file_list] #join filename with folder path\n",
    "    #print(\"There are {} in the folder: {}\".format(len(file_list),file_list))\n",
    "    return file_paths, file_list\n",
    "    \n",
    "    \n",
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "Y_train, Y_validation, Y_test = create_dataset_splitted(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20175a8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fe67f",
   "metadata": {},
   "source": [
    "Class to load the STFT from files. Each file has a (128,513) matrix containing the STFT of a 3 seconds audio clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187ddbe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae70891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  127940\n",
      "len of validation dataset:  16000\n",
      "len of test dataset:  16000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDataset(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd9c68",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "We will use Z-Score to normalize the training, validation and test set by calculating the mean and the std deviation on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f571ada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_mean'\n",
    "std_save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_std_deviation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677263cf",
   "metadata": {},
   "source": [
    "## Calculation of mean and standard deviation (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5795abf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum: tensor(8.9296e+09)\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum=0\n",
    "\n",
    "#iter all the training set by batches and calculate the sum of all the sample values (513*128 values for each sample)\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum:\",current_sum)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "        current_sum += torch.sum(sample)\n",
    "        #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum:\",current_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c794bf2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of training set: tensor(1.0629)\n",
      "Saving the mean in file: ./data/fma_small_stft_transposed_22050_overlapped/train_mean\n"
     ]
    }
   ],
   "source": [
    "mean = current_sum/(len(train_dataset)*513*128) #divide the sum for the total number of values considerated\n",
    "print(\"mean of training set:\",mean)\n",
    "\n",
    "print(\"Saving the mean in file:\",save_filename) \n",
    "np.save(save_filename,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec52eb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum of squares: tensor(7.8299e+10)\n"
     ]
    }
   ],
   "source": [
    "#now let's calculate the standard deviation (squared root of the variance)\n",
    "\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum_of_squares = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum_of_squares:\",current_sum_of_squares)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample shape\",sample.shape)\n",
    "        for row in sample:\n",
    "            #print(\"row shape:\",row.shape)\n",
    "            for elem in row:\n",
    "                #print(\"elem: shape\",elem.shape,\"content:\",elem)\n",
    "                difference = elem - mean\n",
    "                difference_squared = difference**2\n",
    "                current_sum_of_squares += difference_squared\n",
    "                #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum of squares:\",current_sum_of_squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cecfb0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance: tensor(9.3202)\n",
      "std_deviation: 3.0528938510507846\n",
      "Saving the std_deviation in file: ./data/fma_small_stft_transposed_22050_overlapped/train_std_deviation\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "variance = current_sum_of_squares/((len(train_dataset) * 513 * 128)-1)\n",
    "std_deviation = math.sqrt(variance)\n",
    "\n",
    "print(\"variance:\",variance)\n",
    "print(\"std_deviation:\",std_deviation)\n",
    "\n",
    "print(\"Saving the std_deviation in file:\",std_save_filename) \n",
    "np.save(std_save_filename,std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fc2ab",
   "metadata": {},
   "source": [
    "## Load calculated mean and std deviation from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977c1ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean: 1.0629134\n",
      "loaded std: 3.0528938510507846\n"
     ]
    }
   ],
   "source": [
    "loaded_mean = np.load(save_filename+'.npy')\n",
    "print(\"loaded mean:\",loaded_mean)\n",
    "\n",
    "loaded_std = np.load(std_save_filename+'.npy')\n",
    "print(\"loaded std:\",loaded_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d4804",
   "metadata": {},
   "source": [
    "## Create the normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092efbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#verify that the normalization has worked\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\\ntotal_n_batches = len(train_loader)/batch_size\\n\\n#iter all the training set by batches and normalize the value of each tensor\\nfor batch_idx, batch in enumerate(train_loader):\\n    print(\"batch\",batch_idx,\"/\",total_n_batches)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\\n    print(\"labels:\",labels)\\n    for sample in inputs:\\n        print(\"sample: shape\",sample.shape,\"content:\",sample)\\n   \\nvalidation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\\ntotal_n_batches = len(validation_loader)/batch_size\\n\\n#iter all the training set by batches and normalize the value of each tensor\\nfor batch_idx, batch in enumerate(validation_loader):\\n    print(\"batch\",batch_idx,\"/\",total_n_batches)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\\n    print(\"labels:\",labels)\\n    for sample in inputs:\\n        print(\"sample: shape\",sample.shape,\"content:\",sample)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean= loaded_mean, std= loaded_std)\n",
    "])\n",
    "\n",
    "train_dataset = MyDataset(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test,  transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a52dc",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f745b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.bn4 = nn.BatchNorm1d(300)\n",
    "        self.dense2 = nn.Linear(300,150)\n",
    "        self.bn5 = nn.BatchNorm1d(150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8734822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet2, self).__init__()\n",
    "        self.drop=nn.Dropout(0.2)\n",
    "        # STFT spectrogram input: (batch_size, 1, 128, 513)\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513))\n",
    "        self.batch1=nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128,128, kernel_size=(4, 1),padding=1)\n",
    "        self.batch2=nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=(4, 1),padding=2)\n",
    "        self.batch3=nn.BatchNorm2d(128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(128,1))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(128,1))\n",
    "        self.fc1 = nn.Linear(250,128)\n",
    "        self.bn1=nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.bn2=nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 8)  # 8 classes for genre predictions\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x=self.batch1(x)\n",
    "        x = torch.relu(x)\n",
    "        y=x\n",
    "        x = self.conv2(x)\n",
    "        x=self.batch2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x=self.batch3(x)\n",
    "        x = torch.relu(x)\n",
    "        # Sum between the first and third conv layers\n",
    "        x = x[:, :, :, 0] + y[:, :, :, 0]\n",
    "        \n",
    "        x = torch.relu(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x=self.drop(x)\n",
    "        x=self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)        \n",
    "        x=self.drop(x)\n",
    "        x=self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845a53e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d034d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "LEARNING_RATE=0.0001\n",
    "\n",
    "learning_rate_list = [0.001, 0.0001]\n",
    "batch_size_list = [128,256,512]\n",
    "#reg_list=[0.01, 0.001, 0.0001,0.00001,0.000001]\n",
    "reg_list=[0.001, 0.0001,0.00001,0.000001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd576a3",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "223742e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model, validation_dataset, Y_validation):\n",
    "    # Stop parameters learning\n",
    "    model.eval()\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confusion_matrix = np.zeros((8, 8), dtype=int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample, label in validation_loader:\n",
    "            \n",
    "            sample = sample.unsqueeze(1)\n",
    "\n",
    "            # Predict label\n",
    "            output = model(sample)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            max_index = torch.argmax(output).item()  # The index with maximum probability\n",
    "\n",
    "            confusion_matrix[label][max_index] += 1\n",
    "\n",
    "            correct += (max_index == label)\n",
    "\n",
    "    #cm = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "    #cm.plot()\n",
    "    #print(confusion_matrix)\n",
    "    accuracy = 100 * correct / len(Y_validation)\n",
    "    average_loss = total_loss / len(Y_validation)\n",
    "\n",
    "    model.train()\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0ca4e98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, num_epochs, learning_rate, verbose = False, RGB=False,reg=1e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    counted_labels=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", dtype:\",inputs.dtype,\" content: \",inputs)\n",
    "                print(\"min value:\",torch.min(inputs))\n",
    "                print(\"max value:\",torch.max(inputs))\n",
    "                print(\"\\nlabels shape:\",labels.size(),\",dtype:\",labels.dtype,\", content: \",labels)\n",
    "            if(RGB==False):\n",
    "                inputs = inputs.unsqueeze(1) #add a dimension if input is to be considered just grayscale\n",
    "                #if input is RGB, there are already 3 channels\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if(verbose == True):\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "                print(\"List of labels until now:\",counted_labels)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                counted_labels[labels[index].item()]+=1\n",
    "                if(labels[index].item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "        \n",
    "        #calculate train loss and accuracy\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        train_loss_list.append(average_loss)\n",
    "        train_acc_list.append(average_accuracy)\n",
    "        \n",
    "        #calculate validation loss and accuracy\n",
    "        val_acc, val_loss = test(model, validation_dataset, Y_validation)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}],Train Loss: {average_loss:.4f}. Train Accuracy: {average_accuracy} Val Loss: {val_loss} Val Accuracy: {val_acc}\")\n",
    "        progress_bar.close()\n",
    "                            \n",
    "        return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef7fab",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1 + BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3685ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1_Small(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1_Small, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(2, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv2 = nn.Conv2d(64,128, kernel_size=(2, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv3 = nn.Conv2d(128,64, kernel_size=(4, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(2, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(256, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dense2 = nn.Linear(128,64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.dense3 = nn.Linear(64, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dense2(x)\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.bn5(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61b169d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 127, 1]          65,728\n",
      "       BatchNorm2d-2           [-1, 64, 127, 1]             128\n",
      "              ReLU-3           [-1, 64, 127, 1]               0\n",
      "         MaxPool2d-4            [-1, 64, 31, 1]               0\n",
      "            Conv2d-5           [-1, 128, 30, 1]          16,512\n",
      "       BatchNorm2d-6           [-1, 128, 30, 1]             256\n",
      "              ReLU-7           [-1, 128, 30, 1]               0\n",
      "         MaxPool2d-8            [-1, 128, 7, 1]               0\n",
      "            Conv2d-9             [-1, 64, 4, 1]          32,832\n",
      "      BatchNorm2d-10             [-1, 64, 4, 1]             128\n",
      "        AvgPool2d-11             [-1, 64, 2, 1]               0\n",
      "        MaxPool2d-12             [-1, 64, 2, 1]               0\n",
      "          Flatten-13                  [-1, 256]               0\n",
      "           Linear-14                   [-1, 64]          16,448\n",
      "          Dropout-15                   [-1, 64]               0\n",
      "      BatchNorm1d-16                   [-1, 64]             128\n",
      "             ReLU-17                   [-1, 64]               0\n",
      "           Linear-18                    [-1, 8]             520\n",
      "          Softmax-19                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 132,680\n",
      "Trainable params: 132,680\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 0.31\n",
      "Params size (MB): 0.51\n",
      "Estimated Total Size (MB): 1.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_NNet1_Small = NNet1_Small()\n",
    "summary(model_NNet1_Small, (1, 128, 513))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac93d69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7fca7761a44c39b3e4ea0409cb47e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae253e4449d45b6a52ca9d1bcaf267c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535e398606044685bb522c1740cca4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd719a10db046e98fb00e19d3abfd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c532dff2f5e64657a5be7f4926182448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeca89a3b7fc4cd7bb8aa42f59d6b5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cee2f1ff63243f690371eb041508488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0db9587d2d49df8d7fa8323eef8c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9f584bfb814c169b26655e2b1ee9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f594a78b44d48039f989d42ed87365b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained with learning rate= 0.001  and with regularization term= 0.001\n",
      "Loss: [1.8163834485039114, 1.8075053253173827, 1.8355079351514578, 1.8247116443812847, 1.8116622361838817, 1.8000316052734853, 1.790355814024806, 1.7835539383068681, 1.8119577863514424, 1.7961394810155034]\n",
      "Accuracy: [tensor([45.3563]), tensor([46.3875]), tensor([43.6375]), tensor([44.2687]), tensor([45.8875]), tensor([46.8625]), tensor([47.9625]), tensor([48.6875]), tensor([45.9250]), tensor([47.3750])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d11ba5ff7d48f89254c29305856f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f92bb43e96d4739b3d00e17dae31d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afd4aeb4089448db84a508f7679d0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09ceedb0e854261b3ea4818ddcc362c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9f9acaa9f94317b840118e5a997ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ace5433e99421daf3eb528fec32f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9e3b7912d145a495c9b26f39c0966d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b27f11843d14d3fb0bffb807c788a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98547d9462a485499972b5e5df1cab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc93ffce41a4248a4d42e3d2a03f389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained with learning rate= 0.001  and with regularization term= 0.0001\n",
      "Loss: [1.812225155711174, 1.8132132166102528, 1.811153077110648, 1.7980445506349205, 1.7656186630129813, 1.7702294467240571, 1.7902081690058111, 1.8171059103533627, 1.7804105890914799, 1.7852117345780134]\n",
      "Accuracy: [tensor([45.8187]), tensor([45.1750]), tensor([45.5438]), tensor([46.7875]), tensor([50.5625]), tensor([49.7000]), tensor([47.3438]), tensor([44.7438]), tensor([48.6813]), tensor([48.1562])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832a0a898a5a4191828489692dac220f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d9c7cfeb3545b395063374637392a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ee21e9dfe4434fb2d5703c0c0a7336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3029f8273924f5eb77a61798fca6a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62467fb4316c4c72a76ab57df205a122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d68757ce944e819f009fe656f0042d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e705ddf0e663445ba960e49bb5350e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433d61f6c4434ab4b1bc488228b00924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f7b38efe0240fdad74635204af1159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b890381f81f0424c8465799327c7e317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained with learning rate= 0.001  and with regularization term= 1e-05\n",
      "Loss: [1.8176241820454597, 1.782411430813372, 1.8017939607203006, 1.7755754545256495, 1.7865685252472758, 1.7916197644919156, 1.779641671948135, 1.7848620892614127, 1.7815787559822203, 1.7722470450922847]\n",
      "Accuracy: [tensor([45.4938]), tensor([48.7188]), tensor([46.4437]), tensor([49.3187]), tensor([48.1750]), tensor([47.6625]), tensor([48.8812]), tensor([48.4875]), tensor([48.7250]), tensor([49.4813])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f198f235c8e4071977ef396830e471a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-131017767fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreg_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmodel_NNet1_Small\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNet1_Small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_NNet1_Small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained with learning rate=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" and with regularization term=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f8744f99e831>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose, RGB, reg)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#labels need to be a vector of class indexes (0-7) of dim (batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO: Add batch_size optimization loop\n",
    "for i in learning_rate_list:\n",
    "    for j in reg_list:\n",
    "        model_NNet1_Small = NNet1_Small()\n",
    "        train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model_NNet1_Small, train_dataset, batch_size=128, num_epochs=10, learning_rate=i, reg=j)\n",
    "        print(\"Trained with learning rate=\",i,\" and with regularization term=\",j)\n",
    "        print(\"Loss:\",val_loss_list)\n",
    "        print(\"Accuracy:\",val_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343371e",
   "metadata": {},
   "source": [
    "# Raw audio (1D)\n",
    "\n",
    "In the following section we'll implement a Neural Network based on the classification of the raw audio. The raw audio is an array containing the amplitude of the audio wave for each sample of a 3 second clip (We used a sampling rate of 22050 Hz). Each entry has a size of (1,66150)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabb348",
   "metadata": {},
   "source": [
    "## Raw audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30092ba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyDatasetRaw(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose=False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        raw_vector = np.load(file_path).astype(np.int16) # Ensure int16 data type\n",
    "        if(self.verbose==True):\n",
    "            print(\"raw vector shape:\",raw_vector.shape)\n",
    "        raw_vector = torch.tensor(raw_vector)\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            #convert to float64 tensor\n",
    "            raw_vector = raw_vector.double()\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.unsqueeze(raw_vector, dim=0)\n",
    "            #print(\"TRANSFORM: after first unsqueeze:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.unsqueeze(raw_vector, dim=0) #unsqueeze two times (needed for torchvision normalize method)\n",
    "            #print(\"TRANSFORM: after second unsqueeze:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = self.transform(raw_vector) #normalize the sample\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.squeeze(raw_vector, dim=0)\n",
    "            raw_vector = torch.squeeze(raw_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after double squeeze shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "\n",
    "        \n",
    "        return raw_vector, label        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99825d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_file_paths_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-aa32c4b45983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile_paths_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_paths_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_paths_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file_paths_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_file_paths_split' is not defined"
     ]
    }
   ],
   "source": [
    "file_paths_train, file_paths_validation, file_paths_test = get_file_paths_split()\n",
    "print(len(file_paths_train))\n",
    "print(len(file_paths_validation))\n",
    "print(len(file_paths_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de69e8",
   "metadata": {},
   "source": [
    "# Normalization of raw audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d390039",
   "metadata": {},
   "source": [
    "We calculate mean and std deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_save_filename_raw = './data/fma_small_raw_array_22050_overlapped/train_mean'\n",
    "std_save_filename_raw = './data/fma_small_raw_array_22050_overlapped/train_std_deviation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b38ea",
   "metadata": {},
   "source": [
    "# Calculation of mean raw (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba507d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDatasetRaw(file_paths_train, Y_train)\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum=0\n",
    "\n",
    "#iter all the training set by batches and calculate the sum of all the sample values (513*128 values for each sample)\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if(batch_idx%1000==0):\n",
    "        print(\"batch\",batch_idx,\"/\",total_n_batches,\"(\",round((batch_idx/len(train_dataset)*100)),\"%), current_sum:\",current_sum)\n",
    "    \n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "        current_sum += torch.sum(sample)\n",
    "       \n",
    "        #print(\"type of current_sum:\",current_sum.dtype)\n",
    "        #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum:\",current_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"current_sum\",current_sum)\n",
    "mean_raw = current_sum/(len(train_dataset)*66150) #divide the sum for the total number of values considerated\n",
    "print(\"mean of training set:\",mean_raw)\n",
    "\n",
    "print(\"Saving the mean in file:\",mean_save_filename_raw) \n",
    "np.save(mean_save_filename_raw,mean_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcee895",
   "metadata": {},
   "source": [
    "# Calculation of std deviation raw (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's calculate the standard deviation (squared root of the variance)\n",
    "\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum_of_squares = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if(batch_idx%1000==0):\n",
    "        print(\"batch\",batch_idx,\"/\",total_n_batches,round((batch_idx/len(train_dataset)*100)),\"%, current_sum_of_squares:\",current_sum_of_squares)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\\n\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for elem in inputs:\n",
    "        elem = elem.double() #convert to float64 for precise calculations\n",
    "        #print(\"elem: shape\",elem.shape,\"content:\\n\",elem)\n",
    "        difference = elem - mean_raw\n",
    "        #print(\"difference: shape:\",difference.shape,\"content:\\n\", difference)\n",
    "        difference_squared = difference**2\n",
    "        #print(\"difference_squared: shape:\",difference_squared.shape,\"content:\\n\", difference_squared)\n",
    "        current_sum_of_squares += torch.sum(difference_squared)\n",
    "        #print(\"current_sum_of_squares:\",current_sum_of_squares)\n",
    "print(\"final sum of squares:\",current_sum_of_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "variance_raw = current_sum_of_squares/((len(train_dataset)*66150)-1)\n",
    "std_deviation_raw = math.sqrt(variance)\n",
    "\n",
    "print(\"variance raw:\",variance)\n",
    "print(\"std_deviation_raw:\",std_deviation_raw)\n",
    "\n",
    "print(\"Saving the std_deviation_raw in file:\",std_save_filename_raw) \n",
    "np.save(std_save_filename_raw,std_deviation_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cb8ee",
   "metadata": {},
   "source": [
    "# Load mean and std from file (fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d39f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mean_raw = np.load(mean_save_filename_raw+'.npy')\n",
    "print(\"loaded mean:\",loaded_mean_raw)\n",
    "\n",
    "loaded_std_raw = np.load(std_save_filename_raw+'.npy')\n",
    "print(\"loaded std:\",loaded_std_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7112c8",
   "metadata": {},
   "source": [
    "## Create dataset for raw audio\n",
    "\n",
    "The labels are the same as the STFT dataset (in the same order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "45f5f209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path=\"data/fma_small_raw_array_22050_overlapped\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDatasetRaw(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDatasetRaw(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDatasetRaw(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da518866",
   "metadata": {},
   "source": [
    "# Neural Network Architecture for raw audio\n",
    "\n",
    "We implemented a lightweight CNN to classify the samples with their raw audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "68b4a333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet_Raw(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(NNet_Raw, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=16)\n",
    "        self.conv2 = nn.Conv1d(16, 8, kernel_size=16)\n",
    "        self.conv3 = nn.Conv1d(25,25, kernel_size=16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=128)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=4)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(8)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(24)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(16)\n",
    "        self.inBN = nn.BatchNorm1d(1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(256, 24)\n",
    "        self.fc3 = nn.Linear(24, 8)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.inBN(x.float())\n",
    "        x=self.maxpool1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x=self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm2(x)     \n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0c90b296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1             [-1, 1, 66150]               2\n",
      "         MaxPool1d-2             [-1, 1, 16537]               0\n",
      "            Conv1d-3            [-1, 16, 16522]             272\n",
      "              ReLU-4            [-1, 16, 16522]               0\n",
      "       BatchNorm1d-5            [-1, 16, 16522]              32\n",
      "         MaxPool1d-6             [-1, 16, 4130]               0\n",
      "            Conv1d-7              [-1, 8, 4115]           2,056\n",
      "              ReLU-8              [-1, 8, 4115]               0\n",
      "       BatchNorm1d-9              [-1, 8, 4115]              16\n",
      "        MaxPool1d-10                [-1, 8, 32]               0\n",
      "           Linear-11                   [-1, 24]           6,168\n",
      "             ReLU-12                   [-1, 24]               0\n",
      "      BatchNorm1d-13                   [-1, 24]              48\n",
      "          Dropout-14                   [-1, 24]               0\n",
      "           Linear-15                    [-1, 8]             200\n",
      "          Softmax-16                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 8,794\n",
      "Trainable params: 8,794\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 7.94\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 8.23\n",
      "----------------------------------------------------------------\n",
      "NNet_Raw(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(16,), stride=(1,))\n",
      "  (conv2): Conv1d(16, 8, kernel_size=(16,), stride=(1,))\n",
      "  (conv3): Conv1d(25, 25, kernel_size=(16,), stride=(1,))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool1d(kernel_size=128, stride=128, padding=0, dilation=1, ceil_mode=False)\n",
      "  (maxpool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (batchnorm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (inBN): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=24, bias=True)\n",
      "  (fc3): Linear(in_features=24, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MyModel=NNet_Raw()\n",
    "summary(MyModel, (1,66150))\n",
    "print(MyModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f0168",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(MyModel, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08cc6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "641166af",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# NN on echonest features\n",
    "\n",
    "As you can see below, unfortunately we cannot perform a train using echonest features (danceability, tempo, acousticness, ...) because there are only 1300 tracks with echonest features which are not NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25e87d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening csvs...\n",
      "small dataset shape: (8000, 52)\n",
      "echonest csv shape (only audio features): (13129, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"opening csvs...\")\n",
    "\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "echonest = utils.load('data/fma_metadata/echonest.csv')\n",
    "echonest = echonest['echonest', 'audio_features']\n",
    "small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "\n",
    "print(\"small dataset shape:\",small.shape)\n",
    "print(\"echonest csv shape (only audio features):\",echonest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8b18347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track ids shape: 8000 content: [2, 5, 10, 140, 141, 148, 182, 190, 193, 194] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.416675</td>\n",
       "      <td>0.675894</td>\n",
       "      <td>0.634476</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.177647</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>165.922</td>\n",
       "      <td>0.576661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.745566</td>\n",
       "      <td>0.701470</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.124595</td>\n",
       "      <td>100.260</td>\n",
       "      <td>0.621661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.951670</td>\n",
       "      <td>0.658179</td>\n",
       "      <td>0.924525</td>\n",
       "      <td>0.965427</td>\n",
       "      <td>0.115474</td>\n",
       "      <td>0.032985</td>\n",
       "      <td>111.562</td>\n",
       "      <td>0.963590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.376312</td>\n",
       "      <td>0.734079</td>\n",
       "      <td>0.265685</td>\n",
       "      <td>0.669581</td>\n",
       "      <td>0.085995</td>\n",
       "      <td>0.039068</td>\n",
       "      <td>107.952</td>\n",
       "      <td>0.609991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.963657</td>\n",
       "      <td>0.435933</td>\n",
       "      <td>0.075632</td>\n",
       "      <td>0.345493</td>\n",
       "      <td>0.105686</td>\n",
       "      <td>0.026658</td>\n",
       "      <td>33.477</td>\n",
       "      <td>0.163950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154413</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154414</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155066</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        acousticness  danceability    energy  instrumentalness  liveness  \\\n",
       "2           0.416675      0.675894  0.634476          0.010628  0.177647   \n",
       "5           0.043567      0.745566  0.701470          0.000697  0.373143   \n",
       "10          0.951670      0.658179  0.924525          0.965427  0.115474   \n",
       "140         0.376312      0.734079  0.265685          0.669581  0.085995   \n",
       "141         0.963657      0.435933  0.075632          0.345493  0.105686   \n",
       "...              ...           ...       ...               ...       ...   \n",
       "154308           NaN           NaN       NaN               NaN       NaN   \n",
       "154309           NaN           NaN       NaN               NaN       NaN   \n",
       "154413           NaN           NaN       NaN               NaN       NaN   \n",
       "154414           NaN           NaN       NaN               NaN       NaN   \n",
       "155066           NaN           NaN       NaN               NaN       NaN   \n",
       "\n",
       "        speechiness    tempo   valence  \n",
       "2          0.159310  165.922  0.576661  \n",
       "5          0.124595  100.260  0.621661  \n",
       "10         0.032985  111.562  0.963590  \n",
       "140        0.039068  107.952  0.609991  \n",
       "141        0.026658   33.477  0.163950  \n",
       "...             ...      ...       ...  \n",
       "154308          NaN      NaN       NaN  \n",
       "154309          NaN      NaN       NaN  \n",
       "154413          NaN      NaN       NaN  \n",
       "154414          NaN      NaN       NaN  \n",
       "155066          NaN      NaN       NaN  \n",
       "\n",
       "[8000 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select small dataset from echonest csv\n",
    "\n",
    "track_ids = small.index.values.tolist()\n",
    "print(\"Track ids shape:\",len(track_ids),\"content:\",track_ids[:10],\"...\")\n",
    "echonest_small = pd.DataFrame(echonest,index=track_ids)\n",
    "\n",
    "ipd.display(echonest_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12714b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_echonest: shape: (8000, 8)\n",
      "There are 6706 rows containing NaN only as data\n"
     ]
    }
   ],
   "source": [
    "X_train_echonest = echonest_small.to_numpy(dtype=np.float16)\n",
    "print(\"X_train_echonest: shape:\",X_train_echonest.shape)\n",
    "\n",
    "nan_rows = np.argwhere(np.isnan(X_train_echonest).all(axis=1))\n",
    "\n",
    "print(\"There are\",len(nan_rows),\"rows containing NaN only as data\")\n",
    "#nan_rows = nan_rows.squeeze()\n",
    "#for i in nan_rows:\n",
    "#    print(\"row:\",i,\":\",X_train_echonest[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917bc8e",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "We try to use transfer learning using a ResNet18 by torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "497e31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 24, 64, 257]             648\n",
      "       BatchNorm2d-2          [-1, 24, 64, 257]              48\n",
      "              ReLU-3          [-1, 24, 64, 257]               0\n",
      "         MaxPool2d-4          [-1, 24, 32, 129]               0\n",
      "            Conv2d-5           [-1, 24, 16, 65]             216\n",
      "       BatchNorm2d-6           [-1, 24, 16, 65]              48\n",
      "            Conv2d-7           [-1, 24, 16, 65]             576\n",
      "       BatchNorm2d-8           [-1, 24, 16, 65]              48\n",
      "              ReLU-9           [-1, 24, 16, 65]               0\n",
      "           Conv2d-10          [-1, 24, 32, 129]             576\n",
      "      BatchNorm2d-11          [-1, 24, 32, 129]              48\n",
      "             ReLU-12          [-1, 24, 32, 129]               0\n",
      "           Conv2d-13           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-14           [-1, 24, 16, 65]              48\n",
      "           Conv2d-15           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-16           [-1, 24, 16, 65]              48\n",
      "             ReLU-17           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-18           [-1, 48, 16, 65]               0\n",
      "           Conv2d-19           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-20           [-1, 24, 16, 65]              48\n",
      "             ReLU-21           [-1, 24, 16, 65]               0\n",
      "           Conv2d-22           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-23           [-1, 24, 16, 65]              48\n",
      "           Conv2d-24           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-25           [-1, 24, 16, 65]              48\n",
      "             ReLU-26           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-27           [-1, 48, 16, 65]               0\n",
      "           Conv2d-28           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-29           [-1, 24, 16, 65]              48\n",
      "             ReLU-30           [-1, 24, 16, 65]               0\n",
      "           Conv2d-31           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-32           [-1, 24, 16, 65]              48\n",
      "           Conv2d-33           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-34           [-1, 24, 16, 65]              48\n",
      "             ReLU-35           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-36           [-1, 48, 16, 65]               0\n",
      "           Conv2d-37           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-38           [-1, 24, 16, 65]              48\n",
      "             ReLU-39           [-1, 24, 16, 65]               0\n",
      "           Conv2d-40           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-41           [-1, 24, 16, 65]              48\n",
      "           Conv2d-42           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-43           [-1, 24, 16, 65]              48\n",
      "             ReLU-44           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-45           [-1, 48, 16, 65]               0\n",
      "           Conv2d-46            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-47            [-1, 48, 8, 33]              96\n",
      "           Conv2d-48            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-49            [-1, 48, 8, 33]              96\n",
      "             ReLU-50            [-1, 48, 8, 33]               0\n",
      "           Conv2d-51           [-1, 48, 16, 65]           2,304\n",
      "      BatchNorm2d-52           [-1, 48, 16, 65]              96\n",
      "             ReLU-53           [-1, 48, 16, 65]               0\n",
      "           Conv2d-54            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-55            [-1, 48, 8, 33]              96\n",
      "           Conv2d-56            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-57            [-1, 48, 8, 33]              96\n",
      "             ReLU-58            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-59            [-1, 96, 8, 33]               0\n",
      "           Conv2d-60            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-61            [-1, 48, 8, 33]              96\n",
      "             ReLU-62            [-1, 48, 8, 33]               0\n",
      "           Conv2d-63            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-64            [-1, 48, 8, 33]              96\n",
      "           Conv2d-65            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-66            [-1, 48, 8, 33]              96\n",
      "             ReLU-67            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-68            [-1, 96, 8, 33]               0\n",
      "           Conv2d-69            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-70            [-1, 48, 8, 33]              96\n",
      "             ReLU-71            [-1, 48, 8, 33]               0\n",
      "           Conv2d-72            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-73            [-1, 48, 8, 33]              96\n",
      "           Conv2d-74            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-75            [-1, 48, 8, 33]              96\n",
      "             ReLU-76            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-77            [-1, 96, 8, 33]               0\n",
      "           Conv2d-78            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-79            [-1, 48, 8, 33]              96\n",
      "             ReLU-80            [-1, 48, 8, 33]               0\n",
      "           Conv2d-81            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-82            [-1, 48, 8, 33]              96\n",
      "           Conv2d-83            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-84            [-1, 48, 8, 33]              96\n",
      "             ReLU-85            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-86            [-1, 96, 8, 33]               0\n",
      "           Conv2d-87            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-88            [-1, 48, 8, 33]              96\n",
      "             ReLU-89            [-1, 48, 8, 33]               0\n",
      "           Conv2d-90            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-91            [-1, 48, 8, 33]              96\n",
      "           Conv2d-92            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-93            [-1, 48, 8, 33]              96\n",
      "             ReLU-94            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-95            [-1, 96, 8, 33]               0\n",
      "           Conv2d-96            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-97            [-1, 48, 8, 33]              96\n",
      "             ReLU-98            [-1, 48, 8, 33]               0\n",
      "           Conv2d-99            [-1, 48, 8, 33]             432\n",
      "     BatchNorm2d-100            [-1, 48, 8, 33]              96\n",
      "          Conv2d-101            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-102            [-1, 48, 8, 33]              96\n",
      "            ReLU-103            [-1, 48, 8, 33]               0\n",
      "InvertedResidual-104            [-1, 96, 8, 33]               0\n",
      "          Conv2d-105            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-106            [-1, 48, 8, 33]              96\n",
      "            ReLU-107            [-1, 48, 8, 33]               0\n",
      "          Conv2d-108            [-1, 48, 8, 33]             432\n",
      "     BatchNorm2d-109            [-1, 48, 8, 33]              96\n",
      "          Conv2d-110            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-111            [-1, 48, 8, 33]              96\n",
      "            ReLU-112            [-1, 48, 8, 33]               0\n",
      "InvertedResidual-113            [-1, 96, 8, 33]               0\n",
      "          Conv2d-114            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-115            [-1, 48, 8, 33]              96\n",
      "            ReLU-116            [-1, 48, 8, 33]               0\n",
      "          Conv2d-117            [-1, 48, 8, 33]             432\n",
      "     BatchNorm2d-118            [-1, 48, 8, 33]              96\n",
      "          Conv2d-119            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-120            [-1, 48, 8, 33]              96\n",
      "            ReLU-121            [-1, 48, 8, 33]               0\n",
      "InvertedResidual-122            [-1, 96, 8, 33]               0\n",
      "          Conv2d-123            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-124            [-1, 96, 4, 17]             192\n",
      "          Conv2d-125            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-126            [-1, 96, 4, 17]             192\n",
      "            ReLU-127            [-1, 96, 4, 17]               0\n",
      "          Conv2d-128            [-1, 96, 8, 33]           9,216\n",
      "     BatchNorm2d-129            [-1, 96, 8, 33]             192\n",
      "            ReLU-130            [-1, 96, 8, 33]               0\n",
      "          Conv2d-131            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-132            [-1, 96, 4, 17]             192\n",
      "          Conv2d-133            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-134            [-1, 96, 4, 17]             192\n",
      "            ReLU-135            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-136           [-1, 192, 4, 17]               0\n",
      "          Conv2d-137            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-138            [-1, 96, 4, 17]             192\n",
      "            ReLU-139            [-1, 96, 4, 17]               0\n",
      "          Conv2d-140            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-141            [-1, 96, 4, 17]             192\n",
      "          Conv2d-142            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-143            [-1, 96, 4, 17]             192\n",
      "            ReLU-144            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-145           [-1, 192, 4, 17]               0\n",
      "          Conv2d-146            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-147            [-1, 96, 4, 17]             192\n",
      "            ReLU-148            [-1, 96, 4, 17]               0\n",
      "          Conv2d-149            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-150            [-1, 96, 4, 17]             192\n",
      "          Conv2d-151            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-152            [-1, 96, 4, 17]             192\n",
      "            ReLU-153            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-154           [-1, 192, 4, 17]               0\n",
      "          Conv2d-155            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-156            [-1, 96, 4, 17]             192\n",
      "            ReLU-157            [-1, 96, 4, 17]               0\n",
      "          Conv2d-158            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-159            [-1, 96, 4, 17]             192\n",
      "          Conv2d-160            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-161            [-1, 96, 4, 17]             192\n",
      "            ReLU-162            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-163           [-1, 192, 4, 17]               0\n",
      "          Conv2d-164          [-1, 1024, 4, 17]         196,608\n",
      "     BatchNorm2d-165          [-1, 1024, 4, 17]           2,048\n",
      "            ReLU-166          [-1, 1024, 4, 17]               0\n",
      "          Linear-167                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 1,366,792\n",
      "Trainable params: 1,366,792\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 33.40\n",
      "Params size (MB): 5.21\n",
      "Estimated Total Size (MB): 39.37\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/riccardo/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "model_ResNet18 = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x0_5', pretrained=True)\n",
    "summary(model_ResNet18, (3,128,513))\n",
    "\n",
    "\n",
    "# Remove layers you want to discard\n",
    "\n",
    "class MyModel_ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel_ResNet18, self).__init__()\n",
    "        pretrained_model = torchvision.models.shufflenet_v2_x0_5(pretrained=True)\n",
    "        layers=list(pretrained_model.children())[:-4]\n",
    "        #print(layers)\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "        #self.conv1=nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "        #self.relu1=nn.ReLU(inplace=True)\n",
    "        #self.mp1=nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        #self.conv2=nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        '''self.relu2=nn.ReLU(inplace=True)\n",
    "        self.mp2=nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv3=nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3=nn.ReLU(inplace=True)\n",
    "        self.conv4=nn.Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4=nn.ReLU(inplace=True)\n",
    "        self.conv5=nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5=nn.ReLU(inplace=True)\n",
    "        self.mp5=nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)'''\n",
    "        \n",
    "        \n",
    "        self.pool= nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten=nn.Flatten() \n",
    "        self.last_layer = nn.Linear(49920, 32)\n",
    "        self.fc= nn.Linear(32, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x=self.conv1(x)\n",
    "        #x=self.relu1(x)\n",
    "        #x=self.mp1(x)\n",
    "        '''x=self.conv2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.mp2(x)    \n",
    "        x=self.conv4(x)\n",
    "        x=self.relu4(x)\n",
    "        x=self.conv5(x)\n",
    "        x=self.relu5(x)\n",
    "        x=self.mp5(x)'''\n",
    "        \n",
    "        x = self.features(x)\n",
    "        #x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.last_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8b7bc56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 24, 64, 257]             648\n",
      "       BatchNorm2d-2          [-1, 24, 64, 257]              48\n",
      "              ReLU-3          [-1, 24, 64, 257]               0\n",
      "         MaxPool2d-4          [-1, 24, 32, 129]               0\n",
      "            Conv2d-5           [-1, 24, 16, 65]             216\n",
      "       BatchNorm2d-6           [-1, 24, 16, 65]              48\n",
      "            Conv2d-7           [-1, 24, 16, 65]             576\n",
      "       BatchNorm2d-8           [-1, 24, 16, 65]              48\n",
      "              ReLU-9           [-1, 24, 16, 65]               0\n",
      "           Conv2d-10          [-1, 24, 32, 129]             576\n",
      "      BatchNorm2d-11          [-1, 24, 32, 129]              48\n",
      "             ReLU-12          [-1, 24, 32, 129]               0\n",
      "           Conv2d-13           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-14           [-1, 24, 16, 65]              48\n",
      "           Conv2d-15           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-16           [-1, 24, 16, 65]              48\n",
      "             ReLU-17           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-18           [-1, 48, 16, 65]               0\n",
      "           Conv2d-19           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-20           [-1, 24, 16, 65]              48\n",
      "             ReLU-21           [-1, 24, 16, 65]               0\n",
      "           Conv2d-22           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-23           [-1, 24, 16, 65]              48\n",
      "           Conv2d-24           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-25           [-1, 24, 16, 65]              48\n",
      "             ReLU-26           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-27           [-1, 48, 16, 65]               0\n",
      "           Conv2d-28           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-29           [-1, 24, 16, 65]              48\n",
      "             ReLU-30           [-1, 24, 16, 65]               0\n",
      "           Conv2d-31           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-32           [-1, 24, 16, 65]              48\n",
      "           Conv2d-33           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-34           [-1, 24, 16, 65]              48\n",
      "             ReLU-35           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-36           [-1, 48, 16, 65]               0\n",
      "           Conv2d-37           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-38           [-1, 24, 16, 65]              48\n",
      "             ReLU-39           [-1, 24, 16, 65]               0\n",
      "           Conv2d-40           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-41           [-1, 24, 16, 65]              48\n",
      "           Conv2d-42           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-43           [-1, 24, 16, 65]              48\n",
      "             ReLU-44           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-45           [-1, 48, 16, 65]               0\n",
      "          Flatten-46                [-1, 49920]               0\n",
      "           Linear-47                    [-1, 8]         399,368\n",
      "          Softmax-48                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 407,000\n",
      "Trainable params: 407,000\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 20.44\n",
      "Params size (MB): 1.55\n",
      "Estimated Total Size (MB): 22.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "model = MyModel_ResNet18()\n",
    "summary(model, (3,128,513))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b4b15",
   "metadata": {},
   "source": [
    "## RGB Dataset\n",
    "Resnet18 expects RGB input images, so our dataset need to be converted from a one to a three-channel. We'll do dat by copying three times the same image in the three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5e94008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDatasetRGB(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "                \n",
    "        #do ResNet18 normalization:\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        #copy the channel 3 times (need to unsqueeze to create a new dimension first)\n",
    "        #print(\"DATASET*  sample shape is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        stft_vector = stft_vector.unsqueeze(0).repeat(3,1,1)\n",
    "        stft_vector = stft_vector.to(torch.float32) #float32 needed for ResNet18 model (downcast from float64)\n",
    "        #print(\"DATASET* sample shape after repeat is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        #print(\"stft_vector dtype:\",stft_vector.dtype)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c24605b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor batch_idx, batch  in enumerate(train_data_loader):\\n    print (\"batch index:\",batch_idx)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    \\n    for idx, sample in enumerate(inputs):\\n        label = labels[idx]\\n        print(\"inputs: shape:\",inputs.shape)\\n        print(\"sample: shape:\",sample.shape)\\n'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform_ResNet18 = transforms.Compose([\n",
    "    transforms.Normalize(mean= [loaded_mean,loaded_mean,loaded_mean], std=[loaded_std,loaded_std,loaded_std]) #our values\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #ResNet18 specific values\n",
    "])\n",
    "\n",
    "train_dataset = MyDatasetRGB(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDatasetRGB(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDatasetRGB(test_file_paths, Y_test,  transform = transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size = 10, shuffle=True)\n",
    "\n",
    "'''\n",
    "for batch_idx, batch  in enumerate(train_data_loader):\n",
    "    print (\"batch index:\",batch_idx)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    \n",
    "    for idx, sample in enumerate(inputs):\n",
    "        label = labels[idx]\n",
    "        print(\"inputs: shape:\",inputs.shape)\n",
    "        print(\"sample: shape:\",sample.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5f3747da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2d031c3ccc4949aed77c2afbc91a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-cf08808a9749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRGB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-4e326a5b92a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose, RGB)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Extract the inputs and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-2d7d4f15fcf3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         x=self.mp5(x)'''\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    163\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "#todo: convert input tensor from float64 to double\n",
    "\n",
    "#todo normalize again using resnet18 suggested mean and std\n",
    "#TODO: FIX TEST FUNCTION USING RGB (maybe already works)\n",
    "\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.001,verbose=False, RGB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0f8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
