{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15945d26",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6eab989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tqdm.notebook as tq\n",
    "import utils\n",
    "from pydub import AudioSegment\n",
    "from tkinter import Tcl # file sorting by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7763c",
   "metadata": {},
   "source": [
    "# Creation of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8ded2",
   "metadata": {},
   "source": [
    "### Dictionary creation for the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c4013",
   "metadata": {},
   "source": [
    "We want a dictionary indicating a numbeer for each genre:\n",
    "\n",
    "{0: 'Hip-Hop', 1: 'Pop', 2: 'Folk', 3: 'Rock', 4: 'Experimental', 5: 'International', 6: 'Electronic', 7: 'Instrumental'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e985c",
   "metadata": {},
   "source": [
    "### Creation of the labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be9951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_folder: data/fma_small_stft_transposed/train\n",
      "validation_folder: data/fma_small_stft_transposed/validation\n",
      "test_folder: data/fma_small_stft_transposed/test \n",
      "\n",
      "audio directory:  ./data/fma_small/\n",
      "Loading tracks.csv...\n",
      "small dataset shape: (8000, 52)\n",
      "Track.csv: 6400 training samples, 800 validation samples, 800 test samples\n",
      "\n",
      "there are 8 unique genres\n",
      "Dictionary of genres created: {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Rock': 3, 'Experimental': 4, 'International': 5, 'Electronic': 6, 'Instrumental': 7}\n",
      "labels length: 63970\n",
      "labels length: 8000\n",
      "labels length: 8000\n"
     ]
    }
   ],
   "source": [
    "def create_single_dataset(folder_path, tracks_dataframe, genre_dictionary):    \n",
    "    labels = []\n",
    "   \n",
    "    _, file_list = get_sorted_file_paths(folder_path)\n",
    "    \n",
    "    for i,file in enumerate(file_list):\n",
    "        #print(\"considering file:\",file, \"({}/{})\".format(i,len(file_list)))\n",
    "        track_id_clip_id = file.split('.')[0]\n",
    "        track_id = track_id_clip_id.split('_')[0]\n",
    "        #print(\"track id with clip: {}, track id: {}\".format(track_id_clip_id, track_id))\n",
    "        genre = tracks_dataframe.loc[int(track_id)]\n",
    "        #print(\"genre from dataframe: \", genre)\n",
    "        label = genre_dictionary[genre]\n",
    "        #print(\"label from dictionary:\",label)\n",
    "        labels.append(label)\n",
    "    print(\"labels length: {}\".format(len(labels)))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "#create the train,validation and test vectors using the files in the train/validation/test folders\n",
    "def create_dataset_splitted(folder_path):\n",
    "    train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "    validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "    test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "    \n",
    "    print(\"train_folder:\",train_folder)\n",
    "    print(\"validation_folder:\",validation_folder)\n",
    "    print(\"test_folder:\",test_folder,\"\\n\")\n",
    "    \n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "    \n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "\n",
    "    small_training_top_genres = small_training['genre_top']\n",
    "    small_validation_top_genres = small_validation['genre_top']\n",
    "    small_test_top_genres = small_test['genre_top']\n",
    "    \n",
    "    #create dictionary of genre classes:\n",
    "    unique_genres = small_training_top_genres.unique()\n",
    "    unique_genres = np.array(unique_genres)\n",
    "    print(\"there are {} unique genres\".format(len(unique_genres)))\n",
    "    genre_dictionary = {}\n",
    "    for i,genre in enumerate(unique_genres):\n",
    "        genre_dictionary[genre] = i\n",
    "    print(\"Dictionary of genres created:\",genre_dictionary)\n",
    "    \n",
    "    \n",
    "    Y_train = create_single_dataset(train_folder, small_training_top_genres, genre_dictionary)\n",
    "    Y_validation = create_single_dataset(validation_folder, small_validation_top_genres, genre_dictionary)\n",
    "    Y_test = create_single_dataset(test_folder, small_test_top_genres, genre_dictionary)\n",
    "    \n",
    "    return Y_train, Y_validation, Y_test\n",
    " \n",
    "def get_sorted_file_paths(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #sort the dataset files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_list = Tcl().call('lsort', '-dict', file_list) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...\n",
    "    file_paths = [os.path.join(folder_path, file_name) for file_name in file_list] #join filename with folder path\n",
    "    #print(\"There are {} in the folder: {}\".format(len(file_list),file_list))\n",
    "    return file_paths, file_list\n",
    "    \n",
    "    \n",
    "folder_path=\"data/fma_small_stft_transposed\"\n",
    "Y_train, Y_validation, Y_test = create_dataset_splitted(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20175a8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187ddbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list, labels):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae70891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  63970\n",
      "len of validation dataset:  8000\n",
      "len of test dataset:  8000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_stft_transposed\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDataset(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef67f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_dimension_errors(filepaths):\n",
    "    error_indexes = []\n",
    "    progress = 0\n",
    "    for file in filepaths:\n",
    "        progress+=1\n",
    "        print(\"checked {}/{} files\".format(progress,len(filepaths)))\n",
    "        x = np.load(file)\n",
    "        if(x.shape != (128,513)):\n",
    "            error_indexes.append(x)\n",
    "            print(\"error\")\n",
    "    print(\"{} errors found in files: {}\".format(len(error_indexes),error_indexes))\n",
    "    for idx,error in enumerate(error_indexes):\n",
    "        print(\"index: {}, shape: {}\".format(idx,error.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a52dc",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f745b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513), stride=(1,513))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1), stride=(1,513))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1), stride=(1,513))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.dense2 = nn.Linear(300, 150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x=self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845a53e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d034d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "LEARNING_RATE=0.0001\n",
    "\n",
    "learning_rate_list = [0.01, 0.001, 0.0001]\n",
    "batch_size_list = [128,256,512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd576a3",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "223742e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, validation_dataset, Y_validation):\n",
    "    #Stop parameters learning\n",
    "    model.eval()\n",
    "    \n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=10)\n",
    "    \n",
    "    # Definisci i pesi delle classi in base alla loro importanza (puoi sperimentare con questi valori)\n",
    "    class_weights = torch.tensor([2.5, 2.5, 1.5, 1.0, 0.1, 1.0, 1.5, 2.5])\n",
    "\n",
    "    # Crea una funzione di perdita con pesi\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confusion_matrix = np.zeros((8,8 ), dtype=int)\n",
    "\n",
    "    correct_maj=0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            \n",
    "            \n",
    "            #print(\"Inputs:\",inputs,\"size:\",inputs.size())\n",
    "            #print(\"Labels:\",labels,\"size:\",labels.size())\n",
    "            inputs=inputs.unsqueeze(1)\n",
    "            #predict label\n",
    "            outputs = model(inputs)\n",
    "            #print(\"Outputs:\",outputs,\"size:\",outputs.size())\n",
    "            #compute loss\n",
    "            #TO DO:adapt it to the voting\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            voting=outputs.sum(dim=0)\n",
    "            #print(\"Voting:\",voting,\"size:\",voting.size())\n",
    "            predicted= torch.argmax(voting)\n",
    "            #print(\"winning class\",predicted)\n",
    "            correct += (predicted == labels[0])\n",
    "            confusion_matrix[predicted][labels[0]]+=1\n",
    "            \n",
    "            votes=[0,0,0,0,0,0,0,0]\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                votes[max_index]+=1\n",
    "            majority_prediction=votes.index(max(votes)) \n",
    "            correct_maj += (majority_prediction == labels[0])\n",
    "            \n",
    "            \n",
    "    maj_acc=correct_maj*100/800        \n",
    "            \n",
    "    cm=ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "    cm.plot()\n",
    "    print(confusion_matrix)\n",
    "    accuracy = 100*correct / 800 \n",
    "    average_loss = total_loss / 800\n",
    "    \n",
    "    print(\"Majority:\",maj_acc,\"\\t Probability:\",accuracy)\n",
    "    model.train()\n",
    "    return accuracy, average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ca4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, num_epochs, learning_rate, verbose = False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    counted_labels=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", content: \",inputs)\n",
    "                print(\"\\nlabels shape:\",labels.size(),\", content: \",labels)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if(verbose == True):\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "                print(\"List of labels until now:\",counted_labels)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                counted_labels[labels[index].item()]+=1\n",
    "                if(labels[index].item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "        \n",
    "        #calculate train loss and accuracy\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        train_loss_list.append(average_loss)\n",
    "        train_acc_list.append(average_accuracy)\n",
    "        \n",
    "        #calculate validation loss and accuracy\n",
    "        val_acc, val_loss = test(model, validation_dataset, Y_validation)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],Train Loss: {average_loss:.4f}. Train Accuracy: {average_accuracy} Val Loss: {val_loss} Val Accuracy: {val_acc}\")\n",
    "        progress_bar.close()\n",
    "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbbc7adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing model with batch size=128, lr=0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d27d5a19f74dedb5f955d6aebbb7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [100 100 100 100 100 100 100 100]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]]\n",
      "Epoch [1/10],Train Loss: 2.1489. Train Accuracy: 12.4859375 Val Loss: 2.149008959531784 Val Accuracy: 12.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5090e0725ced4d92b51cfb9cddfd2cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e522e69697f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_lr_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'_bs_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Traing model with batch size={bs}, lr={lr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mperformance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mperformance_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0b7ef43db2fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#calculate validation loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mval_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mval_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d6457c5ef5be>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, validation_dataset, Y_validation)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;31m#print(\"Inputs:\",inputs,\"size:\",inputs.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m#print(\"Labels:\",labels,\"size:\",labels.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1f58f3fb8878>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mstft_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#load from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstft_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 440\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0m_check_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfortran_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_array_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0mheader_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhlength_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhlength_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;31m# The header is a pretty-printed string representation of a literal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summary(model, (1, 128, 513))\n",
    "performance_list = []\n",
    "model_list = []\n",
    "directory = 'models/'\n",
    "\n",
    "for lr in learning_rate_list:   \n",
    "    for bs in batch_size_list:\n",
    "        model = NNet1() #reinitialize model\n",
    "        file_path = directory + 'model' + '_lr_' + str(lr).split('.')[0] + '-' + str(lr).split('.')[1]+ '_bs_' + str(bs)\n",
    "        print(f\"Traing model with batch size={bs}, lr={lr}\")\n",
    "        performance=train(model, train_dataset, batch_size=bs, num_epochs=EPOCHS, learning_rate=lr)\n",
    "        performance_list.append(performance)\n",
    "        model_list.append(model)\n",
    "        print(\"saving model in\", file_path)\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "        print(performance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d928b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'performance_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-feab07013d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mparameter_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'performance_list' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "plt.plot(np.arange(1,EPOCHS+1), performance[1][:], label='Loss') \n",
    "plt.plot(np.arange(1,EPOCHS+1), performance[0][:], label='Accuracy')\n",
    "plt.legend()  # Display the legend showing the labels\n",
    "plt.show()\n",
    "print(performance[0][:])\n",
    "'''\n",
    "\n",
    "parameter_list = []\n",
    "for lr in learning_rate_list:\n",
    "    for bs in batch_size_list:\n",
    "        parameter_list.append([lr,bs])\n",
    "\n",
    "for performance, parameters in zip(performance_list, parameter_list):\n",
    "    plt.plot(np.arange(1,EPOCHS+1), performance[1][:], label='Val Loss') \n",
    "    plt.plot(np.arange(1,EPOCHS+1), performance[0][:], label='Val Accuracy')\n",
    "    plt.title(f\"Lr = {parameters[0]}, Batch size = {parameters[1]}\")\n",
    "    plt.legend()  # Display the legend showing the labels\n",
    "    plt.show()\n",
    "    print(performance[0][:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef7fab",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1 + BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3685ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet1_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1_BN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513), stride=(1, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1), stride=(1, 513))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1), stride=(1, 513))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.bn4 = nn.BatchNorm1d(300)\n",
    "        self.dense2 = nn.Linear(300, 150)\n",
    "        self.bn5 = nn.BatchNorm1d(150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b169d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bce4196d1cf49f9a37cc705fa7db246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [14 11  9 53  3 11  3  5]\n",
      " [60 82 90 47 90 81 80 87]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [25  4  0  0  1  8 14  1]\n",
      " [ 1  3  1  0  6  0  3  7]]\n",
      "Majority: tensor(21.1250) \t Probability: tensor(20.5000)\n",
      "Epoch [1/20],Train Loss: 1.8791. Train Accuracy: 41.028125 Val Loss: 2.03497205093503 Val Accuracy: 20.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b65b842b1845ce8cefbf06873560bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 2  3  2 32  1  1  0  4]\n",
      " [48 88 98 68 96 85 59 94]\n",
      " [ 0  0  0  0  0  4  0  0]\n",
      " [50  8  0  0  3 10 39  1]\n",
      " [ 0  0  0  0  0  0  2  1]]\n",
      "Majority: tensor(21.2500) \t Probability: tensor(21.5000)\n",
      "Epoch [2/20],Train Loss: 1.7869. Train Accuracy: 49.5 Val Loss: 2.031885189265013 Val Accuracy: 21.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417e6a1151274ee29173ddd6cea03611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  1  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 8  6  5 44  2  2  0  4]\n",
      " [58 88 95 56 95 87 69 95]\n",
      " [ 0  0  0  0  0  1  0  0]\n",
      " [29  5  0  0  3  9 31  1]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Majority: tensor(22.3750) \t Probability: tensor(22.)\n",
      "Epoch [3/20],Train Loss: 1.7493. Train Accuracy: 52.9875 Val Loss: 2.0337091624736785 Val Accuracy: 22.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9eb8dd12df4de296890b03864db5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_NNet1_BN = NNet1_BN()\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model_NNet1_BN, train_dataset, batch_size=128, num_epochs=20, learning_rate=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343371e",
   "metadata": {},
   "source": [
    "# Raw audio (1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabb348",
   "metadata": {},
   "source": [
    "## Raw audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30092ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset of way audio\n",
    "#class MyDatasetRaw(Dataset):\n",
    "    #def __init__(self, file_list, labels):\n",
    "        #self.file_list = file_list\n",
    "        #self.labels=labels\n",
    "\n",
    "    #def __len__(self):\n",
    "        #return len(self.file_list)\n",
    "\n",
    "    #def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        #file_path = self.file_list[idx]\n",
    "        #label = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        #print(\"opening file:\",file_path)\n",
    "        #audio_file = AudioSegment.from_file(file_path)\n",
    "\n",
    "        #data = audio_file._data\n",
    "        #pcm16_signed_integers = []\n",
    "\n",
    "        # This loop decodes the bytestring into PCM samples.\n",
    "        # The bytestring is a stream of little-endian encoded signed integers.\n",
    "        # This basically just cuts each two-byte sample out of the bytestring, converts\n",
    "        # it to an integer, and appends it to the list of samples.\n",
    "        #for sample_index in range(len(data)//2):\n",
    "            #sample = int.from_bytes(data[sample_index*2:sample_index*2+2], 'big', signed=True)\n",
    "            #pcm16_signed_integers.append(sample)\n",
    "\n",
    "\n",
    "        #load audio in ram\n",
    "        #x = np.load(file_path,allow_pickle=True) #load the MONO audio file from the data/fma_small directory\n",
    "        #x=torch.tensor(pcm16_signed_integers)\n",
    "        #x = x.type(torch.FloatTensor)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Load the MP3 file\n",
    "        #audio = AudioSegment.from_mp3(file_path)\n",
    "\n",
    "        # Set the desired new sampling rate\n",
    "        #new_sampling_rate = 8000  # for example, 16000 Hz\n",
    "\n",
    "        # Resample the audio to the new sampling rate\n",
    "        #resampled_audio = audio.set_frame_rate(new_sampling_rate)\n",
    "\n",
    "        # Convert the resampled audio to a raw PCM array\n",
    "        #resampled_pcm_audio = resampled_audio.raw_data\n",
    "        #resampled_pcm_array = np.frombuffer(resampled_pcm_audio, dtype=np.int16)\n",
    "        \n",
    "        #print(\"Versione 2:\",resampled_pcm_array.size,\"\\tVersione 1:\",x.shape)\n",
    "        #x=torch.tensor(resampled_pcm_array)\n",
    "        #x = x.type(torch.FloatTensor)\n",
    "        #àreturn x, label\n",
    "    \n",
    "# Define the custom class for accessing our dataset\n",
    "class MyDatasetRaw(Dataset):\n",
    "    def __init__(self, file_list, labels):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        raw_vector = np.load(file_path).astype(np.int16)  # Ensure int16 data type\n",
    "        raw_vector = torch.tensor(raw_vector)\n",
    "        return raw_vector[:200000], label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c527e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns file paths list for train, validation and test\n",
    "def get_file_paths():\n",
    "    folder_raw = './data/fma_small'\n",
    "    file_paths_raw = []\n",
    "\n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "    \n",
    "    #--------------TRAIN----------------------\n",
    "    print(\"Creating train dataset...\")\n",
    "    file_paths_train = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_train = np.array(small_training.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_train)} samples. Here's the first ones: {track_ids_train[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_train:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_train.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_train = np.array(Tcl().call('lsort', '-dict', file_paths_train)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    #--------------VALIDATION----------------------\n",
    "    print(\"Creating validation dataset...\")\n",
    "    file_paths_validation = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_validation = np.array(small_validation.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_validation)} samples. Here's the first ones: {track_ids_validation[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_validation:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_validation.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_validation = np.array(Tcl().call('lsort', '-dict', file_paths_validation)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    #--------------TEST----------------------\n",
    "    print(\"Creating test dataset...\")\n",
    "    file_paths_test = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_test = np.array(small_test.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_test)} samples. Here's the first ones: {track_ids_test[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_test:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_test.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_test = np.array(Tcl().call('lsort', '-dict', file_paths_test)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    \n",
    "    #create the datasets\n",
    "    \n",
    "    return file_paths_train, file_paths_validation, file_paths_test\n",
    "\n",
    "#returns file paths list for train, validation and test for each 3s clip\n",
    "def get_file_paths_split():\n",
    "    folder_raw = './data/fma_small_raw_array'\n",
    "    folder_train = folder_raw + '/train'\n",
    "    folder_validation = folder_raw + '/validation'\n",
    "    folder_test = folder_raw + '/test'\n",
    "    file_paths_raw = []\n",
    "\n",
    "    #--------------TRAIN----------------------\n",
    "    print(\"Creating train dataset...\")\n",
    "    file_paths_train = os.listdir(folder_train)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_train = np.array(Tcl().call('lsort', '-dict', file_paths_train)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_train = []\n",
    "    for file in file_paths_train:\n",
    "        complete_file_path_train.append(folder_train + '/' + file)\n",
    "\n",
    "    #--------------VALIDATION----------------------\n",
    "    print(\"Creating validation dataset...\")\n",
    "    file_paths_validation = os.listdir(folder_validation)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_validation = np.array(Tcl().call('lsort', '-dict', file_paths_validation)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_validation = []\n",
    "    for file in file_paths_validation:\n",
    "        complete_file_path_validation.append(folder_validation + '/' + file)\n",
    "\n",
    "    #--------------TEST----------------------\n",
    "    print(\"Creating test dataset...\")\n",
    "    file_paths_test = os.listdir(folder_test)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_test = np.array(Tcl().call('lsort', '-dict', file_paths_test)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_test = []\n",
    "    for file in file_paths_test:\n",
    "        complete_file_path_test.append(folder_test + '/' + file)\n",
    "    \n",
    "    return complete_file_path_train, complete_file_path_validation, complete_file_path_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99825d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset...\n",
      "Creating validation dataset...\n",
      "Creating test dataset...\n",
      "6397\n",
      "800\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "file_paths_train, file_paths_validation, file_paths_test = get_file_paths_split()\n",
    "print(len(file_paths_train))\n",
    "print(len(file_paths_validation))\n",
    "print(len(file_paths_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891ab30",
   "metadata": {},
   "source": [
    "## Fix wrong sampling rates in the dataset\n",
    "\n",
    "Some songs in the fma_small dataset has a different sampling rate from 44100. Let's fix them by using librosa resampling tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4f82d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****Checking sample rates to be 44100 in validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/anaconda3/envs/nndl/lib/python3.6/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/fma_small_raw_split/train/148_0.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__soundfile_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Otherwise, create the soundfile object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    657\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLibsndfileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error opening {0!r}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening './data/fma_small_raw_split/train/148_0.mp3': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-ac5d47883530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n*****Checking sample rates to be\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"in validation set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_paths_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mtarget_sr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wrong sr found! value:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"resampling to\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/librosa/util/decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPurePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PySoundFile failed. Trying audioread instead.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# If the input was not an audioread object, try to open it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mBackendClass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mBackendClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \"\"\"\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/fma_small_raw_split/train/148_0.mp3'"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "#fix sampling rate\n",
    "target_sr = 44100\n",
    "wrong_sr_list = []\n",
    "\n",
    "\"\"\"\n",
    "print(\"\\n******Checking sample rates to be\",target_sr,\"in train set\")\n",
    "for file in file_paths_train:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "   \"\"\"     \n",
    "print(\"\\n*****Checking sample rates to be\",target_sr,\"in validation set\")\n",
    "for file in file_paths_validation:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "\n",
    "        \n",
    "print(\"\\n*****Checking sample rates to be\",target_sr,\"in test set\")\n",
    "for file in file_paths_test:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "        \n",
    "print(\"There were a total of\",len(wrong_sr_list),\"files with wrong sampling rate. Now they have been corrected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029105b",
   "metadata": {},
   "source": [
    "## Create labels for raw audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ea1db",
   "metadata": {},
   "source": [
    "We have to take one label each ten from Y_label. Now we are not using 10 clips for each audio but just the whole audio itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25f8b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train reshaped: 6397\n",
      "Y_validation reshaped: 800\n",
      "Y_test reshaped: 800\n"
     ]
    }
   ],
   "source": [
    "#take a element every ten (not using 10 clips per audio anymore)\n",
    "Y_train_reshaped = [elem for elem in Y_train[0::10]] \n",
    "print(\"Y_train reshaped:\",len(Y_train_reshaped))\n",
    "Y_validation_reshaped = [elem for elem in Y_validation[0::10]] \n",
    "print(\"Y_validation reshaped:\",len(Y_validation_reshaped))\n",
    "Y_test_reshaped = [elem for elem in Y_test[0::10]] \n",
    "print(\"Y_test reshaped:\",len(Y_test_reshaped))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7112c8",
   "metadata": {},
   "source": [
    "## Create dataset for raw audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f5f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDatasetRaw(file_paths_train, Y_train)\n",
    "validation_dataset = MyDatasetRaw(file_paths_validation, Y_validation)\n",
    "test_dataset = MyDatasetRaw(file_paths_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f89626be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom model class\n",
    "class NNet_Raw(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet_Raw, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(1599936, 64)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(8)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa495fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet_Raw2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet_Raw2, self).__init__()\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.layer1 = nn.Conv1d(1, 8, 7, 3)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(8)\n",
    "        self.layer2 = nn.Conv1d(8, 16, 5, 2)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(16)\n",
    "        self.layer3 = nn.Conv1d(16, 16, 3, 2)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(16)\n",
    "        self.layer4 = nn.Conv1d(16, 16, 3, 2)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(16)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(112, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.dropout1(self.lrelu(self.layer1(x)))\n",
    "        y = self.dropout1(self.lrelu(self.batchnorm1(self.layer2(y))))\n",
    "        y = self.dropout1(self.lrelu(self.batchnorm2(self.layer3(y))))\n",
    "        y = self.dropout2(self.lrelu(self.batchnorm3(self.layer4(y))))\n",
    "        y = self.flatten(y)\n",
    "        y = self.fc(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68b4a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet_Raw3(nn.Module):    \n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(NNet_Raw3, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=8, stride=16)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=8, stride=8)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128) \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(24960, 128) \n",
    "        self.fc2 = nn.Linear(128, 32) \n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm3(x) \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x) \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c9d7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_raw(model, validation_dataset, Y_validation):\n",
    "    #Stop parameters learning\n",
    "    model.eval()\n",
    "    \n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset)\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confusion_matrix = np.zeros((8,8 ), dtype=int)\n",
    "\n",
    "    correct_maj=0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input, label in validation_loader:\n",
    "            \n",
    "            \n",
    "            #print(\"Inputs:\",inputs,\"size:\",inputs.size())\n",
    "            #print(\"Labels:\",labels,\"size:\",labels.size())\n",
    "            input=input.unsqueeze(1)\n",
    "            #predict label\n",
    "            output = model(input)\n",
    "            \n",
    "            predicted= torch.argmax(output)\n",
    "            #print(\"winning class\",predicted)\n",
    "            correct += (predicted == label)\n",
    "            confusion_matrix[predicted][label]+=1\n",
    "            \n",
    "      \n",
    "            \n",
    "    cm=ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "    cm.plot()\n",
    "    print(confusion_matrix)\n",
    "    accuracy = 100*correct / 800 \n",
    "    average_loss = total_loss / 800\n",
    "    \n",
    "    model.train()\n",
    "    return accuracy, average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45cb4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_raw(model, dataset, batch_size, num_epochs, learning_rate, verbose = False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    counted_labels=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", content: \",inputs)\n",
    "                print(\"\\nlabels shape:\",labels.size(),\", content: \",labels)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if(verbose == True):\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "                print(\"List of labels until now:\",counted_labels)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                counted_labels[labels[index].item()]+=1\n",
    "                if(labels[index].item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "        \n",
    "        #calculate train loss and accuracy\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        train_loss_list.append(average_loss)\n",
    "        train_acc_list.append(average_accuracy)\n",
    "        \n",
    "        #calculate validation loss and accuracy\n",
    "        val_acc, val_loss = test_raw(model, validation_dataset, Y_validation)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],Train Loss: {average_loss:.4f}. Train Accuracy: {average_accuracy} Val Loss: {val_loss} Val Accuracy: {val_acc}\")\n",
    "        progress_bar.close()\n",
    "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c90b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 32, 12500]             288\n",
      "              ReLU-2            [-1, 32, 12500]               0\n",
      "         MaxPool1d-3             [-1, 32, 6250]               0\n",
      "       BatchNorm1d-4             [-1, 32, 6250]              64\n",
      "           Dropout-5             [-1, 32, 6250]               0\n",
      "            Conv1d-6              [-1, 64, 781]          16,448\n",
      "              ReLU-7              [-1, 64, 781]               0\n",
      "         MaxPool1d-8              [-1, 64, 390]               0\n",
      "       BatchNorm1d-9              [-1, 64, 390]             128\n",
      "          Dropout-10              [-1, 64, 390]               0\n",
      "           Linear-11                  [-1, 128]       3,195,008\n",
      "             ReLU-12                  [-1, 128]               0\n",
      "      BatchNorm1d-13                  [-1, 128]             256\n",
      "          Dropout-14                  [-1, 128]               0\n",
      "           Linear-15                   [-1, 32]           4,128\n",
      "             ReLU-16                   [-1, 32]               0\n",
      "      BatchNorm1d-17                   [-1, 32]              64\n",
      "          Dropout-18                   [-1, 32]               0\n",
      "           Linear-19                    [-1, 8]             264\n",
      "================================================================\n",
      "Total params: 3,216,648\n",
      "Trainable params: 3,216,648\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.76\n",
      "Forward/backward pass size (MB): 12.02\n",
      "Params size (MB): 12.27\n",
      "Estimated Total Size (MB): 25.05\n",
      "----------------------------------------------------------------\n",
      "NNet_Raw3(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(8,), stride=(16,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(8,), stride=(8,))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (batchnorm1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=24960, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MyModel=NNet_Raw3()\n",
    "summary(MyModel, (1,200000))\n",
    "print(MyModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab3f0168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be481bfa8bb94cc2b585c6171f0b435a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 80  90 140 120  50 230  90   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]]\n",
      "Epoch [1/10],Train Loss: 2.0277. Train Accuracy: 19.015625 Val Loss: 0.0 Val Accuracy: tensor([10.])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3785d4017c0d4ffaade914eb7b960fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [ 80  90 140 120  50 230  90   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]]\n",
      "Epoch [2/10],Train Loss: 1.9530. Train Accuracy: 18.78125 Val Loss: 0.0 Val Accuracy: tensor([28.7500])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005c29e5f4664479a3cfdbcf6f7b62b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 80  90 140 120  50 230  90   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]]\n",
      "Epoch [3/10],Train Loss: 1.9601. Train Accuracy: 18.828125 Val Loss: 0.0 Val Accuracy: tensor([10.])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6065f11bb5ff4740adc7556dcd4e2640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-6a022fbd3e8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-c6da235a227e>\u001b[0m in \u001b[0;36mtrain_raw\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#labels need to be a vector of class indexes (0-7) of dim (batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train_raw(MyModel, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08cc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871ba9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
