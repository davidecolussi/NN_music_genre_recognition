{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15945d26",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6eab989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tqdm.notebook as tq\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc028685",
   "metadata": {},
   "source": [
    "## Creation of training / validation / test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747fbede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio directory:  ./data/fma_small/\n",
      "Loading tracks.csv...\n",
      "small dataset shape: (8000, 52)\n"
     ]
    }
   ],
   "source": [
    "AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "print(\"audio directory: \",AUDIO_DIR)\n",
    "print(\"Loading tracks.csv...\")\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "#get only the small subset of the dataset\n",
    "small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "print(\"small dataset shape:\",small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af713974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since each track is split in clips, create a lable for each clip\n",
    "def expand_labels(labels_set,expand_factor):\n",
    "    # Repeat each element expand_factor times using np.repeat()\n",
    "    expanded_array = np.repeat(labels_set, expand_factor)\n",
    "    #convert to numpy array\n",
    "    expanded_array = np.array(expanded_array)\n",
    "    # return the expanded array variable\n",
    "    return expanded_array\n",
    "\n",
    "#transforms a vector of strings into a vector of integer following a dictionary\n",
    "def to_integer_vector(labels_vector, unique_genres):\n",
    "    #create dictionary genre-integer {'Rock':1, 'Pop', 2, ...}\n",
    "    dictionary = {}\n",
    "    i=1\n",
    "    for genre in unique_genres:\n",
    "        dictionary[genre]=i\n",
    "        i+=1\n",
    "    print(\"dictionary created:\",dictionary,\"\\n\")\n",
    "    output_vector = []\n",
    "    #using the dictionary, transform the label vector ['Rock','Pop',...] into a vector [1,2, ...]\n",
    "    for elem in labels_vector:\n",
    "        output_vector.append(dictionary[elem])        \n",
    "    return np.array(output_vector) # convert to numpy array and return the vector\n",
    "\n",
    "#transforms a vector of integers into a vector of one hot encoded label of dim (len(labels) x num_classes)\n",
    "def to_one_hot(labels_vector, num_classes):\n",
    "    output = []\n",
    "    print(\"Creating one hot encoded lables...\")\n",
    "    #cicle through all elements to be encoded\n",
    "    for elem in labels_vector:\n",
    "        one_hot = [0]*num_classes # [0, 0, ... 0]\n",
    "        one_hot[elem-1] = 1\n",
    "        output.append(one_hot)\n",
    "    return np.array(output) \n",
    "\n",
    "#the function which call all the other funcitons to generate the final one hot encoded label vector\n",
    "def generate_one_hot_encoded_labels(data, n_clips_per_track):\n",
    "    print(\"Number of clips per track used:\",n_clips_per_track)\n",
    "    labels = expand_labels(data, n_clips_per_track) #expand the labels by the number of clips per track amount\n",
    "    unique_genres = np.unique(labels)\n",
    "    print(\"There are {} unique genres:\".format(len(unique_genres)),unique_genres) #get a vector of integers labels [1,2,4,...]\n",
    "    integer_label_vector = to_integer_vector(labels,unique_genres) \n",
    "    labels_one_hot = to_one_hot(integer_label_vector, len(unique_genres)) #get one hot encoded vector of labels\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20b2b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 training tracks, 800 validation tracks, 800 testing tracks\n",
      "\n",
      "Number of clips per track used: 10\n",
      "There are 8 unique genres: ['Electronic' 'Experimental' 'Folk' 'Hip-Hop' 'Instrumental'\n",
      " 'International' 'Pop' 'Rock']\n",
      "dictionary created: {'Electronic': 1, 'Experimental': 2, 'Folk': 3, 'Hip-Hop': 4, 'Instrumental': 5, 'International': 6, 'Pop': 7, 'Rock': 8} \n",
      "\n",
      "Creating one hot encoded lables...\n",
      "Number of clips per track used: 10\n",
      "There are 8 unique genres: ['Electronic' 'Experimental' 'Folk' 'Hip-Hop' 'Instrumental'\n",
      " 'International' 'Pop' 'Rock']\n",
      "dictionary created: {'Electronic': 1, 'Experimental': 2, 'Folk': 3, 'Hip-Hop': 4, 'Instrumental': 5, 'International': 6, 'Pop': 7, 'Rock': 8} \n",
      "\n",
      "Creating one hot encoded lables...\n",
      "Number of clips per track used: 10\n",
      "There are 8 unique genres: ['Electronic' 'Experimental' 'Folk' 'Hip-Hop' 'Instrumental'\n",
      " 'International' 'Pop' 'Rock']\n",
      "dictionary created: {'Electronic': 1, 'Experimental': 2, 'Folk': 3, 'Hip-Hop': 4, 'Instrumental': 5, 'International': 6, 'Pop': 7, 'Rock': 8} \n",
      "\n",
      "Creating one hot encoded lables...\n",
      "Training labels vector: (64000, 8),\n",
      "Validation labels vector: (8000, 8),\n",
      "Test labels vector: (8000, 8)\n"
     ]
    }
   ],
   "source": [
    "#retrieve labels for each subset\n",
    "tr_labels = small.loc[small[('set', 'split')] == 'training', ('track', 'genre_top')].values\n",
    "vl_labels = small.loc[small[('set', 'split')] == 'validation', ('track', 'genre_top')].values\n",
    "ts_labels = small.loc[small[('set', 'split')] == 'test', ('track', 'genre_top')].values\n",
    "\n",
    "print('{} training tracks, {} validation tracks, {} testing tracks\\n'.format(*map(len, [tr_labels, vl_labels, ts_labels])))\n",
    "\n",
    "\n",
    "n_clips_per_track = 10 #number of clips per track\n",
    "\n",
    "tr_labels_one_hot = generate_one_hot_encoded_labels(tr_labels, n_clips_per_track)\n",
    "vl_labels_one_hot = generate_one_hot_encoded_labels(vl_labels, n_clips_per_track)\n",
    "ts_labels_one_hot = generate_one_hot_encoded_labels(ts_labels, n_clips_per_track)\n",
    "\n",
    "print('Training labels vector: {},\\nValidation labels vector: {},\\nTest labels vector: {}'.format(tr_labels_one_hot.shape, vl_labels_one_hot.shape, ts_labels_one_hot.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a52dc",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83f745b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513), stride=(1,513))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1), stride=(1,513))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1), stride=(1,513))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.dense2 = nn.Linear(300, 150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20175a8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "187ddbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=16 #number of samples taken at a time for the train\n",
    "\n",
    "# Define the custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list, labels):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns an item\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        \n",
    "        return stft_vector, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd576a3",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ca4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, num_epochs, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            #print(\"inputs shape:\",inputs.size(),\", content: \",inputs)\n",
    "            #print(\"labels shape:\",labels.size(),\", content: \",labels)\n",
    "            \n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            #print(\"inputs unsqueezed shape:\",inputs.size(),\", content: \",inputs)\n",
    "\n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            #print(\"\\noutputs type:\",type(outputs),\"content:\",outputs)\n",
    "            #print(\"\\nlabels type:\",type(labels),\"content:\",labels)\n",
    "\n",
    "            loss = criterion(outputs, labels.float()) #labels need to be a vector of float, not Long\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #rint(\"outputs shape:\",outputs.size(),\"content:\",outputs)\n",
    "            #print(\"labels shape:\",labels.size(),\"content:\",labels)\n",
    "\n",
    "            #calculate train accuracy\n",
    "            for idx, predicted_label in enumerate(outputs):\n",
    "                #print(\"predicted_label size:\",predicted_label.size(),\"content:\",predicted_label)\n",
    "                #print(\"labels[idx] size:\",labels[idx].size(),\"content:\",labels[idx])\n",
    "                max_idx = torch.argmax(predicted_label).item() #index with max argument in the one hot predicted label vector\n",
    "                #print(\"max_idx content:\",max_idx)\n",
    "\n",
    "                if(labels[idx][max_idx].item() == 1):\n",
    "                    correct += 1\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            #print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"Batch accuracy\": accuracy, \"Average accuracy\": average_accuracy})\n",
    "\n",
    "\n",
    "\n",
    "            #bps = (batch_idx + 1) / (time.time() - start_time)\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "       \n",
    "\n",
    "\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}. Accuracy: {average_accuracy}\")\n",
    "        progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef67f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset(filepaths):\n",
    "    error_indexes = []\n",
    "    progress = 0\n",
    "    for file in filepaths:\n",
    "        progress+=1\n",
    "        print(\"checked {}/{} files\".format(progress,len(filepaths)))\n",
    "        x = np.load(file)\n",
    "        if(x.shape != (128,513)):\n",
    "            error_indexes.append(x)\n",
    "            print(\"error\")\n",
    "    print(\"{} errors found in files: {}\".format(len(error_indexes),error_indexes))\n",
    "    for idx,error in enumerate(error_indexes):\n",
    "        print(\"index: {}, shape: {}\".format(idx,error.shape))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path=\"data/fma_small_stft/train/\"\n",
    "folder_path=\"data/fma_small_stft_transposed/train/\"\n",
    "file_list = os.listdir(folder_path)\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in file_list]\n",
    "print(file_paths)\n",
    "print(\"\\nNumber of training samples:\",len(file_paths),\"\\n\")\n",
    "verify_dataset(file_paths)\n",
    "train_dataset = MyDataset(file_paths, tr_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc7adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 125, 1]         262,784\n",
      "              ReLU-2          [-1, 128, 125, 1]               0\n",
      "         MaxPool2d-3           [-1, 128, 62, 1]               0\n",
      "            Conv2d-4           [-1, 128, 59, 1]          65,664\n",
      "              ReLU-5           [-1, 128, 59, 1]               0\n",
      "         MaxPool2d-6           [-1, 128, 29, 1]               0\n",
      "            Conv2d-7           [-1, 256, 26, 1]         131,328\n",
      "         AvgPool2d-8            [-1, 256, 1, 1]               0\n",
      "         MaxPool2d-9            [-1, 256, 1, 1]               0\n",
      "          Flatten-10                  [-1, 512]               0\n",
      "           Linear-11                  [-1, 300]         153,900\n",
      "             ReLU-12                  [-1, 300]               0\n",
      "           Linear-13                  [-1, 150]          45,150\n",
      "             ReLU-14                  [-1, 150]               0\n",
      "           Linear-15                    [-1, 8]           1,208\n",
      "          Softmax-16                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 660,034\n",
      "Trainable params: 660,034\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 0.51\n",
      "Params size (MB): 2.52\n",
      "Estimated Total Size (MB): 3.28\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4514a456c65947719518b642baa079ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NNet1()\n",
    "summary(model, (1, 128, 513))\n",
    "train(model, train_dataset, batch_size=256, num_epochs=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc08332",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e231a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom model class\n",
    "class NNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=(4, 513),padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=(4, 1),padding=\"same\")\n",
    "        self.conv3 = nn.Conv2d(256, 256, kernel_size=(4, 1),padding=\"same\")\n",
    "        self.dense1 = nn.Linear(256*125*1, 300)\n",
    "        self.dense2 = nn.Linear(300, 150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x1 = x  # Save the output of the first convolutional layer for later\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.ReLU()(self.conv3(x))\n",
    "        x += x1  # Sum the output of the first convolutional layer with the third convolutional layer\n",
    "        \n",
    "        x = torch.cat((nn.AvgPool2d(kernel_size=(125, 1))(x), nn.MaxPool2d(kernel_size=(125, 1))(x)), dim=1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.ReLU()(self.dense1(x))\n",
    "        x = nn.ReLU()(self.dense2(x))\n",
    "        x = nn.Softmax(dim=1)(self.dense3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda60597",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = NNet2()\n",
    "summary(model2, (1, 128, 513))\n",
    "train(model2, train_dataset, batch_size=256, num_epochs=8, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c443ec",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d3d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGenreClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(AudioGenreClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Time-distributed layer\n",
    "        self.time_distributed = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        self.bilstm = nn.LSTM(256, hidden_size, num_layers = num_layers, bidirectional=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Time-distributed layer\n",
    "        x = self.time_distributed(x)\n",
    "\n",
    "        # Bidirectional LSTM layers\n",
    "        output, _ = self.bilstm(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.softmax(self.attention(output), dim=1)\n",
    "        attended_output = torch.sum(attention_weights * output, dim=1)\n",
    "\n",
    "        # Pooling layer\n",
    "        pooled_output = self.pooling(attended_output.permute(0, 1).unsqueeze(2))\n",
    "        \n",
    "        # Reshape and pass through the output layer\n",
    "        pooled_output = pooled_output.squeeze(2)\n",
    "        output = self.fc(pooled_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7264b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRNN(model, dataset, batch_size, num_epochs, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \",device)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            #print(\"inputs shape:\",inputs.shape,\", content: \",inputs)\n",
    "            #print(\"labels shape:\",labels.shape,\", content: \",labels)\n",
    "            #inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            #print(\"\\noutputs type:\",type(outputs),\"content:\",outputs)\n",
    "            #print(\"\\nlabels type:\",type(labels),\"content:\",labels)\n",
    "\n",
    "            loss = criterion(outputs, labels.float()) #labels need to be a vector of float, not Long\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #print(\"outputs shape:\",outputs.size(),\"content:\",outputs)\n",
    "            #print(\"labels shape:\",labels.size(),\"content:\",labels)\n",
    "\n",
    "            #calculate train accuracy\n",
    "            for idx, predicted_label in enumerate(outputs):\n",
    "                #print(\"predicted_label size:\",predicted_label.size(),\"content:\",predicted_label)\n",
    "                #print(\"labels[idx] size:\",labels[idx].size(),\"content:\",labels[idx])\n",
    "                max_idx = torch.argmax(predicted_label).item() #index with max argument in the one hot predicted label vector\n",
    "                #print(\"max_idx content:\",max_idx)\n",
    "\n",
    "                if(labels[idx][max_idx].item() == 1):\n",
    "                    correct += 1\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            #print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"Batch accuracy\": accuracy, \"Average accuracy\": average_accuracy})\n",
    "\n",
    "\n",
    "\n",
    "            #bps = (batch_idx + 1) / (time.time() - start_time)\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "       \n",
    "\n",
    "\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}. Accuracy: {average_accuracy}\")\n",
    "        progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89cf6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ee7f7997804a5b8b153a57f96411c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fc5733ba26ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodelRNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioGenreClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m513\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-34f84ef57f6c>\u001b[0m in \u001b[0;36mtrainRNN\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#labels need to be a vector of float, not Long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fma3/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fma3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelRNN = AudioGenreClassifier(input_size=513, hidden_size=256, num_layers=5, num_classes=8)\n",
    "trainRNN(modelRNN, train_dataset, batch_size=32, num_epochs=8, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59461c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
