{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15945d26",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6eab989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tqdm.notebook as tq\n",
    "import utils\n",
    "from pydub import AudioSegment\n",
    "from tkinter import Tcl # file sorting by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c3e14",
   "metadata": {},
   "source": [
    "# Load STFT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8ded2",
   "metadata": {},
   "source": [
    "### Dictionary creation for the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c4013",
   "metadata": {},
   "source": [
    "We want a dictionary indicating a numbeer for each genre:\n",
    "\n",
    "{0: 'Hip-Hop', 1: 'Pop', 2: 'Folk', 3: 'Rock', 4: 'Experimental', 5: 'International', 6: 'Electronic', 7: 'Instrumental'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e985c",
   "metadata": {},
   "source": [
    "### Creation of the labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0be9951a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_folder: data/fma_small_stft_transposed_22050_overlapped/train\n",
      "validation_folder: data/fma_small_stft_transposed_22050_overlapped/validation\n",
      "test_folder: data/fma_small_stft_transposed_22050_overlapped/test \n",
      "\n",
      "audio directory:  ./data/fma_small/\n",
      "Loading tracks.csv...\n",
      "small dataset shape: (8000, 52)\n",
      "Track.csv: 6400 training samples, 800 validation samples, 800 test samples\n",
      "\n",
      "there are 8 unique genres\n",
      "Dictionary of genres created: {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Rock': 3, 'Experimental': 4, 'International': 5, 'Electronic': 6, 'Instrumental': 7}\n",
      "labels length: 127940\n",
      "labels length: 16000\n",
      "labels length: 16000\n"
     ]
    }
   ],
   "source": [
    "def create_single_dataset(folder_path, tracks_dataframe, genre_dictionary):    \n",
    "    labels = []\n",
    "   \n",
    "    _, file_list = get_sorted_file_paths(folder_path)\n",
    "    \n",
    "    for i,file in enumerate(file_list):\n",
    "        #print(\"considering file:\",file, \"({}/{})\".format(i,len(file_list)))\n",
    "        track_id_clip_id = file.split('.')[0]\n",
    "        track_id = track_id_clip_id.split('_')[0]\n",
    "        #print(\"track id with clip: {}, track id: {}\".format(track_id_clip_id, track_id))\n",
    "        genre = tracks_dataframe.loc[int(track_id)]\n",
    "        #print(\"genre from dataframe: \", genre)\n",
    "        label = genre_dictionary[genre]\n",
    "        #print(\"label from dictionary:\",label)\n",
    "        labels.append(label)\n",
    "    print(\"labels length: {}\".format(len(labels)))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "#create the train,validation and test vectors using the files in the train/validation/test folders\n",
    "def create_dataset_splitted(folder_path):\n",
    "    train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "    validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "    test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "    \n",
    "    print(\"train_folder:\",train_folder)\n",
    "    print(\"validation_folder:\",validation_folder)\n",
    "    print(\"test_folder:\",test_folder,\"\\n\")\n",
    "    \n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "    \n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "\n",
    "    small_training_top_genres = small_training['genre_top']\n",
    "    small_validation_top_genres = small_validation['genre_top']\n",
    "    small_test_top_genres = small_test['genre_top']\n",
    "    \n",
    "    #create dictionary of genre classes:\n",
    "    unique_genres = small_training_top_genres.unique()\n",
    "    unique_genres = np.array(unique_genres)\n",
    "    print(\"there are {} unique genres\".format(len(unique_genres)))\n",
    "    genre_dictionary = {}\n",
    "    for i,genre in enumerate(unique_genres):\n",
    "        genre_dictionary[genre] = i\n",
    "    print(\"Dictionary of genres created:\",genre_dictionary)\n",
    "    \n",
    "    \n",
    "    Y_train = create_single_dataset(train_folder, small_training_top_genres, genre_dictionary)\n",
    "    Y_validation = create_single_dataset(validation_folder, small_validation_top_genres, genre_dictionary)\n",
    "    Y_test = create_single_dataset(test_folder, small_test_top_genres, genre_dictionary)\n",
    "    \n",
    "    return Y_train, Y_validation, Y_test\n",
    " \n",
    "def get_sorted_file_paths(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #sort the dataset files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_list = Tcl().call('lsort', '-dict', file_list) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...\n",
    "    file_paths = [os.path.join(folder_path, file_name) for file_name in file_list] #join filename with folder path\n",
    "    #print(\"There are {} in the folder: {}\".format(len(file_list),file_list))\n",
    "    return file_paths, file_list\n",
    "    \n",
    "    \n",
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "Y_train, Y_validation, Y_test = create_dataset_splitted(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20175a8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fe67f",
   "metadata": {},
   "source": [
    "Class to load the STFT from files. Each file has a (128,513) matrix containing the STFT of a 3 seconds audio clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "187ddbe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cae70891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  127940\n",
      "len of validation dataset:  16000\n",
      "len of test dataset:  16000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDataset(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd9c68",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "We will use Z-Score to normalize the training, validation and test set by calculating the mean and the std deviation on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f571ada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_mean'\n",
    "std_save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_std_deviation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677263cf",
   "metadata": {},
   "source": [
    "## Calculation of mean and standard deviation (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5795abf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum: tensor(8.9296e+09)\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum=0\n",
    "\n",
    "#iter all the training set by batches and calculate the sum of all the sample values (513*128 values for each sample)\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum:\",current_sum)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "        current_sum += torch.sum(sample)\n",
    "        #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum:\",current_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c794bf2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of training set: tensor(1.0629)\n",
      "Saving the mean in file: ./data/fma_small_stft_transposed_22050_overlapped/train_mean\n"
     ]
    }
   ],
   "source": [
    "mean = current_sum/(len(train_dataset)*513*128) #divide the sum for the total number of values considerated\n",
    "print(\"mean of training set:\",mean)\n",
    "\n",
    "print(\"Saving the mean in file:\",save_filename) \n",
    "np.save(save_filename,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec52eb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum of squares: tensor(7.8299e+10)\n"
     ]
    }
   ],
   "source": [
    "#now let's calculate the standard deviation (squared root of the variance)\n",
    "\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum_of_squares = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum_of_squares:\",current_sum_of_squares)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample shape\",sample.shape)\n",
    "        for row in sample:\n",
    "            #print(\"row shape:\",row.shape)\n",
    "            for elem in row:\n",
    "                #print(\"elem: shape\",elem.shape,\"content:\",elem)\n",
    "                difference = elem - mean\n",
    "                difference_squared = difference**2\n",
    "                current_sum_of_squares += difference_squared\n",
    "                #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum of squares:\",current_sum_of_squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cecfb0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_sum_of_squares' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c28b8f122248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_sum_of_squares\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m513\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstd_deviation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'current_sum_of_squares' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "variance = current_sum_of_squares/((len(train_dataset) * 513 * 128)-1)\n",
    "std_deviation = math.sqrt(variance)\n",
    "\n",
    "print(\"variance:\",variance)\n",
    "print(\"std_deviation:\",std_deviation)\n",
    "\n",
    "print(\"Saving the std_deviation in file:\",std_save_filename) \n",
    "np.save(std_save_filename,std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fc2ab",
   "metadata": {},
   "source": [
    "## Load calculated mean and std deviation from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "977c1ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean: 1.0629134\n",
      "loaded std: 3.0528938510507846\n"
     ]
    }
   ],
   "source": [
    "loaded_mean = np.load(save_filename+'.npy')\n",
    "print(\"loaded mean:\",loaded_mean)\n",
    "\n",
    "loaded_std = np.load(std_save_filename+'.npy')\n",
    "print(\"loaded std:\",loaded_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d4804",
   "metadata": {},
   "source": [
    "## Create the normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "092efbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean= loaded_mean, std= loaded_std)\n",
    "])\n",
    "\n",
    "train_dataset = MyDataset(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test,  transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a52dc",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f745b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.bn4 = nn.BatchNorm1d(300)\n",
    "        self.dense2 = nn.Linear(300,150)\n",
    "        self.bn5 = nn.BatchNorm1d(150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8734822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet2, self).__init__()\n",
    "        \n",
    "        self.drop=nn.Dropout(0.2)\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513))\n",
    "        self.batch1=nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128,128, kernel_size=(4, 1),padding=1)\n",
    "        self.batch2=nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=(4, 1),padding=2)\n",
    "        self.batch3=nn.BatchNorm2d(128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(128,1))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(128,1))\n",
    "        self.fc1 = nn.Linear(250,128)\n",
    "        self.bn1=nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.bn2=nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.batch1(x)\n",
    "        x = torch.relu(x)\n",
    "        y = x\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch3(x)\n",
    "        x = torch.relu(x)\n",
    "        # Sum between the first and third conv layers\n",
    "        x = x[:, :, :, 0] + y[:, :, :, 0]\n",
    "        \n",
    "        x = torch.relu(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)        \n",
    "        x = self.drop(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845a53e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d034d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "LEARNING_RATE=0.0001\n",
    "\n",
    "learning_rate_list = [0.0001,0.00001]\n",
    "batch_size_list = [128,256,512]\n",
    "reg_list=[0.001,0.0001,0.00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd576a3",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "223742e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model, validation_dataset, Y_validation):\n",
    "    # Stop parameters learning\n",
    "    model.eval()\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    #confusion_matrix = np.zeros((8, 8), dtype=int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample, label in validation_loader:\n",
    "            \n",
    "            sample = sample.unsqueeze(1)\n",
    "\n",
    "            # Predict label\n",
    "            output = model(sample)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            max_index = torch.argmax(output).item()  # The index with maximum probability\n",
    "\n",
    "            #confusion_matrix[label][max_index] += 1\n",
    "\n",
    "            correct += (max_index == label)\n",
    "\n",
    "    #cm = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "    #cm.plot()\n",
    "    #print(confusion_matrix)\n",
    "    accuracy = 100 * correct / len(Y_validation)\n",
    "    average_loss = total_loss / len(Y_validation)\n",
    "\n",
    "    model.train()\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0ca4e98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, num_epochs, learning_rate, verbose = False, RGB=False, reg=1e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    counted_labels=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", dtype:\",inputs.dtype,\" content: \",inputs)\n",
    "                print(\"min value:\",torch.min(inputs))\n",
    "                print(\"max value:\",torch.max(inputs))\n",
    "                print(\"\\nlabels shape:\",labels.size(),\",dtype:\",labels.dtype,\", content: \",labels)\n",
    "            if(RGB==False):\n",
    "                inputs = inputs.unsqueeze(1) #add a dimension if input is to be considered just grayscale\n",
    "                #if input is RGB, there are already 3 channels\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if(verbose == True):\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "                print(\"List of labels until now:\",counted_labels)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                counted_labels[labels[index].item()]+=1\n",
    "                if(labels[index].item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "        \n",
    "        #calculate train loss and accuracy\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        train_loss_list.append(average_loss)\n",
    "        train_acc_list.append(average_accuracy)\n",
    "        \n",
    "        #calculate validation loss and accuracy\n",
    "        val_acc, val_loss = test(model, validation_dataset, Y_validation)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],Train Loss: {average_loss:.4f}. Train Accuracy: {average_accuracy} Val Loss: {val_loss} Val Accuracy: {val_acc}\")\n",
    "        progress_bar.close()\n",
    "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef7fab",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1 + BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3685ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1_Small(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1_Small, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(2, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv2 = nn.Conv2d(64,128, kernel_size=(2, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv3 = nn.Conv2d(128,64, kernel_size=(4, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(2, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(256, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dense2 = nn.Linear(64, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61b169d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 127, 1]          65,728\n",
      "       BatchNorm2d-2           [-1, 64, 127, 1]             128\n",
      "              ReLU-3           [-1, 64, 127, 1]               0\n",
      "         MaxPool2d-4            [-1, 64, 31, 1]               0\n",
      "            Conv2d-5           [-1, 128, 30, 1]          16,512\n",
      "       BatchNorm2d-6           [-1, 128, 30, 1]             256\n",
      "              ReLU-7           [-1, 128, 30, 1]               0\n",
      "         MaxPool2d-8            [-1, 128, 7, 1]               0\n",
      "            Conv2d-9             [-1, 64, 4, 1]          32,832\n",
      "      BatchNorm2d-10             [-1, 64, 4, 1]             128\n",
      "        AvgPool2d-11             [-1, 64, 2, 1]               0\n",
      "        MaxPool2d-12             [-1, 64, 2, 1]               0\n",
      "          Flatten-13                  [-1, 256]               0\n",
      "           Linear-14                   [-1, 64]          16,448\n",
      "          Dropout-15                   [-1, 64]               0\n",
      "      BatchNorm1d-16                   [-1, 64]             128\n",
      "             ReLU-17                   [-1, 64]               0\n",
      "           Linear-18                    [-1, 8]             520\n",
      "          Softmax-19                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 132,680\n",
      "Trainable params: 132,680\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 0.31\n",
      "Params size (MB): 0.51\n",
      "Estimated Total Size (MB): 1.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_NNet1_Small = NNet1_Small()\n",
    "summary(model_NNet1_Small, (1, 128, 513))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ac93d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/NNet1_Small/lr_00001_reg_00001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ec5a34ec7a41cb954ffd3ab387fa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],Train Loss: 1.9128. Train Accuracy: 40.0125 Val Loss: 1.8593959403708578 Val Accuracy: tensor([45.8375])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18431f0347c241b396da79001e3ad217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10],Train Loss: 1.7943. Train Accuracy: 50.90078125 Val Loss: 1.8144761948511003 Val Accuracy: tensor([47.6750])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d2aac2dcf6445ab5767fbaf73149b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10],Train Loss: 1.7451. Train Accuracy: 54.8390625 Val Loss: 1.7945972227230669 Val Accuracy: tensor([48.6312])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e803478695754141ba5feedc0356ec9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10],Train Loss: 1.7185. Train Accuracy: 56.94296875 Val Loss: 1.7790009344667197 Val Accuracy: tensor([49.8688])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac1940ed41743b6b8cae6f47742c8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10],Train Loss: 1.7011. Train Accuracy: 58.36640625 Val Loss: 1.7896004073768854 Val Accuracy: tensor([48.6375])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da175630fa1e4a33b9f8dfe1bb9fdb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10],Train Loss: 1.6879. Train Accuracy: 59.5078125 Val Loss: 1.779199239976704 Val Accuracy: tensor([49.3750])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828f15834bad4958853aac2e08982bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10],Train Loss: 1.6773. Train Accuracy: 60.453125 Val Loss: 1.7810167167559265 Val Accuracy: tensor([48.8063])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fbdafe50b34f439de5f98f8f8e4d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10],Train Loss: 1.6657. Train Accuracy: 61.46953125 Val Loss: 1.8000839565470814 Val Accuracy: tensor([47.2313])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e605e0ff2204edb82f629dc3081782a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10],Train Loss: 1.6564. Train Accuracy: 62.5 Val Loss: 1.7685952556654811 Val Accuracy: tensor([50.5125])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc075b532a34569b0a3ae323f7fad3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10],Train Loss: 1.6467. Train Accuracy: 63.62109375 Val Loss: 1.7865395892858504 Val Accuracy: tensor([48.4625])\n",
      "Trained with learning rate= 0.0001  and with regularization term= 0.0001\n",
      "Loss: [1.8593959403708578, 1.8144761948511003, 1.7945972227230669, 1.7790009344667197, 1.7896004073768854, 1.779199239976704, 1.7810167167559265, 1.8000839565470814, 1.7685952556654811, 1.7865395892858504]\n",
      "Accuracy: [tensor([45.8375]), tensor([47.6750]), tensor([48.6312]), tensor([49.8688]), tensor([48.6375]), tensor([49.3750]), tensor([48.8063]), tensor([47.2313]), tensor([50.5125]), tensor([48.4625])]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Add batch_size optimization loop\n",
    "save_directory=\"./results/NNet1_Small/\"\n",
    "\n",
    "lr_list= [ 0.0001]\n",
    "r_list=[0.0001]\n",
    "\n",
    "for i in lr_list:\n",
    "    for j in r_list:\n",
    "        if(j!=1e-5 and j!=1e-6):\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"0\"+str(j).split(\".\")[1]\n",
    "        elif(j==1e-5):\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"00001\"\n",
    "        else:\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"000001\"\n",
    "        print(filename)\n",
    "        model = NNet1_Small()\n",
    "        train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=128, num_epochs=10, learning_rate=i, reg=j)\n",
    "        print(\"Trained with learning rate=\",i,\" and with regularization term=\",j)\n",
    "        print(\"Loss:\",val_loss_list)\n",
    "        print(\"Accuracy:\",val_acc_list)\n",
    "        save_values=[val_loss_list,val_acc_list]\n",
    "        np.savetxt(filename,save_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f9acda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./best_models/results/NNet1_Small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4e50d7cba247878996f440f6f69019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],Train Loss: 1.8076. Train Accuracy: 46.7859375 Val Loss: 1.815145688265562 Val Accuracy: tensor([44.8750])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcb5c8483dd400588fcb0f57796dc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10],Train Loss: 1.7305. Train Accuracy: 54.23203125 Val Loss: 1.8216929790973664 Val Accuracy: tensor([44.4750])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30e1b2d9ec14c28b9e56e1769d7d881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10],Train Loss: 1.7002. Train Accuracy: 57.21328125 Val Loss: 1.777285971775651 Val Accuracy: tensor([49.3688])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c656fa70b815457fb72164ada8199ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10],Train Loss: 1.6824. Train Accuracy: 58.903125 Val Loss: 1.7788502685800194 Val Accuracy: tensor([48.8625])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ce0c0b233241f7898ed14c71a7141c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10],Train Loss: 1.6681. Train Accuracy: 60.378125 Val Loss: 1.8065976762101055 Val Accuracy: tensor([46.3187])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164c1ed705d5409c95bd6c0ab9285f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10],Train Loss: 1.6567. Train Accuracy: 61.48125 Val Loss: 1.7716257748901845 Val Accuracy: tensor([49.6875])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ffdaefcdf24f988692391426f39ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10],Train Loss: 1.6456. Train Accuracy: 62.65234375 Val Loss: 1.7790606408789753 Val Accuracy: tensor([48.9062])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd17aef0a5e4619b84f09b84f57835c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10],Train Loss: 1.6398. Train Accuracy: 63.2359375 Val Loss: 1.8199008042141795 Val Accuracy: tensor([44.9313])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0718098a9704b8693557bf138f4fb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10],Train Loss: 1.6301. Train Accuracy: 64.175 Val Loss: 1.7777148803249 Val Accuracy: tensor([49.1500])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3906b5cedc4e03ae8abfaab5b37e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10],Train Loss: 1.6235. Train Accuracy: 64.859375 Val Loss: 1.7791675435304641 Val Accuracy: tensor([48.9125])\n",
      "Trained with learning rate= 0.001  and with regularization term= 1e-05\n",
      "Loss: [1.815145688265562, 1.8216929790973664, 1.777285971775651, 1.7788502685800194, 1.8065976762101055, 1.7716257748901845, 1.7790606408789753, 1.8199008042141795, 1.7777148803249, 1.7791675435304641]\n",
      "Accuracy: [tensor([44.8750]), tensor([44.4750]), tensor([49.3688]), tensor([48.8625]), tensor([46.3187]), tensor([49.6875]), tensor([48.9062]), tensor([44.9313]), tensor([49.1500]), tensor([48.9125])]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Add batch_size optimization loop\n",
    "save_directory=\"./best_models/\"\n",
    "\n",
    "learning_rate_list = [0.001]\n",
    "batch_size_list = [128,256,512]\n",
    "reg_list=[0.00001]\n",
    "\n",
    "for i in learning_rate_list:\n",
    "    for j in reg_list:\n",
    "        filename=save_directory+\"results/NNet1_Small\"        \n",
    "        print(filename)\n",
    "        model = NNet1_Small()\n",
    "        train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=64, num_epochs=10, learning_rate=i, reg=j)\n",
    "        print(\"Trained with learning rate=\",i,\" and with regularization term=\",j)\n",
    "        print(\"Loss:\",val_loss_list)\n",
    "        print(\"Accuracy:\",val_acc_list)\n",
    "        save_values=[ train_loss_list, train_acc_list, val_loss_list,val_acc_list]\n",
    "        np.savetxt(filename,save_values)\n",
    "        torch.save(model.state_dict(), save_directory+\"models/NNet1_Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ff00810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/NNet1/lr_0001_reg_00001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b0a9b6a82f490e9c63a79700d338e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],Train Loss: 1.7846. Train Accuracy: 48.8125 Val Loss: 1.8186630289033054 Val Accuracy: tensor([45.0438])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f01d2d7ee24921b32583c560e0f7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10],Train Loss: 1.7078. Train Accuracy: 56.38828125 Val Loss: 1.7965274090394379 Val Accuracy: tensor([47.3312])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cd3affa33948838f3b14406ba0e197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10],Train Loss: 1.6791. Train Accuracy: 59.2859375 Val Loss: 1.8315542057305574 Val Accuracy: tensor([43.2938])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0926537ca644e78f66ba9e3cf341a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10],Train Loss: 1.6581. Train Accuracy: 61.3921875 Val Loss: 1.7864047286510467 Val Accuracy: tensor([48.2188])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6790123a80478594c541c98c16924f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10],Train Loss: 1.6425. Train Accuracy: 62.96328125 Val Loss: 1.8000281608402728 Val Accuracy: tensor([46.9125])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0305ffe3a2cb49678dbe214e7686a91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10],Train Loss: 1.6282. Train Accuracy: 64.45703125 Val Loss: 1.800094104759395 Val Accuracy: tensor([46.9250])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8581ea5bc843fdb68d808fe01996ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10],Train Loss: 1.6155. Train Accuracy: 65.6921875 Val Loss: 1.791915791131556 Val Accuracy: tensor([47.6937])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a81c0292a164775ada6c8dc0ce3ea21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10],Train Loss: 1.6059. Train Accuracy: 66.7046875 Val Loss: 1.795549453318119 Val Accuracy: tensor([47.0687])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb693f13c6d84104afe0a692ec5d5456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10],Train Loss: 1.5951. Train Accuracy: 67.81796875 Val Loss: 1.8151078140363097 Val Accuracy: tensor([45.2000])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93db689b5014c84b974a26a379ccd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10],Train Loss: 1.5874. Train Accuracy: 68.5265625 Val Loss: 1.817907063394785 Val Accuracy: tensor([45.0563])\n",
      "Trained with learning rate= 0.001  and with regularization term= 0.0001\n",
      "Loss: [1.8186630289033054, 1.7965274090394379, 1.8315542057305574, 1.7864047286510467, 1.8000281608402728, 1.800094104759395, 1.791915791131556, 1.795549453318119, 1.8151078140363097, 1.817907063394785]\n",
      "Accuracy: [tensor([45.0438]), tensor([47.3312]), tensor([43.2938]), tensor([48.2188]), tensor([46.9125]), tensor([46.9250]), tensor([47.6937]), tensor([47.0687]), tensor([45.2000]), tensor([45.0563])]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Add batch_size optimization loop\n",
    "save_directory=\"./results/NNet1/\"\n",
    "\n",
    "lr_list= [ 0.001]\n",
    "r_list=[0.0001]\n",
    "\n",
    "for i in lr_list:\n",
    "    for j in r_list:\n",
    "        if(j!=1e-5 and j!=1e-6):\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"0\"+str(j).split(\".\")[1]\n",
    "        elif(j==1e-5):\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"000001\"\n",
    "        else:\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"0000001\"\n",
    "        print(filename)\n",
    "        model = NNet1()\n",
    "        train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=128, num_epochs=10, learning_rate=i, reg=j)\n",
    "        print(\"Trained with learning rate=\",i,\" and with regularization term=\",j)\n",
    "        print(\"Loss:\",val_loss_list)\n",
    "        print(\"Accuracy:\",val_acc_list)\n",
    "        save_values=[val_loss_list,val_acc_list]\n",
    "        np.savetxt(filename,save_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "563b26ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/NNet2/lr_0001_reg_000001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9901a1650bbb472fb1c337476dd7cbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],Train Loss: 1.9199. Train Accuracy: 34.6078125 Val Loss: 1.9418242857679724 Val Accuracy: tensor([30.8687])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8329b876f121413191bee96db220d317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10],Train Loss: 1.8650. Train Accuracy: 40.284375 Val Loss: 1.9395233458429575 Val Accuracy: tensor([32.0500])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f830ad386594d13b45b461eaa7079b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10],Train Loss: 1.8494. Train Accuracy: 41.84609375 Val Loss: 1.9370984275490046 Val Accuracy: tensor([32.5500])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cc8623143149bf9f7baff5835f248b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10],Train Loss: 1.8395. Train Accuracy: 42.89453125 Val Loss: 1.92977271284163 Val Accuracy: tensor([33.4000])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff727ee1a41c4b848121e4c77bb8d6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10],Train Loss: 1.8325. Train Accuracy: 43.5859375 Val Loss: 1.925821688786149 Val Accuracy: tensor([33.7625])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896271fbba594f7690c0e2edd1b8b33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10],Train Loss: 1.8262. Train Accuracy: 44.16640625 Val Loss: 1.9328955449312926 Val Accuracy: tensor([32.8125])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e971681e027548eb833f7928baef8726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10],Train Loss: 1.8205. Train Accuracy: 44.8453125 Val Loss: 1.928550441160798 Val Accuracy: tensor([33.3563])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b694a9b31072495598c7ff9bc84c3388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10],Train Loss: 1.8164. Train Accuracy: 45.246875 Val Loss: 1.9409718203023076 Val Accuracy: tensor([32.3875])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507ab350b61249b299c4831363d98bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10],Train Loss: 1.8107. Train Accuracy: 45.8328125 Val Loss: 1.933287391282618 Val Accuracy: tensor([32.9688])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20592b8b1dc483bb0853ab0f1b15ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10],Train Loss: 1.8072. Train Accuracy: 46.17734375 Val Loss: 1.915308596238494 Val Accuracy: tensor([35.0125])\n",
      "Trained with learning rate= 0.001  and with regularization term= 1e-05\n",
      "Loss: [1.9418242857679724, 1.9395233458429575, 1.9370984275490046, 1.92977271284163, 1.925821688786149, 1.9328955449312926, 1.928550441160798, 1.9409718203023076, 1.933287391282618, 1.915308596238494]\n",
      "Accuracy: [tensor([30.8687]), tensor([32.0500]), tensor([32.5500]), tensor([33.4000]), tensor([33.7625]), tensor([32.8125]), tensor([33.3563]), tensor([32.3875]), tensor([32.9688]), tensor([35.0125])]\n",
      "./results/NNet2/lr_0001_reg_0000001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6da9f0f9444e45bf2a4343de798bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],Train Loss: 1.9162. Train Accuracy: 34.9390625 Val Loss: 1.931972404450178 Val Accuracy: tensor([32.5750])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2ea6802e0645ac9ab6002faa70dd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10],Train Loss: 1.8648. Train Accuracy: 40.1859375 Val Loss: 1.924639566987753 Val Accuracy: tensor([32.9813])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50087465682d4c849ad03d4d5760163e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10],Train Loss: 1.8509. Train Accuracy: 41.6546875 Val Loss: 1.9423593048751355 Val Accuracy: tensor([31.5063])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b023513d4b3648b78eb78382d2e44d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10],Train Loss: 1.8413. Train Accuracy: 42.5703125 Val Loss: 1.9347730785906314 Val Accuracy: tensor([32.7875])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d6e6ca48f9454fb0659b5b05a4e6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10],Train Loss: 1.8351. Train Accuracy: 43.20078125 Val Loss: 1.9543162247166037 Val Accuracy: tensor([31.0562])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d05c75cf6d468ba17b9456905c13a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10],Train Loss: 1.8287. Train Accuracy: 43.84921875 Val Loss: 1.9442311258390546 Val Accuracy: tensor([32.0250])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b13c08a3b44c20a9a5d5690175b46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10],Train Loss: 1.8245. Train Accuracy: 44.3 Val Loss: 1.9336775086149574 Val Accuracy: tensor([33.0375])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96217b302ee451698d6e236fe790558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10],Train Loss: 1.8198. Train Accuracy: 44.71875 Val Loss: 1.9319837843030692 Val Accuracy: tensor([32.9188])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4af611fafc54f52b503bee2a0eb06ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10],Train Loss: 1.8143. Train Accuracy: 45.28046875 Val Loss: 1.9393858048543333 Val Accuracy: tensor([32.5563])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3e19a01d214be581a5e30a7d00b992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10],Train Loss: 1.8107. Train Accuracy: 45.72734375 Val Loss: 1.9426354933977128 Val Accuracy: tensor([32.1813])\n",
      "Trained with learning rate= 0.001  and with regularization term= 1e-06\n",
      "Loss: [1.931972404450178, 1.924639566987753, 1.9423593048751355, 1.9347730785906314, 1.9543162247166037, 1.9442311258390546, 1.9336775086149574, 1.9319837843030692, 1.9393858048543333, 1.9426354933977128]\n",
      "Accuracy: [tensor([32.5750]), tensor([32.9813]), tensor([31.5063]), tensor([32.7875]), tensor([31.0562]), tensor([32.0250]), tensor([33.0375]), tensor([32.9188]), tensor([32.5563]), tensor([32.1813])]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Add batch_size optimization loop\n",
    "save_directory=\"./results/NNet2/\"\n",
    "\n",
    "learning_rate_list = [0.001]\n",
    "batch_size_list = [128,256,512]\n",
    "reg_list=[0.00001,0.000001]\n",
    "\n",
    "for i in learning_rate_list:\n",
    "    for j in reg_list:\n",
    "        if(j!=1e-5 and j!=1e-6):\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"0\"+str(j).split(\".\")[1]\n",
    "        elif(j==1e-5):\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"000001\"\n",
    "        else:\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"0000001\"\n",
    "\n",
    "       \n",
    "        print(filename)\n",
    "        model = NNet2()\n",
    "        train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=128, num_epochs=10, learning_rate=i, reg=j)\n",
    "        print(\"Trained with learning rate=\",i,\" and with regularization term=\",j)\n",
    "        print(\"Loss:\",val_loss_list)\n",
    "        print(\"Accuracy:\",val_acc_list)\n",
    "        save_values=[val_loss_list,val_acc_list]\n",
    "        np.savetxt(filename,save_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343371e",
   "metadata": {},
   "source": [
    "# Raw audio (1D)\n",
    "\n",
    "In the following section we'll implement a Neural Network based on the classification of the raw audio. The raw audio is an array containing the amplitude of the audio wave for each sample of a 3 second clip (We used a sampling rate of 22050 Hz). Each entry has a size of (1,66150)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabb348",
   "metadata": {},
   "source": [
    "## Raw audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30092ba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyDatasetRaw(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose=False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        raw_vector = np.load(file_path).astype(np.int16) # Ensure int16 data type\n",
    "        if(self.verbose==True):\n",
    "            print(\"raw vector shape:\",raw_vector.shape)\n",
    "        raw_vector = torch.tensor(raw_vector)\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            \n",
    "            #convert to float64 tensor\n",
    "            raw_vector = raw_vector.double()\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.unsqueeze(raw_vector, dim=0)\n",
    "            #print(\"TRANSFORM: after first unsqueeze:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.unsqueeze(raw_vector, dim=0) #unsqueeze two times (needed for torchvision normalize method)\n",
    "            #print(\"TRANSFORM: after second unsqueeze:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = self.transform(raw_vector) #normalize the sample\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.squeeze(raw_vector, dim=0)\n",
    "            raw_vector = torch.squeeze(raw_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after double squeeze shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "        \n",
    "        return raw_vector, label        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de69e8",
   "metadata": {},
   "source": [
    "# Normalization of raw audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d390039",
   "metadata": {},
   "source": [
    "We calculate mean and std deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1ec8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_save_filename_raw = './data/fma_small_raw_array_22050_overlapped/train_mean'\n",
    "std_save_filename_raw = './data/fma_small_raw_array_22050_overlapped/train_std_deviation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b38ea",
   "metadata": {},
   "source": [
    "# Calculation of mean raw (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba507d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDatasetRaw(file_paths_train, Y_train)\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum=0\n",
    "\n",
    "#iter all the training set by batches and calculate the sum of all the sample values (513*128 values for each sample)\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if(batch_idx%1000==0):\n",
    "        print(\"batch\",batch_idx,\"/\",total_n_batches,\"(\",round((batch_idx/len(train_dataset)*100)),\"%), current_sum:\",current_sum)\n",
    "    \n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "        current_sum += torch.sum(sample)\n",
    "       \n",
    "        #print(\"type of current_sum:\",current_sum.dtype)\n",
    "        #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum:\",current_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"current_sum\",current_sum)\n",
    "mean_raw = current_sum/(len(train_dataset)*66150) #divide the sum for the total number of values considerated\n",
    "print(\"mean of training set:\",mean_raw)\n",
    "\n",
    "print(\"Saving the mean in file:\",mean_save_filename_raw) \n",
    "np.save(mean_save_filename_raw,mean_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcee895",
   "metadata": {},
   "source": [
    "# Calculation of std deviation raw (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's calculate the standard deviation (squared root of the variance)\n",
    "\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum_of_squares = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if(batch_idx%1000==0):\n",
    "        print(\"batch\",batch_idx,\"/\",total_n_batches,round((batch_idx/len(train_dataset)*100)),\"%, current_sum_of_squares:\",current_sum_of_squares)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\\n\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for elem in inputs:\n",
    "        elem = elem.double() #convert to float64 for precise calculations\n",
    "        #print(\"elem: shape\",elem.shape,\"content:\\n\",elem)\n",
    "        difference = elem - mean_raw\n",
    "        #print(\"difference: shape:\",difference.shape,\"content:\\n\", difference)\n",
    "        difference_squared = difference**2\n",
    "        #print(\"difference_squared: shape:\",difference_squared.shape,\"content:\\n\", difference_squared)\n",
    "        current_sum_of_squares += torch.sum(difference_squared)\n",
    "        #print(\"current_sum_of_squares:\",current_sum_of_squares)\n",
    "print(\"final sum of squares:\",current_sum_of_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "variance_raw = current_sum_of_squares/((len(train_dataset)*66150)-1)\n",
    "std_deviation_raw = math.sqrt(variance)\n",
    "\n",
    "print(\"variance raw:\",variance)\n",
    "print(\"std_deviation_raw:\",std_deviation_raw)\n",
    "\n",
    "print(\"Saving the std_deviation_raw in file:\",std_save_filename_raw) \n",
    "np.save(std_save_filename_raw,std_deviation_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cb8ee",
   "metadata": {},
   "source": [
    "# Load mean and std from file (fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5d39f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean: -16.984083\n",
      "loaded std: 1032.9510216986293\n"
     ]
    }
   ],
   "source": [
    "loaded_mean_raw = np.load(mean_save_filename_raw+'.npy')\n",
    "print(\"loaded mean:\",loaded_mean_raw)\n",
    "\n",
    "loaded_std_raw = np.load(std_save_filename_raw+'.npy')\n",
    "print(\"loaded std:\",loaded_std_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7112c8",
   "metadata": {},
   "source": [
    "## Create dataset for raw audio\n",
    "\n",
    "The labels are the same as the STFT dataset (in the same order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45f5f209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  127940\n",
      "len of validation dataset:  16000\n",
      "len of test dataset:  16000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_raw_array_22050_overlapped\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDatasetRaw(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDatasetRaw(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDatasetRaw(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da518866",
   "metadata": {},
   "source": [
    "# Neural Network Architecture for raw audio\n",
    "\n",
    "We implemented a lightweight CNN to classify the samples with their raw audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68b4a333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet_Raw(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(NNet_Raw, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=16)\n",
    "        self.conv2 = nn.Conv1d(32, 8, kernel_size=16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=32)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=8)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(8)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(24)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(248, 24)\n",
    "        self.fc3 = nn.Linear(24, 8)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(x.float())\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x=self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm2(x)     \n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5145a19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./best_models/results/NNet_Raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaad310a637e4afe944e43d31a36e8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],Train Loss: 1.9130. Train Accuracy: 36.08515625 Val Loss: 1.9141093373969198 Val Accuracy: tensor([34.8312])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400643342b5b47b9bcc5a9405ae6e828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10],Train Loss: 1.8659. Train Accuracy: 40.35859375 Val Loss: 1.8956503313481807 Val Accuracy: tensor([37.2125])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7030e6f0684f6e9421ddcbac5a39d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10],Train Loss: 1.8495. Train Accuracy: 41.93671875 Val Loss: 1.8685628568083048 Val Accuracy: tensor([39.6875])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b20f8155b994dd5a1bc0d6217825176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10],Train Loss: 1.8408. Train Accuracy: 42.78125 Val Loss: 1.873325175538659 Val Accuracy: tensor([39.0438])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d585015dd1224498a63b2054882c8cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10],Train Loss: 1.8347. Train Accuracy: 43.46796875 Val Loss: 1.8723637982606889 Val Accuracy: tensor([39.3937])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb9a2c63f754da9a40c53775cedc5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10],Train Loss: 1.8309. Train Accuracy: 43.7421875 Val Loss: 1.8777673560529948 Val Accuracy: tensor([38.9188])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8475f3849174b28b7f7ea52885feb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10],Train Loss: 1.8263. Train Accuracy: 44.21328125 Val Loss: 1.855006039455533 Val Accuracy: tensor([41.2812])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e216606205f408f80f14848a690606a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10],Train Loss: 1.8224. Train Accuracy: 44.6875 Val Loss: 1.8739052537232637 Val Accuracy: tensor([38.9688])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0267e68c5b4478936e9f4dde20e181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10],Train Loss: 1.8179. Train Accuracy: 45.11796875 Val Loss: 1.8722335183471441 Val Accuracy: tensor([39.2125])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fa48a3e578444ebaab0b2ce99fef55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10],Train Loss: 1.8131. Train Accuracy: 45.55546875 Val Loss: 1.8769738131389022 Val Accuracy: tensor([38.6000])\n",
      "Trained with learning rate= 0.001  and with regularization term= 0.0001\n",
      "Loss: [1.9141093373969198, 1.8956503313481807, 1.8685628568083048, 1.873325175538659, 1.8723637982606889, 1.8777673560529948, 1.855006039455533, 1.8739052537232637, 1.8722335183471441, 1.8769738131389022]\n",
      "Accuracy: [tensor([34.8312]), tensor([37.2125]), tensor([39.6875]), tensor([39.0438]), tensor([39.3937]), tensor([38.9188]), tensor([41.2812]), tensor([38.9688]), tensor([39.2125]), tensor([38.6000])]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Add batch_size optimization loop\n",
    "save_directory=\"./best_models/\"\n",
    "\n",
    "learning_rate_list = [0.001]\n",
    "batch_size_list = [128,256,512]\n",
    "reg_list=[0.0001]\n",
    "\n",
    "for i in learning_rate_list:\n",
    "    for j in reg_list:\n",
    "        filename=save_directory+\"results/NNet_Raw\"        \n",
    "        print(filename)\n",
    "        model = NNet_Raw()\n",
    "        train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=128, num_epochs=10, learning_rate=i, reg=j)\n",
    "        print(\"Trained with learning rate=\",i,\" and with regularization term=\",j)\n",
    "        print(\"Loss:\",val_loss_list)\n",
    "        print(\"Accuracy:\",val_acc_list)\n",
    "        save_values=[ train_loss_list, train_acc_list, val_loss_list,val_acc_list]\n",
    "        np.savetxt(filename,save_values)\n",
    "        torch.save(model.state_dict(), save_directory+\"models/NNet_Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90b296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MyModel=NNet_Raw()\n",
    "summary(MyModel, [1,66150])\n",
    "print(MyModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab3f0168",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4da7d32ceeda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MyModel' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(MyModel, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.01, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f82462",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Results Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdead8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_value(values,loss):\n",
    "    if loss==True:\n",
    "        max_v=100\n",
    "    else:\n",
    "        max_v=0\n",
    "    index=-1\n",
    "    for i in range(len(values)):\n",
    "        if loss==True:\n",
    "            if values[i]<max_v:\n",
    "                max_v=values[i]\n",
    "                index=i+1\n",
    "        else:\n",
    "            if values[i]>max_v:\n",
    "                max_v=values[i]\n",
    "                index=i+1\n",
    "    return max_v,index\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4a9f9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: NNet_Raw\n",
      "Best Model for accuracy: lr_0001_reg_00001\n",
      "Value: 41.58124923706055 Epoch: 9\n",
      "Best Model for loss: lr_0001_reg_00001\n",
      "Value: 1.8506742839217185 Epoch: 9\n",
      "\n",
      "Model: NNet1\n",
      "Best Model for accuracy: lr_00001_reg_0001\n",
      "Value: 50.9375 Epoch: 2\n",
      "Best Model for loss: lr_0001_reg_000001\n",
      "Value: 1.7595391278117896 Epoch: 4\n",
      "\n",
      "Model: NNet1_Small\n",
      "Best Model for accuracy: lr_00001_reg_00001\n",
      "Value: 50.51250076293945 Epoch: 9\n",
      "Best Model for loss: lr_0001_reg_000001\n",
      "Value: 1.7661717012748122 Epoch: 10\n",
      "\n",
      "Model: Ensemble\n",
      "Best Model for accuracy: lr_0001_reg_000001\n",
      "Value: 50.76874923706055 Epoch: 2\n",
      "Best Model for loss: lr_0001_reg_000001\n",
      "Value: 1.769827719859779 Epoch: 2\n",
      "\n",
      "Model: NNet2\n",
      "Best Model for accuracy: lr_00001_reg_000001\n",
      "Value: 38.79375076293945 Epoch: 9\n",
      "Best Model for loss: lr_00001_reg_000001\n",
      "Value: 1.8803115216344595 Epoch: 9\n",
      "\n",
      "['Transfer_Learning', 'NNet_Raw', 'NNet1', 'NNet1_Small', 'Ensemble', 'NNet2', 'Ensemble_No_Weights']\n"
     ]
    }
   ],
   "source": [
    "res_directory=\"./results/\"\n",
    "models=os.listdir(res_directory) \n",
    "for i in models:\n",
    "    best_loss=100\n",
    "    best_acc=0\n",
    "    best_loss_ep=0\n",
    "    best_acc_ep=0\n",
    "    model_folder=res_directory+i\n",
    "    trials=os.listdir(model_folder)\n",
    "    if(len(trials)==0):\n",
    "        continue\n",
    "    best_trials=[(best_loss,best_loss_ep),(best_acc,best_acc_ep)]\n",
    "    best_trials_names=['','']\n",
    "    for j in trials:\n",
    "        res=np.loadtxt(model_folder+\"/\"+j)\n",
    "        loss,epoch_l=find_best_value(res[0],True)\n",
    "        accuracy,epoch_a=find_best_value(res[1],False)\n",
    "        if(loss<best_loss):\n",
    "            best_trials_names[0]=j\n",
    "            best_loss=loss\n",
    "            best_loss_ep=epoch_l\n",
    "            best_trials[0]=(best_loss,best_loss_ep)\n",
    "        if(accuracy>best_acc):\n",
    "            best_trials_names[1]=j\n",
    "            best_acc=accuracy\n",
    "            best_acc_ep=epoch_a\n",
    "            best_trials[1]=(best_acc,best_acc_ep)\n",
    "            \n",
    "    print(\"Model:\",i)\n",
    "    print(\"Best Model for accuracy:\",best_trials_names[1])\n",
    "    print(\"Value:\",best_trials[1][0],\"Epoch:\",best_trials[1][1])\n",
    "    print(\"Best Model for loss:\",best_trials_names[0])\n",
    "    print(\"Value:\",best_trials[0][0],\"Epoch:\",best_trials[0][1])\n",
    "    print(\"\")\n",
    "    \n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641166af",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# NN on echonest features\n",
    "\n",
    "As you can see below, unfortunately we cannot perform a train using echonest features (danceability, tempo, acousticness, ...) because there are only 1300 tracks with echonest features which are not NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25e87d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening csvs...\n",
      "small dataset shape: (8000, 52)\n",
      "echonest csv shape (only audio features): (13129, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"opening csvs...\")\n",
    "\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "echonest = utils.load('data/fma_metadata/echonest.csv')\n",
    "echonest = echonest['echonest', 'audio_features']\n",
    "small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "\n",
    "print(\"small dataset shape:\",small.shape)\n",
    "print(\"echonest csv shape (only audio features):\",echonest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8b18347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track ids shape: 8000 content: [2, 5, 10, 140, 141, 148, 182, 190, 193, 194] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.416675</td>\n",
       "      <td>0.675894</td>\n",
       "      <td>0.634476</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.177647</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>165.922</td>\n",
       "      <td>0.576661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.745566</td>\n",
       "      <td>0.701470</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.124595</td>\n",
       "      <td>100.260</td>\n",
       "      <td>0.621661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.951670</td>\n",
       "      <td>0.658179</td>\n",
       "      <td>0.924525</td>\n",
       "      <td>0.965427</td>\n",
       "      <td>0.115474</td>\n",
       "      <td>0.032985</td>\n",
       "      <td>111.562</td>\n",
       "      <td>0.963590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.376312</td>\n",
       "      <td>0.734079</td>\n",
       "      <td>0.265685</td>\n",
       "      <td>0.669581</td>\n",
       "      <td>0.085995</td>\n",
       "      <td>0.039068</td>\n",
       "      <td>107.952</td>\n",
       "      <td>0.609991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.963657</td>\n",
       "      <td>0.435933</td>\n",
       "      <td>0.075632</td>\n",
       "      <td>0.345493</td>\n",
       "      <td>0.105686</td>\n",
       "      <td>0.026658</td>\n",
       "      <td>33.477</td>\n",
       "      <td>0.163950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154413</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154414</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155066</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        acousticness  danceability    energy  instrumentalness  liveness  \\\n",
       "2           0.416675      0.675894  0.634476          0.010628  0.177647   \n",
       "5           0.043567      0.745566  0.701470          0.000697  0.373143   \n",
       "10          0.951670      0.658179  0.924525          0.965427  0.115474   \n",
       "140         0.376312      0.734079  0.265685          0.669581  0.085995   \n",
       "141         0.963657      0.435933  0.075632          0.345493  0.105686   \n",
       "...              ...           ...       ...               ...       ...   \n",
       "154308           NaN           NaN       NaN               NaN       NaN   \n",
       "154309           NaN           NaN       NaN               NaN       NaN   \n",
       "154413           NaN           NaN       NaN               NaN       NaN   \n",
       "154414           NaN           NaN       NaN               NaN       NaN   \n",
       "155066           NaN           NaN       NaN               NaN       NaN   \n",
       "\n",
       "        speechiness    tempo   valence  \n",
       "2          0.159310  165.922  0.576661  \n",
       "5          0.124595  100.260  0.621661  \n",
       "10         0.032985  111.562  0.963590  \n",
       "140        0.039068  107.952  0.609991  \n",
       "141        0.026658   33.477  0.163950  \n",
       "...             ...      ...       ...  \n",
       "154308          NaN      NaN       NaN  \n",
       "154309          NaN      NaN       NaN  \n",
       "154413          NaN      NaN       NaN  \n",
       "154414          NaN      NaN       NaN  \n",
       "155066          NaN      NaN       NaN  \n",
       "\n",
       "[8000 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select small dataset from echonest csv\n",
    "\n",
    "track_ids = small.index.values.tolist()\n",
    "print(\"Track ids shape:\",len(track_ids),\"content:\",track_ids[:10],\"...\")\n",
    "echonest_small = pd.DataFrame(echonest,index=track_ids)\n",
    "\n",
    "ipd.display(echonest_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12714b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_echonest: shape: (8000, 8)\n",
      "There are 6706 rows containing NaN only as data\n"
     ]
    }
   ],
   "source": [
    "X_train_echonest = echonest_small.to_numpy(dtype=np.float16)\n",
    "print(\"X_train_echonest: shape:\",X_train_echonest.shape)\n",
    "\n",
    "nan_rows = np.argwhere(np.isnan(X_train_echonest).all(axis=1))\n",
    "\n",
    "print(\"There are\",len(nan_rows),\"rows containing NaN only as data\")\n",
    "#nan_rows = nan_rows.squeeze()\n",
    "#for i in nan_rows:\n",
    "#    print(\"row:\",i,\":\",X_train_echonest[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917bc8e",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "We try to use transfer learning using a ResNet18 by torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "497e31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 24, 64, 257]             648\n",
      "       BatchNorm2d-2          [-1, 24, 64, 257]              48\n",
      "              ReLU-3          [-1, 24, 64, 257]               0\n",
      "         MaxPool2d-4          [-1, 24, 32, 129]               0\n",
      "            Conv2d-5           [-1, 24, 16, 65]             216\n",
      "       BatchNorm2d-6           [-1, 24, 16, 65]              48\n",
      "            Conv2d-7           [-1, 24, 16, 65]             576\n",
      "       BatchNorm2d-8           [-1, 24, 16, 65]              48\n",
      "              ReLU-9           [-1, 24, 16, 65]               0\n",
      "           Conv2d-10          [-1, 24, 32, 129]             576\n",
      "      BatchNorm2d-11          [-1, 24, 32, 129]              48\n",
      "             ReLU-12          [-1, 24, 32, 129]               0\n",
      "           Conv2d-13           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-14           [-1, 24, 16, 65]              48\n",
      "           Conv2d-15           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-16           [-1, 24, 16, 65]              48\n",
      "             ReLU-17           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-18           [-1, 48, 16, 65]               0\n",
      "           Conv2d-19           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-20           [-1, 24, 16, 65]              48\n",
      "             ReLU-21           [-1, 24, 16, 65]               0\n",
      "           Conv2d-22           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-23           [-1, 24, 16, 65]              48\n",
      "           Conv2d-24           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-25           [-1, 24, 16, 65]              48\n",
      "             ReLU-26           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-27           [-1, 48, 16, 65]               0\n",
      "           Conv2d-28           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-29           [-1, 24, 16, 65]              48\n",
      "             ReLU-30           [-1, 24, 16, 65]               0\n",
      "           Conv2d-31           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-32           [-1, 24, 16, 65]              48\n",
      "           Conv2d-33           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-34           [-1, 24, 16, 65]              48\n",
      "             ReLU-35           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-36           [-1, 48, 16, 65]               0\n",
      "           Conv2d-37           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-38           [-1, 24, 16, 65]              48\n",
      "             ReLU-39           [-1, 24, 16, 65]               0\n",
      "           Conv2d-40           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-41           [-1, 24, 16, 65]              48\n",
      "           Conv2d-42           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-43           [-1, 24, 16, 65]              48\n",
      "             ReLU-44           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-45           [-1, 48, 16, 65]               0\n",
      "           Conv2d-46            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-47            [-1, 48, 8, 33]              96\n",
      "           Conv2d-48            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-49            [-1, 48, 8, 33]              96\n",
      "             ReLU-50            [-1, 48, 8, 33]               0\n",
      "           Conv2d-51           [-1, 48, 16, 65]           2,304\n",
      "      BatchNorm2d-52           [-1, 48, 16, 65]              96\n",
      "             ReLU-53           [-1, 48, 16, 65]               0\n",
      "           Conv2d-54            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-55            [-1, 48, 8, 33]              96\n",
      "           Conv2d-56            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-57            [-1, 48, 8, 33]              96\n",
      "             ReLU-58            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-59            [-1, 96, 8, 33]               0\n",
      "           Conv2d-60            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-61            [-1, 48, 8, 33]              96\n",
      "             ReLU-62            [-1, 48, 8, 33]               0\n",
      "           Conv2d-63            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-64            [-1, 48, 8, 33]              96\n",
      "           Conv2d-65            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-66            [-1, 48, 8, 33]              96\n",
      "             ReLU-67            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-68            [-1, 96, 8, 33]               0\n",
      "           Conv2d-69            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-70            [-1, 48, 8, 33]              96\n",
      "             ReLU-71            [-1, 48, 8, 33]               0\n",
      "           Conv2d-72            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-73            [-1, 48, 8, 33]              96\n",
      "           Conv2d-74            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-75            [-1, 48, 8, 33]              96\n",
      "             ReLU-76            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-77            [-1, 96, 8, 33]               0\n",
      "           Conv2d-78            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-79            [-1, 48, 8, 33]              96\n",
      "             ReLU-80            [-1, 48, 8, 33]               0\n",
      "           Conv2d-81            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-82            [-1, 48, 8, 33]              96\n",
      "           Conv2d-83            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-84            [-1, 48, 8, 33]              96\n",
      "             ReLU-85            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-86            [-1, 96, 8, 33]               0\n",
      "           Conv2d-87            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-88            [-1, 48, 8, 33]              96\n",
      "             ReLU-89            [-1, 48, 8, 33]               0\n",
      "           Conv2d-90            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-91            [-1, 48, 8, 33]              96\n",
      "           Conv2d-92            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-93            [-1, 48, 8, 33]              96\n",
      "             ReLU-94            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-95            [-1, 96, 8, 33]               0\n",
      "           Conv2d-96            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-97            [-1, 48, 8, 33]              96\n",
      "             ReLU-98            [-1, 48, 8, 33]               0\n",
      "           Conv2d-99            [-1, 48, 8, 33]             432\n",
      "     BatchNorm2d-100            [-1, 48, 8, 33]              96\n",
      "          Conv2d-101            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-102            [-1, 48, 8, 33]              96\n",
      "            ReLU-103            [-1, 48, 8, 33]               0\n",
      "InvertedResidual-104            [-1, 96, 8, 33]               0\n",
      "          Conv2d-105            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-106            [-1, 48, 8, 33]              96\n",
      "            ReLU-107            [-1, 48, 8, 33]               0\n",
      "          Conv2d-108            [-1, 48, 8, 33]             432\n",
      "     BatchNorm2d-109            [-1, 48, 8, 33]              96\n",
      "          Conv2d-110            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-111            [-1, 48, 8, 33]              96\n",
      "            ReLU-112            [-1, 48, 8, 33]               0\n",
      "InvertedResidual-113            [-1, 96, 8, 33]               0\n",
      "          Conv2d-114            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-115            [-1, 48, 8, 33]              96\n",
      "            ReLU-116            [-1, 48, 8, 33]               0\n",
      "          Conv2d-117            [-1, 48, 8, 33]             432\n",
      "     BatchNorm2d-118            [-1, 48, 8, 33]              96\n",
      "          Conv2d-119            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-120            [-1, 48, 8, 33]              96\n",
      "            ReLU-121            [-1, 48, 8, 33]               0\n",
      "InvertedResidual-122            [-1, 96, 8, 33]               0\n",
      "          Conv2d-123            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-124            [-1, 96, 4, 17]             192\n",
      "          Conv2d-125            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-126            [-1, 96, 4, 17]             192\n",
      "            ReLU-127            [-1, 96, 4, 17]               0\n",
      "          Conv2d-128            [-1, 96, 8, 33]           9,216\n",
      "     BatchNorm2d-129            [-1, 96, 8, 33]             192\n",
      "            ReLU-130            [-1, 96, 8, 33]               0\n",
      "          Conv2d-131            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-132            [-1, 96, 4, 17]             192\n",
      "          Conv2d-133            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-134            [-1, 96, 4, 17]             192\n",
      "            ReLU-135            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-136           [-1, 192, 4, 17]               0\n",
      "          Conv2d-137            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-138            [-1, 96, 4, 17]             192\n",
      "            ReLU-139            [-1, 96, 4, 17]               0\n",
      "          Conv2d-140            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-141            [-1, 96, 4, 17]             192\n",
      "          Conv2d-142            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-143            [-1, 96, 4, 17]             192\n",
      "            ReLU-144            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-145           [-1, 192, 4, 17]               0\n",
      "          Conv2d-146            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-147            [-1, 96, 4, 17]             192\n",
      "            ReLU-148            [-1, 96, 4, 17]               0\n",
      "          Conv2d-149            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-150            [-1, 96, 4, 17]             192\n",
      "          Conv2d-151            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-152            [-1, 96, 4, 17]             192\n",
      "            ReLU-153            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-154           [-1, 192, 4, 17]               0\n",
      "          Conv2d-155            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-156            [-1, 96, 4, 17]             192\n",
      "            ReLU-157            [-1, 96, 4, 17]               0\n",
      "          Conv2d-158            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-159            [-1, 96, 4, 17]             192\n",
      "          Conv2d-160            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-161            [-1, 96, 4, 17]             192\n",
      "            ReLU-162            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-163           [-1, 192, 4, 17]               0\n",
      "          Conv2d-164          [-1, 1024, 4, 17]         196,608\n",
      "     BatchNorm2d-165          [-1, 1024, 4, 17]           2,048\n",
      "            ReLU-166          [-1, 1024, 4, 17]               0\n",
      "          Linear-167                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 1,366,792\n",
      "Trainable params: 1,366,792\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 33.40\n",
      "Params size (MB): 5.21\n",
      "Estimated Total Size (MB): 39.37\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/riccardo/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "model_ResNet18 = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x0_5', pretrained=True)\n",
    "summary(model_ResNet18, (3,128,513))\n",
    "\n",
    "\n",
    "# Remove layers you want to discard\n",
    "\n",
    "class MyModel_ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel_ResNet18, self).__init__()\n",
    "        pretrained_model = torchvision.models.shufflenet_v2_x0_5(pretrained=True)\n",
    "        layers=list(pretrained_model.children())[:-4]\n",
    "        #print(layers)\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "        #self.conv1=nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "        #self.relu1=nn.ReLU(inplace=True)\n",
    "        #self.mp1=nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        #self.conv2=nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        '''self.relu2=nn.ReLU(inplace=True)\n",
    "        self.mp2=nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv3=nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3=nn.ReLU(inplace=True)\n",
    "        self.conv4=nn.Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4=nn.ReLU(inplace=True)\n",
    "        self.conv5=nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5=nn.ReLU(inplace=True)\n",
    "        self.mp5=nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)'''\n",
    "        \n",
    "        \n",
    "        self.pool= nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten=nn.Flatten() \n",
    "        self.last_layer = nn.Linear(49920, 32)\n",
    "        self.fc= nn.Linear(32, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x=self.conv1(x)\n",
    "        #x=self.relu1(x)\n",
    "        #x=self.mp1(x)\n",
    "        '''x=self.conv2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.mp2(x)    \n",
    "        x=self.conv4(x)\n",
    "        x=self.relu4(x)\n",
    "        x=self.conv5(x)\n",
    "        x=self.relu5(x)\n",
    "        x=self.mp5(x)'''\n",
    "        \n",
    "        x = self.features(x)\n",
    "        #x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.last_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8b7bc56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 24, 64, 257]             648\n",
      "       BatchNorm2d-2          [-1, 24, 64, 257]              48\n",
      "              ReLU-3          [-1, 24, 64, 257]               0\n",
      "         MaxPool2d-4          [-1, 24, 32, 129]               0\n",
      "            Conv2d-5           [-1, 24, 16, 65]             216\n",
      "       BatchNorm2d-6           [-1, 24, 16, 65]              48\n",
      "            Conv2d-7           [-1, 24, 16, 65]             576\n",
      "       BatchNorm2d-8           [-1, 24, 16, 65]              48\n",
      "              ReLU-9           [-1, 24, 16, 65]               0\n",
      "           Conv2d-10          [-1, 24, 32, 129]             576\n",
      "      BatchNorm2d-11          [-1, 24, 32, 129]              48\n",
      "             ReLU-12          [-1, 24, 32, 129]               0\n",
      "           Conv2d-13           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-14           [-1, 24, 16, 65]              48\n",
      "           Conv2d-15           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-16           [-1, 24, 16, 65]              48\n",
      "             ReLU-17           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-18           [-1, 48, 16, 65]               0\n",
      "           Conv2d-19           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-20           [-1, 24, 16, 65]              48\n",
      "             ReLU-21           [-1, 24, 16, 65]               0\n",
      "           Conv2d-22           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-23           [-1, 24, 16, 65]              48\n",
      "           Conv2d-24           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-25           [-1, 24, 16, 65]              48\n",
      "             ReLU-26           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-27           [-1, 48, 16, 65]               0\n",
      "           Conv2d-28           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-29           [-1, 24, 16, 65]              48\n",
      "             ReLU-30           [-1, 24, 16, 65]               0\n",
      "           Conv2d-31           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-32           [-1, 24, 16, 65]              48\n",
      "           Conv2d-33           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-34           [-1, 24, 16, 65]              48\n",
      "             ReLU-35           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-36           [-1, 48, 16, 65]               0\n",
      "           Conv2d-37           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-38           [-1, 24, 16, 65]              48\n",
      "             ReLU-39           [-1, 24, 16, 65]               0\n",
      "           Conv2d-40           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-41           [-1, 24, 16, 65]              48\n",
      "           Conv2d-42           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-43           [-1, 24, 16, 65]              48\n",
      "             ReLU-44           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-45           [-1, 48, 16, 65]               0\n",
      "          Flatten-46                [-1, 49920]               0\n",
      "           Linear-47                    [-1, 8]         399,368\n",
      "          Softmax-48                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 407,000\n",
      "Trainable params: 407,000\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 20.44\n",
      "Params size (MB): 1.55\n",
      "Estimated Total Size (MB): 22.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "model = MyModel_ResNet18()\n",
    "summary(model, (3,128,513))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b4b15",
   "metadata": {},
   "source": [
    "## RGB Dataset\n",
    "Resnet18 expects RGB input images, so our dataset need to be converted from a one to a three-channel. We'll do dat by copying three times the same image in the three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5e94008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDatasetRGB(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "                \n",
    "        #do ResNet18 normalization:\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        #copy the channel 3 times (need to unsqueeze to create a new dimension first)\n",
    "        #print(\"DATASET*  sample shape is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        stft_vector = stft_vector.unsqueeze(0).repeat(3,1,1)\n",
    "        stft_vector = stft_vector.to(torch.float32) #float32 needed for ResNet18 model (downcast from float64)\n",
    "        #print(\"DATASET* sample shape after repeat is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        #print(\"stft_vector dtype:\",stft_vector.dtype)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c24605b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor batch_idx, batch  in enumerate(train_data_loader):\\n    print (\"batch index:\",batch_idx)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    \\n    for idx, sample in enumerate(inputs):\\n        label = labels[idx]\\n        print(\"inputs: shape:\",inputs.shape)\\n        print(\"sample: shape:\",sample.shape)\\n'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform_ResNet18 = transforms.Compose([\n",
    "    transforms.Normalize(mean= [loaded_mean,loaded_mean,loaded_mean], std=[loaded_std,loaded_std,loaded_std]) #our values\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #ResNet18 specific values\n",
    "])\n",
    "\n",
    "train_dataset = MyDatasetRGB(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDatasetRGB(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDatasetRGB(test_file_paths, Y_test,  transform = transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size = 10, shuffle=True)\n",
    "\n",
    "'''\n",
    "for batch_idx, batch  in enumerate(train_data_loader):\n",
    "    print (\"batch index:\",batch_idx)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    \n",
    "    for idx, sample in enumerate(inputs):\n",
    "        label = labels[idx]\n",
    "        print(\"inputs: shape:\",inputs.shape)\n",
    "        print(\"sample: shape:\",sample.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5f3747da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2d031c3ccc4949aed77c2afbc91a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-cf08808a9749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRGB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-4e326a5b92a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose, RGB)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Extract the inputs and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-2d7d4f15fcf3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         x=self.mp5(x)'''\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    163\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "#todo: convert input tensor from float64 to double\n",
    "\n",
    "#todo normalize again using resnet18 suggested mean and std\n",
    "#TODO: FIX TEST FUNCTION USING RGB (maybe already works)\n",
    "\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.001,verbose=False, RGB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0f8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
