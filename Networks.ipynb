{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15945d26",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6eab989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tqdm.notebook as tq\n",
    "import utils\n",
    "from pydub import AudioSegment\n",
    "from tkinter import Tcl # file sorting by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c3e14",
   "metadata": {},
   "source": [
    "# Load STFT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8ded2",
   "metadata": {},
   "source": [
    "### Dictionary creation for the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c4013",
   "metadata": {},
   "source": [
    "We want a dictionary indicating a numbeer for each genre:\n",
    "\n",
    "{0: 'Hip-Hop', 1: 'Pop', 2: 'Folk', 3: 'Rock', 4: 'Experimental', 5: 'International', 6: 'Electronic', 7: 'Instrumental'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e985c",
   "metadata": {},
   "source": [
    "### Creation of the labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0be9951a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_folder: data/fma_small_stft_transposed_22050_overlapped/train\n",
      "validation_folder: data/fma_small_stft_transposed_22050_overlapped/validation\n",
      "test_folder: data/fma_small_stft_transposed_22050_overlapped/test \n",
      "\n",
      "audio directory:  ./data/fma_small/\n",
      "Loading tracks.csv...\n",
      "small dataset shape: (8000, 52)\n",
      "Track.csv: 6400 training samples, 800 validation samples, 800 test samples\n",
      "\n",
      "there are 8 unique genres\n",
      "Dictionary of genres created: {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Rock': 3, 'Experimental': 4, 'International': 5, 'Electronic': 6, 'Instrumental': 7}\n",
      "labels length: 127940\n",
      "labels length: 16000\n",
      "labels length: 16000\n"
     ]
    }
   ],
   "source": [
    "def create_single_dataset(folder_path, tracks_dataframe, genre_dictionary):    \n",
    "    labels = []\n",
    "   \n",
    "    _, file_list = get_sorted_file_paths(folder_path)\n",
    "    \n",
    "    for i,file in enumerate(file_list):\n",
    "        #print(\"considering file:\",file, \"({}/{})\".format(i,len(file_list)))\n",
    "        track_id_clip_id = file.split('.')[0]\n",
    "        track_id = track_id_clip_id.split('_')[0]\n",
    "        #print(\"track id with clip: {}, track id: {}\".format(track_id_clip_id, track_id))\n",
    "        genre = tracks_dataframe.loc[int(track_id)]\n",
    "        #print(\"genre from dataframe: \", genre)\n",
    "        label = genre_dictionary[genre]\n",
    "        #print(\"label from dictionary:\",label)\n",
    "        labels.append(label)\n",
    "    print(\"labels length: {}\".format(len(labels)))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "#create the train,validation and test vectors using the files in the train/validation/test folders\n",
    "def create_dataset_splitted(folder_path):\n",
    "    train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "    validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "    test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "    \n",
    "    print(\"train_folder:\",train_folder)\n",
    "    print(\"validation_folder:\",validation_folder)\n",
    "    print(\"test_folder:\",test_folder,\"\\n\")\n",
    "    \n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "    \n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "\n",
    "    small_training_top_genres = small_training['genre_top']\n",
    "    small_validation_top_genres = small_validation['genre_top']\n",
    "    small_test_top_genres = small_test['genre_top']\n",
    "    \n",
    "    #create dictionary of genre classes:\n",
    "    unique_genres = small_training_top_genres.unique()\n",
    "    unique_genres = np.array(unique_genres)\n",
    "    print(\"there are {} unique genres\".format(len(unique_genres)))\n",
    "    genre_dictionary = {}\n",
    "    for i,genre in enumerate(unique_genres):\n",
    "        genre_dictionary[genre] = i\n",
    "    print(\"Dictionary of genres created:\",genre_dictionary)\n",
    "    \n",
    "    \n",
    "    Y_train = create_single_dataset(train_folder, small_training_top_genres, genre_dictionary)\n",
    "    Y_validation = create_single_dataset(validation_folder, small_validation_top_genres, genre_dictionary)\n",
    "    Y_test = create_single_dataset(test_folder, small_test_top_genres, genre_dictionary)\n",
    "    \n",
    "    return Y_train, Y_validation, Y_test\n",
    " \n",
    "def get_sorted_file_paths(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #sort the dataset files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_list = Tcl().call('lsort', '-dict', file_list) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...\n",
    "    file_paths = [os.path.join(folder_path, file_name) for file_name in file_list] #join filename with folder path\n",
    "    #print(\"There are {} in the folder: {}\".format(len(file_list),file_list))\n",
    "    return file_paths, file_list\n",
    "    \n",
    "    \n",
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "Y_train, Y_validation, Y_test = create_dataset_splitted(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20175a8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fe67f",
   "metadata": {},
   "source": [
    "Class to load the STFT from files. Each file has a (128,513) matrix containing the STFT of a 3 seconds audio clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "187ddbe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cae70891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  127940\n",
      "len of validation dataset:  16000\n",
      "len of test dataset:  16000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDataset(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd9c68",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "We will use Z-Score to normalize the training, validation and test set by calculating the mean and the std deviation on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f571ada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_mean'\n",
    "std_save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_std_deviation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677263cf",
   "metadata": {},
   "source": [
    "## Calculation of mean and standard deviation (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5795abf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum: tensor(8.9296e+09)\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum=0\n",
    "\n",
    "#iter all the training set by batches and calculate the sum of all the sample values (513*128 values for each sample)\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum:\",current_sum)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "        current_sum += torch.sum(sample)\n",
    "        #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum:\",current_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c794bf2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of training set: tensor(1.0629)\n",
      "Saving the mean in file: ./data/fma_small_stft_transposed_22050_overlapped/train_mean\n"
     ]
    }
   ],
   "source": [
    "mean = current_sum/(len(train_dataset)*513*128) #divide the sum for the total number of values considerated\n",
    "print(\"mean of training set:\",mean)\n",
    "\n",
    "print(\"Saving the mean in file:\",save_filename) \n",
    "np.save(save_filename,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec52eb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum of squares: tensor(7.8299e+10)\n"
     ]
    }
   ],
   "source": [
    "#now let's calculate the standard deviation (squared root of the variance)\n",
    "\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum_of_squares = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum_of_squares:\",current_sum_of_squares)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample shape\",sample.shape)\n",
    "        for row in sample:\n",
    "            #print(\"row shape:\",row.shape)\n",
    "            for elem in row:\n",
    "                #print(\"elem: shape\",elem.shape,\"content:\",elem)\n",
    "                difference = elem - mean\n",
    "                difference_squared = difference**2\n",
    "                current_sum_of_squares += difference_squared\n",
    "                #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum of squares:\",current_sum_of_squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecfb0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "variance = current_sum_of_squares/((len(train_dataset) * 513 * 128)-1)\n",
    "std_deviation = math.sqrt(variance)\n",
    "\n",
    "print(\"variance:\",variance)\n",
    "print(\"std_deviation:\",std_deviation)\n",
    "\n",
    "print(\"Saving the std_deviation in file:\",std_save_filename) \n",
    "np.save(std_save_filename,std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fc2ab",
   "metadata": {},
   "source": [
    "## Load calculated mean and std deviation from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "977c1ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean: 1.0629134\n",
      "loaded std: 3.0528938510507846\n"
     ]
    }
   ],
   "source": [
    "loaded_mean = np.load(save_filename+'.npy')\n",
    "print(\"loaded mean:\",loaded_mean)\n",
    "\n",
    "loaded_std = np.load(std_save_filename+'.npy')\n",
    "print(\"loaded std:\",loaded_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d4804",
   "metadata": {},
   "source": [
    "## Create the normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "092efbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean= loaded_mean, std= loaded_std)\n",
    "])\n",
    "\n",
    "train_dataset = MyDataset(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test,  transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a52dc",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83f745b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4,513))\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.bn4 = nn.BatchNorm1d(300)\n",
    "        self.dense2 = nn.Linear(300,150)\n",
    "        self.bn5 = nn.BatchNorm1d(150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8734822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet2, self).__init__()\n",
    "        self.drop=nn.Dropout(0.2)\n",
    "        # STFT spectrogram input: (batch_size, 1, 128, 513)\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513))\n",
    "        self.batch1=nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128,128, kernel_size=(4, 1),padding=1)\n",
    "        self.batch2=nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=(4, 1),padding=2)\n",
    "        self.batch3=nn.BatchNorm2d(128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(128,1))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(128,1))\n",
    "        self.fc1 = nn.Linear(250,128)\n",
    "        self.bn1=nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.bn2=nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 8)  # 8 classes for genre predictions\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x=self.batch1(x)\n",
    "        x = torch.relu(x)\n",
    "        y=x\n",
    "        x = self.conv2(x)\n",
    "        x=self.batch2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x=self.batch3(x)\n",
    "        x = torch.relu(x)\n",
    "        # Sum between the first and third conv layers\n",
    "        x = x[:, :, :, 0] + y[:, :, :, 0]\n",
    "        \n",
    "        x = torch.relu(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x=self.drop(x)\n",
    "        x=self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)        \n",
    "        x=self.drop(x)\n",
    "        x=self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845a53e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d034d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "LEARNING_RATE=0.0001\n",
    "\n",
    "learning_rate_list = [0.0001,0.00001]\n",
    "batch_size_list = [128,256,512]\n",
    "reg_list=[0.001,0.0001,0.00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd576a3",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "223742e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model, validation_dataset, Y_validation,RGB=False):\n",
    "    # Stop parameters learning\n",
    "    model.eval()\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    #confusion_matrix = np.zeros((8, 8), dtype=int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample, label in validation_loader:\n",
    "            \n",
    "            if RGB==False:\n",
    "                sample = sample.unsqueeze(1)\n",
    "\n",
    "            # Predict label\n",
    "            output = model(sample)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            max_index = torch.argmax(output).item()  # The index with maximum probability\n",
    "\n",
    "            #confusion_matrix[label][max_index] += 1\n",
    "\n",
    "            correct += (max_index == label)\n",
    "\n",
    "    #cm = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "    #cm.plot()\n",
    "    #print(confusion_matrix)\n",
    "    accuracy = 100 * correct / len(Y_validation)\n",
    "    average_loss = total_loss / len(Y_validation)\n",
    "\n",
    "    model.train()\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0ca4e98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, num_epochs, learning_rate, verbose = False, RGB=False, reg=1e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    counted_labels=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", dtype:\",inputs.dtype,\" content: \",inputs)\n",
    "                print(\"min value:\",torch.min(inputs))\n",
    "                print(\"max value:\",torch.max(inputs))\n",
    "                print(\"\\nlabels shape:\",labels.size(),\",dtype:\",labels.dtype,\", content: \",labels)\n",
    "            if(RGB==False):\n",
    "                inputs = inputs.unsqueeze(1) #add a dimension if input is to be considered just grayscale\n",
    "                #if input is RGB, there are already 3 channels\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if(verbose == True):\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "                print(\"List of labels until now:\",counted_labels)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                counted_labels[labels[index].item()]+=1\n",
    "                if(labels[index].item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "        \n",
    "        #calculate train loss and accuracy\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        train_loss_list.append(average_loss)\n",
    "        train_acc_list.append(average_accuracy)\n",
    "        \n",
    "        #calculate validation loss and accuracy\n",
    "        val_acc, val_loss = test(model, validation_dataset, Y_validation,RGB=RGB)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],Train Loss: {average_loss:.4f}. Train Accuracy: {average_accuracy} Val Loss: {val_loss} Val Accuracy: {val_acc}\")\n",
    "        progress_bar.close()\n",
    "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef7fab",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1 + BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3685ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1_Small(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1_Small, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(2, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv2 = nn.Conv2d(64,128, kernel_size=(2, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv3 = nn.Conv2d(128,64, kernel_size=(4, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(2, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(256, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dense2 = nn.Linear(128,64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.dense3 = nn.Linear(64, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dense2(x)\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.bn5(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61b169d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (128 x 513). Kernel size: (513 x 4). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-79d0eb9c1f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_NNet1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNet1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_NNet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m513\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-18df190b998c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (128 x 513). Kernel size: (513 x 4). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "model_NNet1 = NNet1()\n",
    "summary(model_NNet1, (1, 128, 513))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add batch_size optimization loop\n",
    "save_directory=\"./results/NNet1_Small/\"\n",
    "\n",
    "lr_list= [ 0.0001]\n",
    "r_list=[0.0001]\n",
    "\n",
    "for i in lr_list:\n",
    "    for j in r_list:\n",
    "        if(j!=1e-5 and j!=1e-6):\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"0\"+str(j).split(\".\")[1]\n",
    "        elif(j==1e-5):\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"00001\"\n",
    "        else:\n",
    "            filename=save_directory+\"lr_\"+\"0\"+str(i).split(\".\")[1]+\"_reg_\"+\"000001\"\n",
    "        print(filename)\n",
    "        model = NNet1_Small()\n",
    "        train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=128, num_epochs=10, learning_rate=i, reg=j)\n",
    "        print(\"Trained with learning rate=\",i,\" and with regularization term=\",j)\n",
    "        print(\"Loss:\",val_loss_list)\n",
    "        print(\"Accuracy:\",val_acc_list)\n",
    "        save_values=[val_loss_list,val_acc_list]\n",
    "        np.savetxt(filename,save_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabb348",
   "metadata": {},
   "source": [
    "## Raw audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30092ba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyDatasetRaw(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose=False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        raw_vector = np.load(file_path).astype(np.int16) # Ensure int16 data type\n",
    "        if(self.verbose==True):\n",
    "            print(\"raw vector shape:\",raw_vector.shape)\n",
    "        raw_vector = torch.tensor(raw_vector)\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            \n",
    "            #convert to float64 tensor\n",
    "            raw_vector = raw_vector.double()\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.unsqueeze(raw_vector, dim=0)\n",
    "            #print(\"TRANSFORM: after first unsqueeze:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.unsqueeze(raw_vector, dim=0) #unsqueeze two times (needed for torchvision normalize method)\n",
    "            #print(\"TRANSFORM: after second unsqueeze:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = self.transform(raw_vector) #normalize the sample\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.squeeze(raw_vector, dim=0)\n",
    "            raw_vector = torch.squeeze(raw_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after double squeeze shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "        \n",
    "        return raw_vector, label        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de69e8",
   "metadata": {},
   "source": [
    "# Normalization of raw audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d390039",
   "metadata": {},
   "source": [
    "We calculate mean and std deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1ec8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_save_filename_raw = './data/fma_small_raw_array_22050_overlapped/train_mean'\n",
    "std_save_filename_raw = './data/fma_small_raw_array_22050_overlapped/train_std_deviation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b38ea",
   "metadata": {},
   "source": [
    "# Calculation of mean raw (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba507d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDatasetRaw(file_paths_train, Y_train)\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum=0\n",
    "\n",
    "#iter all the training set by batches and calculate the sum of all the sample values (513*128 values for each sample)\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if(batch_idx%1000==0):\n",
    "        print(\"batch\",batch_idx,\"/\",total_n_batches,\"(\",round((batch_idx/len(train_dataset)*100)),\"%), current_sum:\",current_sum)\n",
    "    \n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "        current_sum += torch.sum(sample)\n",
    "       \n",
    "        #print(\"type of current_sum:\",current_sum.dtype)\n",
    "        #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum:\",current_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"current_sum\",current_sum)\n",
    "mean_raw = current_sum/(len(train_dataset)*66150) #divide the sum for the total number of values considerated\n",
    "print(\"mean of training set:\",mean_raw)\n",
    "\n",
    "print(\"Saving the mean in file:\",mean_save_filename_raw) \n",
    "np.save(mean_save_filename_raw,mean_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcee895",
   "metadata": {},
   "source": [
    "# Calculation of std deviation raw (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's calculate the standard deviation (squared root of the variance)\n",
    "\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum_of_squares = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if(batch_idx%1000==0):\n",
    "        print(\"batch\",batch_idx,\"/\",total_n_batches,round((batch_idx/len(train_dataset)*100)),\"%, current_sum_of_squares:\",current_sum_of_squares)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\\n\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for elem in inputs:\n",
    "        elem = elem.double() #convert to float64 for precise calculations\n",
    "        #print(\"elem: shape\",elem.shape,\"content:\\n\",elem)\n",
    "        difference = elem - mean_raw\n",
    "        #print(\"difference: shape:\",difference.shape,\"content:\\n\", difference)\n",
    "        difference_squared = difference**2\n",
    "        #print(\"difference_squared: shape:\",difference_squared.shape,\"content:\\n\", difference_squared)\n",
    "        current_sum_of_squares += torch.sum(difference_squared)\n",
    "        #print(\"current_sum_of_squares:\",current_sum_of_squares)\n",
    "print(\"final sum of squares:\",current_sum_of_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "variance_raw = current_sum_of_squares/((len(train_dataset)*66150)-1)\n",
    "std_deviation_raw = math.sqrt(variance)\n",
    "\n",
    "print(\"variance raw:\",variance)\n",
    "print(\"std_deviation_raw:\",std_deviation_raw)\n",
    "\n",
    "print(\"Saving the std_deviation_raw in file:\",std_save_filename_raw) \n",
    "np.save(std_save_filename_raw,std_deviation_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cb8ee",
   "metadata": {},
   "source": [
    "# Load mean and std from file (fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5d39f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean: -16.984083\n",
      "loaded std: 1032.9510216986293\n"
     ]
    }
   ],
   "source": [
    "loaded_mean_raw = np.load(mean_save_filename_raw+'.npy')\n",
    "print(\"loaded mean:\",loaded_mean_raw)\n",
    "\n",
    "loaded_std_raw = np.load(std_save_filename_raw+'.npy')\n",
    "print(\"loaded std:\",loaded_std_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7112c8",
   "metadata": {},
   "source": [
    "## Create dataset for raw audio\n",
    "\n",
    "The labels are the same as the STFT dataset (in the same order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45f5f209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  127940\n",
      "len of validation dataset:  16000\n",
      "len of test dataset:  16000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_raw_array_22050_overlapped\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDatasetRaw(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDatasetRaw(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDatasetRaw(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da518866",
   "metadata": {},
   "source": [
    "# Neural Network Architecture for raw audio\n",
    "\n",
    "We implemented a lightweight CNN to classify the samples with their raw audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68b4a333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet_Raw(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(NNet_Raw, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=16)\n",
    "        self.conv2 = nn.Conv1d(32, 8, kernel_size=16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=32)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=8)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(8)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(24)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(248, 24)\n",
    "        self.fc3 = nn.Linear(24, 8)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.maxpool1(x.float())\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x=self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm2(x)     \n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c90b296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "rand() argument after * must be an iterable, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8687654415a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNNet_Raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m66150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# batch_size of 2 for batchnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0min_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;31m# print(type(x[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# batch_size of 2 for batchnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0min_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;31m# print(type(x[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: rand() argument after * must be an iterable, not int"
     ]
    }
   ],
   "source": [
    "MyModel=NNet_Raw()\n",
    "summary(MyModel, [1,66150])\n",
    "print(MyModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab3f0168",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4da7d32ceeda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MyModel' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(MyModel, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.01, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f82462",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Results Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdead8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_value(values,loss):\n",
    "    if loss==True:\n",
    "        max_v=100\n",
    "    else:\n",
    "        max_v=0\n",
    "    index=-1\n",
    "    for i in range(len(values)):\n",
    "        if loss==True:\n",
    "            if values[i]<max_v:\n",
    "                max_v=values[i]\n",
    "                index=i+1\n",
    "        else:\n",
    "            if values[i]>max_v:\n",
    "                max_v=values[i]\n",
    "                index=i+1\n",
    "    return max_v,index\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4a9f9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: NNet_Raw\n",
      "Best Model for accuracy: lr_0001_reg_00001\n",
      "Value: 41.58124923706055 Epoch: 9\n",
      "Best Model for loss: lr_0001_reg_00001\n",
      "Value: 1.8506742839217185 Epoch: 9\n",
      "\n",
      "Model: NNet1\n",
      "Best Model for accuracy: lr_00001_reg_0001\n",
      "Value: 50.9375 Epoch: 2\n",
      "Best Model for loss: lr_0001_reg_000001\n",
      "Value: 1.7595391278117896 Epoch: 4\n",
      "\n",
      "Model: NNet1_Small\n",
      "Best Model for accuracy: lr_00001_reg_00001\n",
      "Value: 50.51250076293945 Epoch: 9\n",
      "Best Model for loss: lr_0001_reg_000001\n",
      "Value: 1.7661717012748122 Epoch: 10\n",
      "\n",
      "Model: Ensemble\n",
      "Best Model for accuracy: lr_00001_reg_000001\n",
      "Value: 51.54375076293945 Epoch: 4\n",
      "Best Model for loss: lr_00001_reg_000001\n",
      "Value: 1.7636730014681816 Epoch: 4\n",
      "\n",
      "Model: NNet2\n",
      "Best Model for accuracy: lr_00001_reg_000001\n",
      "Value: 38.79375076293945 Epoch: 9\n",
      "Best Model for loss: lr_00001_reg_000001\n",
      "Value: 1.8803115216344595 Epoch: 9\n",
      "\n",
      "Model: Ensemble_No_Weights\n",
      "Best Model for accuracy: lr_0001_reg_000001\n",
      "Value: 49.03125 Epoch: 7\n",
      "Best Model for loss: lr_0001_reg_000001\n",
      "Value: 1.854172814361751 Epoch: 7\n",
      "\n",
      "['Ensemble_Relu_No_weights', 'Transfer_Learning', 'NNet_Raw', 'NNet1', 'NNet1_Small', 'Ensemble', 'NNet2', 'Ensemble_No_Weights']\n"
     ]
    }
   ],
   "source": [
    "res_directory=\"./results/\"\n",
    "models=os.listdir(res_directory) \n",
    "for i in models:\n",
    "    best_loss=100\n",
    "    best_acc=0\n",
    "    best_loss_ep=0\n",
    "    best_acc_ep=0\n",
    "    model_folder=res_directory+i\n",
    "    trials=os.listdir(model_folder)\n",
    "    if(len(trials)==0):\n",
    "        continue\n",
    "    best_trials=[(best_loss,best_loss_ep),(best_acc,best_acc_ep)]\n",
    "    best_trials_names=['','']\n",
    "    for j in trials:\n",
    "        res=np.loadtxt(model_folder+\"/\"+j)\n",
    "        loss,epoch_l=find_best_value(res[0],True)\n",
    "        accuracy,epoch_a=find_best_value(res[1],False)\n",
    "        if(loss<best_loss):\n",
    "            best_trials_names[0]=j\n",
    "            best_loss=loss\n",
    "            best_loss_ep=epoch_l\n",
    "            best_trials[0]=(best_loss,best_loss_ep)\n",
    "        if(accuracy>best_acc):\n",
    "            best_trials_names[1]=j\n",
    "            best_acc=accuracy\n",
    "            best_acc_ep=epoch_a\n",
    "            best_trials[1]=(best_acc,best_acc_ep)\n",
    "            \n",
    "    print(\"Model:\",i)\n",
    "    print(\"Best Model for accuracy:\",best_trials_names[1])\n",
    "    print(\"Value:\",best_trials[1][0],\"Epoch:\",best_trials[1][1])\n",
    "    print(\"Best Model for loss:\",best_trials_names[0])\n",
    "    print(\"Value:\",best_trials[0][0],\"Epoch:\",best_trials[0][1])\n",
    "    print(\"\")\n",
    "    \n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641166af",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# NN on echonest features\n",
    "\n",
    "As you can see below, unfortunately we cannot perform a train using echonest features (danceability, tempo, acousticness, ...) because there are only 1300 tracks with echonest features which are not NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25e87d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening csvs...\n",
      "small dataset shape: (8000, 52)\n",
      "echonest csv shape (only audio features): (13129, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"opening csvs...\")\n",
    "\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "echonest = utils.load('data/fma_metadata/echonest.csv')\n",
    "echonest = echonest['echonest', 'audio_features']\n",
    "small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "\n",
    "print(\"small dataset shape:\",small.shape)\n",
    "print(\"echonest csv shape (only audio features):\",echonest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8b18347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track ids shape: 8000 content: [2, 5, 10, 140, 141, 148, 182, 190, 193, 194] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.416675</td>\n",
       "      <td>0.675894</td>\n",
       "      <td>0.634476</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.177647</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>165.922</td>\n",
       "      <td>0.576661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.745566</td>\n",
       "      <td>0.701470</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.124595</td>\n",
       "      <td>100.260</td>\n",
       "      <td>0.621661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.951670</td>\n",
       "      <td>0.658179</td>\n",
       "      <td>0.924525</td>\n",
       "      <td>0.965427</td>\n",
       "      <td>0.115474</td>\n",
       "      <td>0.032985</td>\n",
       "      <td>111.562</td>\n",
       "      <td>0.963590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.376312</td>\n",
       "      <td>0.734079</td>\n",
       "      <td>0.265685</td>\n",
       "      <td>0.669581</td>\n",
       "      <td>0.085995</td>\n",
       "      <td>0.039068</td>\n",
       "      <td>107.952</td>\n",
       "      <td>0.609991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.963657</td>\n",
       "      <td>0.435933</td>\n",
       "      <td>0.075632</td>\n",
       "      <td>0.345493</td>\n",
       "      <td>0.105686</td>\n",
       "      <td>0.026658</td>\n",
       "      <td>33.477</td>\n",
       "      <td>0.163950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154413</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154414</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155066</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        acousticness  danceability    energy  instrumentalness  liveness  \\\n",
       "2           0.416675      0.675894  0.634476          0.010628  0.177647   \n",
       "5           0.043567      0.745566  0.701470          0.000697  0.373143   \n",
       "10          0.951670      0.658179  0.924525          0.965427  0.115474   \n",
       "140         0.376312      0.734079  0.265685          0.669581  0.085995   \n",
       "141         0.963657      0.435933  0.075632          0.345493  0.105686   \n",
       "...              ...           ...       ...               ...       ...   \n",
       "154308           NaN           NaN       NaN               NaN       NaN   \n",
       "154309           NaN           NaN       NaN               NaN       NaN   \n",
       "154413           NaN           NaN       NaN               NaN       NaN   \n",
       "154414           NaN           NaN       NaN               NaN       NaN   \n",
       "155066           NaN           NaN       NaN               NaN       NaN   \n",
       "\n",
       "        speechiness    tempo   valence  \n",
       "2          0.159310  165.922  0.576661  \n",
       "5          0.124595  100.260  0.621661  \n",
       "10         0.032985  111.562  0.963590  \n",
       "140        0.039068  107.952  0.609991  \n",
       "141        0.026658   33.477  0.163950  \n",
       "...             ...      ...       ...  \n",
       "154308          NaN      NaN       NaN  \n",
       "154309          NaN      NaN       NaN  \n",
       "154413          NaN      NaN       NaN  \n",
       "154414          NaN      NaN       NaN  \n",
       "155066          NaN      NaN       NaN  \n",
       "\n",
       "[8000 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select small dataset from echonest csv\n",
    "\n",
    "track_ids = small.index.values.tolist()\n",
    "print(\"Track ids shape:\",len(track_ids),\"content:\",track_ids[:10],\"...\")\n",
    "echonest_small = pd.DataFrame(echonest,index=track_ids)\n",
    "\n",
    "ipd.display(echonest_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12714b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_echonest: shape: (8000, 8)\n",
      "There are 6706 rows containing NaN only as data\n"
     ]
    }
   ],
   "source": [
    "X_train_echonest = echonest_small.to_numpy(dtype=np.float16)\n",
    "print(\"X_train_echonest: shape:\",X_train_echonest.shape)\n",
    "\n",
    "nan_rows = np.argwhere(np.isnan(X_train_echonest).all(axis=1))\n",
    "\n",
    "print(\"There are\",len(nan_rows),\"rows containing NaN only as data\")\n",
    "#nan_rows = nan_rows.squeeze()\n",
    "#for i in nan_rows:\n",
    "#    print(\"row:\",i,\":\",X_train_echonest[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917bc8e",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "We try to use transfer learning using a ResNet18 by torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "497e31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "class MyModel_ResNet18(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(MyModel_ResNet18, self).__init__()\n",
    "        pretrained_model = torchvision.models.resnet18(pretrained=pretrained)\n",
    "        layers=list(pretrained_model.children())[:-3]\n",
    "        for param in pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        #print(layers)\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pool= nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten=nn.Flatten() \n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3= nn.Linear(64, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.features(x.float())\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b7bc56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 64, 64, 257]           9,408\n",
      "       BatchNorm2d-2          [-1, 64, 64, 257]             128\n",
      "              ReLU-3          [-1, 64, 64, 257]               0\n",
      "         MaxPool2d-4          [-1, 64, 32, 129]               0\n",
      "            Conv2d-5          [-1, 64, 32, 129]          36,864\n",
      "       BatchNorm2d-6          [-1, 64, 32, 129]             128\n",
      "              ReLU-7          [-1, 64, 32, 129]               0\n",
      "            Conv2d-8          [-1, 64, 32, 129]          36,864\n",
      "       BatchNorm2d-9          [-1, 64, 32, 129]             128\n",
      "             ReLU-10          [-1, 64, 32, 129]               0\n",
      "       BasicBlock-11          [-1, 64, 32, 129]               0\n",
      "           Conv2d-12          [-1, 64, 32, 129]          36,864\n",
      "      BatchNorm2d-13          [-1, 64, 32, 129]             128\n",
      "             ReLU-14          [-1, 64, 32, 129]               0\n",
      "           Conv2d-15          [-1, 64, 32, 129]          36,864\n",
      "      BatchNorm2d-16          [-1, 64, 32, 129]             128\n",
      "             ReLU-17          [-1, 64, 32, 129]               0\n",
      "       BasicBlock-18          [-1, 64, 32, 129]               0\n",
      "           Conv2d-19          [-1, 128, 16, 65]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 16, 65]             256\n",
      "             ReLU-21          [-1, 128, 16, 65]               0\n",
      "           Conv2d-22          [-1, 128, 16, 65]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 16, 65]             256\n",
      "           Conv2d-24          [-1, 128, 16, 65]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 16, 65]             256\n",
      "             ReLU-26          [-1, 128, 16, 65]               0\n",
      "       BasicBlock-27          [-1, 128, 16, 65]               0\n",
      "           Conv2d-28          [-1, 128, 16, 65]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 16, 65]             256\n",
      "             ReLU-30          [-1, 128, 16, 65]               0\n",
      "           Conv2d-31          [-1, 128, 16, 65]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 16, 65]             256\n",
      "             ReLU-33          [-1, 128, 16, 65]               0\n",
      "       BasicBlock-34          [-1, 128, 16, 65]               0\n",
      "           Conv2d-35           [-1, 256, 8, 33]         294,912\n",
      "      BatchNorm2d-36           [-1, 256, 8, 33]             512\n",
      "             ReLU-37           [-1, 256, 8, 33]               0\n",
      "           Conv2d-38           [-1, 256, 8, 33]         589,824\n",
      "      BatchNorm2d-39           [-1, 256, 8, 33]             512\n",
      "           Conv2d-40           [-1, 256, 8, 33]          32,768\n",
      "      BatchNorm2d-41           [-1, 256, 8, 33]             512\n",
      "             ReLU-42           [-1, 256, 8, 33]               0\n",
      "       BasicBlock-43           [-1, 256, 8, 33]               0\n",
      "           Conv2d-44           [-1, 256, 8, 33]         589,824\n",
      "      BatchNorm2d-45           [-1, 256, 8, 33]             512\n",
      "             ReLU-46           [-1, 256, 8, 33]               0\n",
      "           Conv2d-47           [-1, 256, 8, 33]         589,824\n",
      "      BatchNorm2d-48           [-1, 256, 8, 33]             512\n",
      "             ReLU-49           [-1, 256, 8, 33]               0\n",
      "       BasicBlock-50           [-1, 256, 8, 33]               0\n",
      "AdaptiveAvgPool2d-51            [-1, 256, 1, 1]               0\n",
      "          Flatten-52                  [-1, 256]               0\n",
      "           Linear-53                  [-1, 128]          32,896\n",
      "      BatchNorm1d-54                  [-1, 128]             256\n",
      "          Dropout-55                  [-1, 128]               0\n",
      "           Linear-56                   [-1, 64]           8,256\n",
      "      BatchNorm1d-57                   [-1, 64]             128\n",
      "          Dropout-58                   [-1, 64]               0\n",
      "           Linear-59                    [-1, 8]             520\n",
      "          Softmax-60                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 2,824,840\n",
      "Trainable params: 42,056\n",
      "Non-trainable params: 2,782,784\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 78.84\n",
      "Params size (MB): 10.78\n",
      "Estimated Total Size (MB): 90.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "model = MyModel_ResNet18()\n",
    "summary(model, (3,128,513))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b4b15",
   "metadata": {},
   "source": [
    "## RGB Dataset\n",
    "Resnet18 expects RGB input images, so our dataset need to be converted from a one to a three-channel. We'll do dat by copying three times the same image in the three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e94008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDatasetRGB(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "                \n",
    "        #do ResNet18 normalization:\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        #copy the channel 3 times (need to unsqueeze to create a new dimension first)\n",
    "        #print(\"DATASET*  sample shape is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        stft_vector = stft_vector.unsqueeze(0).repeat(3,1,1)\n",
    "        stft_vector = stft_vector.to(torch.float32) #float32 needed for ResNet18 model (downcast from float64)\n",
    "        #print(\"DATASET* sample shape after repeat is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        #print(\"stft_vector dtype:\",stft_vector.dtype)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c24605b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor batch_idx, batch  in enumerate(train_data_loader):\\n    print (\"batch index:\",batch_idx)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    \\n    for idx, sample in enumerate(inputs):\\n        label = labels[idx]\\n        print(\"inputs: shape:\",inputs.shape)\\n        print(\"sample: shape:\",sample.shape)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform_ResNet18 = transforms.Compose([\n",
    "    transforms.Normalize(mean= [loaded_mean,loaded_mean,loaded_mean], std=[loaded_std,loaded_std,loaded_std]) #our values\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #ResNet18 specific values\n",
    "])\n",
    "\n",
    "train_dataset = MyDatasetRGB(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDatasetRGB(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDatasetRGB(test_file_paths, Y_test,  transform = transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size = 10, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f3747da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1c5265a49e42f3a3a7b5029c628150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5],Train Loss: 1.7927. Train Accuracy: 48.46484375 Val Loss: 1.7948411099910737 Val Accuracy: tensor([47.2500])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c6a01c47974d60bb6d55abbba7a9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5],Train Loss: 1.7411. Train Accuracy: 53.0828125 Val Loss: 1.783997517593205 Val Accuracy: tensor([48.2250])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043f723519d0430c9988181f41b0d79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5],Train Loss: 1.7272. Train Accuracy: 54.36015625 Val Loss: 1.7856278691217304 Val Accuracy: tensor([48.1125])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98bf66eac854c6187c8d1695ad6714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5],Train Loss: 1.7195. Train Accuracy: 55.05859375 Val Loss: 1.791919251486659 Val Accuracy: tensor([47.6188])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9f985d3244435fbd98fbbcaf79c033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5],Train Loss: 1.7120. Train Accuracy: 55.890625 Val Loss: 1.7769502711519598 Val Accuracy: tensor([49.1063])\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "#todo: convert input tensor from float64 to double\n",
    "\n",
    "#todo normalize again using resnet18 suggested mean and std\n",
    "#TODO: FIX TEST FUNCTION USING RGB (maybe already works)\n",
    "#A member not well specificated didn't read the line above this one and completely wasted 6 golden hours getting \n",
    "#the error just 5 minutes after he went out from his house.\n",
    "\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=128, num_epochs=5, learning_rate=0.001,verbose=False, RGB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b43d6a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./best_models/results/NNet_Raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d750693d894e9abbbd0830b7f9d9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/9],Train Loss: 1.9081. Train Accuracy: 36.209375 Val Loss: 1.9033585867509246 Val Accuracy: tensor([36.6875])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3026bae76e8248fa96d435cc0e2bac4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/9],Train Loss: 1.8704. Train Accuracy: 39.68046875 Val Loss: 1.8821116178780795 Val Accuracy: tensor([38.4313])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5bc707b984483b92b46157472ce026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/9],Train Loss: 1.8518. Train Accuracy: 41.653125 Val Loss: 1.8654355867728591 Val Accuracy: tensor([40.1188])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c464bba15adf489a8e65a4b576fe3550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/9],Train Loss: 1.8400. Train Accuracy: 42.8296875 Val Loss: 1.8804640002697706 Val Accuracy: tensor([38.5812])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b61383d294f2abbc8076dd845d6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/9],Train Loss: 1.8357. Train Accuracy: 43.2171875 Val Loss: 1.8809217572957277 Val Accuracy: tensor([38.5687])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4154980199a5408f871b7dc2a994acff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/9],Train Loss: 1.8293. Train Accuracy: 43.86484375 Val Loss: 1.845809885494411 Val Accuracy: tensor([42.0812])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd079bcf59e1413fbd0d39481d648d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/9],Train Loss: 1.8249. Train Accuracy: 44.26328125 Val Loss: 1.844606993086636 Val Accuracy: tensor([42.4313])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d49f7b9d9f44524b4e928efe06c33a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/9],Train Loss: 1.8219. Train Accuracy: 44.684375 Val Loss: 1.8478830751031636 Val Accuracy: tensor([41.6562])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963371169a434718a8dac42598d13ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/9],Train Loss: 1.8189. Train Accuracy: 44.88984375 Val Loss: 1.8483450606167315 Val Accuracy: tensor([41.8750])\n",
      "Trained with learning rate= 0.001  and with regularization term= 0.0001\n"
     ]
    }
   ],
   "source": [
    "#TODO: Add batch_size optimization loop\n",
    "save_directory=\"./best_models/\"\n",
    "\n",
    "learning_rate_list = [0.001]\n",
    "reg_list=[0.0001]\n",
    "epochs=9\n",
    "\n",
    "for i in learning_rate_list:\n",
    "    for j in reg_list:\n",
    "        filename=save_directory+\"results/NNet_Raw\"        \n",
    "        print(filename)\n",
    "        model = NNet_Raw()\n",
    "        train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=64, num_epochs=epochs, learning_rate=i, reg=j)\n",
    "        print(\"Trained with learning rate=\",i,\" and with regularization term=\",j)\n",
    "        torch.save(model.state_dict(), save_directory+\"best_models/NNet_Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c13ed2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([62.6583]), 1.6450505791631773)\n"
     ]
    }
   ],
   "source": [
    "model_path=\"./best_models/best_models/\"\n",
    "model_name=\"NNet1\"\n",
    "\n",
    "test_model=NNet1()\n",
    "test_model.load_state_dict(torch.load(model_path+model_name), strict=False)\n",
    "\n",
    "print(test(test_model,train_dataset, Y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f31c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
