{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15945d26",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b6eab989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tqdm.notebook as tq\n",
    "import utils\n",
    "from pydub import AudioSegment\n",
    "from tkinter import Tcl # file sorting by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7763c",
   "metadata": {},
   "source": [
    "# Creation of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8ded2",
   "metadata": {},
   "source": [
    "### Dictionary creation for the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c4013",
   "metadata": {},
   "source": [
    "We want a dictionary indicating a numbeer for each genre:\n",
    "\n",
    "{0: 'Hip-Hop', 1: 'Pop', 2: 'Folk', 3: 'Rock', 4: 'Experimental', 5: 'International', 6: 'Electronic', 7: 'Instrumental'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e985c",
   "metadata": {},
   "source": [
    "### Creation of the labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "0be9951a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_folder: data/fma_small_stft_transposed_22050_overlapped/train\n",
      "validation_folder: data/fma_small_stft_transposed_22050_overlapped/validation\n",
      "test_folder: data/fma_small_stft_transposed_22050_overlapped/test \n",
      "\n",
      "audio directory:  ./data/fma_small/\n",
      "Loading tracks.csv...\n",
      "small dataset shape: (8000, 52)\n",
      "Track.csv: 6400 training samples, 800 validation samples, 800 test samples\n",
      "\n",
      "there are 8 unique genres\n",
      "Dictionary of genres created: {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Rock': 3, 'Experimental': 4, 'International': 5, 'Electronic': 6, 'Instrumental': 7}\n",
      "labels length: 128000\n",
      "labels length: 16000\n",
      "labels length: 16000\n"
     ]
    }
   ],
   "source": [
    "def create_single_dataset(folder_path, tracks_dataframe, genre_dictionary):    \n",
    "    labels = []\n",
    "   \n",
    "    _, file_list = get_sorted_file_paths(folder_path)\n",
    "    \n",
    "    for i,file in enumerate(file_list):\n",
    "        #print(\"considering file:\",file, \"({}/{})\".format(i,len(file_list)))\n",
    "        track_id_clip_id = file.split('.')[0]\n",
    "        track_id = track_id_clip_id.split('_')[0]\n",
    "        #print(\"track id with clip: {}, track id: {}\".format(track_id_clip_id, track_id))\n",
    "        genre = tracks_dataframe.loc[int(track_id)]\n",
    "        #print(\"genre from dataframe: \", genre)\n",
    "        label = genre_dictionary[genre]\n",
    "        #print(\"label from dictionary:\",label)\n",
    "        labels.append(label)\n",
    "    print(\"labels length: {}\".format(len(labels)))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "#create the train,validation and test vectors using the files in the train/validation/test folders\n",
    "def create_dataset_splitted(folder_path):\n",
    "    train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "    validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "    test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "    \n",
    "    print(\"train_folder:\",train_folder)\n",
    "    print(\"validation_folder:\",validation_folder)\n",
    "    print(\"test_folder:\",test_folder,\"\\n\")\n",
    "    \n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "    \n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "\n",
    "    small_training_top_genres = small_training['genre_top']\n",
    "    small_validation_top_genres = small_validation['genre_top']\n",
    "    small_test_top_genres = small_test['genre_top']\n",
    "    \n",
    "    #create dictionary of genre classes:\n",
    "    unique_genres = small_training_top_genres.unique()\n",
    "    unique_genres = np.array(unique_genres)\n",
    "    print(\"there are {} unique genres\".format(len(unique_genres)))\n",
    "    genre_dictionary = {}\n",
    "    for i,genre in enumerate(unique_genres):\n",
    "        genre_dictionary[genre] = i\n",
    "    print(\"Dictionary of genres created:\",genre_dictionary)\n",
    "    \n",
    "    \n",
    "    Y_train = create_single_dataset(train_folder, small_training_top_genres, genre_dictionary)\n",
    "    Y_validation = create_single_dataset(validation_folder, small_validation_top_genres, genre_dictionary)\n",
    "    Y_test = create_single_dataset(test_folder, small_test_top_genres, genre_dictionary)\n",
    "    \n",
    "    return Y_train, Y_validation, Y_test\n",
    " \n",
    "def get_sorted_file_paths(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #sort the dataset files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_list = Tcl().call('lsort', '-dict', file_list) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...\n",
    "    file_paths = [os.path.join(folder_path, file_name) for file_name in file_list] #join filename with folder path\n",
    "    #print(\"There are {} in the folder: {}\".format(len(file_list),file_list))\n",
    "    return file_paths, file_list\n",
    "    \n",
    "    \n",
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "Y_train, Y_validation, Y_test = create_dataset_splitted(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20175a8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "187ddbe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "cae70891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  128000\n",
      "len of validation dataset:  16000\n",
      "len of test dataset:  16000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDataset(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ef67f394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_for_dimension_errors(filepaths):\n",
    "    error_indexes = []\n",
    "    progress = 0\n",
    "    for file in filepaths:\n",
    "        progress+=1\n",
    "        print(\"checked {}/{} files\".format(progress,len(filepaths)))\n",
    "        x = np.load(file)\n",
    "        if(x.shape != (128,513)):\n",
    "            error_indexes.append(x)\n",
    "            print(\"error\")\n",
    "    print(\"{} errors found in files: {}\".format(len(error_indexes),error_indexes))\n",
    "    for idx,error in enumerate(error_indexes):\n",
    "        print(\"index: {}, shape: {}\".format(idx,error.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd9c68",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "We will use Z-Score to normalize the training, validation and test set by calculating the mean and the std deviation on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f571ada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_mean'\n",
    "std_save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_std_deviation'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677263cf",
   "metadata": {},
   "source": [
    "## Calculation of mean and standard deviation (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5795abf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum: tensor(8.9296e+09)\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum=0\n",
    "\n",
    "#iter all the training set by batches and calculate the sum of all the sample values (513*128 values for each sample)\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum:\",current_sum)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "        current_sum += torch.sum(sample)\n",
    "        #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum:\",current_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c794bf2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of training set: tensor(1.0629)\n",
      "Saving the mean in file: ./data/fma_small_stft_transposed_22050_overlapped/train_mean\n"
     ]
    }
   ],
   "source": [
    "mean = current_sum/(len(train_dataset)*513*128) #divide the sum for the total number of values considerated\n",
    "print(\"mean of training set:\",mean)\n",
    "\n",
    "print(\"Saving the mean in file:\",save_filename) \n",
    "np.save(save_filename,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec52eb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum of squares: tensor(7.8299e+10)\n"
     ]
    }
   ],
   "source": [
    "#now let's calculate the standard deviation (squared root of the variance)\n",
    "\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum_of_squares = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum_of_squares:\",current_sum_of_squares)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample shape\",sample.shape)\n",
    "        for row in sample:\n",
    "            #print(\"row shape:\",row.shape)\n",
    "            for elem in row:\n",
    "                #print(\"elem: shape\",elem.shape,\"content:\",elem)\n",
    "                difference = elem - mean\n",
    "                difference_squared = difference**2\n",
    "                current_sum_of_squares += difference_squared\n",
    "                #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum of squares:\",current_sum_of_squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cecfb0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance: tensor(9.3202)\n",
      "std_deviation: 3.0528938510507846\n",
      "Saving the std_deviation in file: ./data/fma_small_stft_transposed_22050_overlapped/train_std_deviation\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "variance = current_sum_of_squares/((len(train_dataset) * 513 * 128)-1)\n",
    "std_deviation = math.sqrt(variance)\n",
    "\n",
    "print(\"variance:\",variance)\n",
    "print(\"std_deviation:\",std_deviation)\n",
    "\n",
    "print(\"Saving the std_deviation in file:\",std_save_filename) \n",
    "np.save(std_save_filename,std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fc2ab",
   "metadata": {},
   "source": [
    "## Load calculated mean and std deviation from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "977c1ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean: 1.0629134\n",
      "loaded std: 3.0528938510507846\n"
     ]
    }
   ],
   "source": [
    "loaded_mean = np.load(save_filename+'.npy')\n",
    "print(\"loaded mean:\",loaded_mean)\n",
    "\n",
    "loaded_std = np.load(std_save_filename+'.npy')\n",
    "print(\"loaded std:\",loaded_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d4804",
   "metadata": {},
   "source": [
    "## Create the normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "092efbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#verify that the normalization has worked\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\\ntotal_n_batches = len(train_loader)/batch_size\\n\\n#iter all the training set by batches and normalize the value of each tensor\\nfor batch_idx, batch in enumerate(train_loader):\\n    print(\"batch\",batch_idx,\"/\",total_n_batches)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\\n    print(\"labels:\",labels)\\n    for sample in inputs:\\n        print(\"sample: shape\",sample.shape,\"content:\",sample)\\n   \\nvalidation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\\ntotal_n_batches = len(validation_loader)/batch_size\\n\\n#iter all the training set by batches and normalize the value of each tensor\\nfor batch_idx, batch in enumerate(validation_loader):\\n    print(\"batch\",batch_idx,\"/\",total_n_batches)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\\n    print(\"labels:\",labels)\\n    for sample in inputs:\\n        print(\"sample: shape\",sample.shape,\"content:\",sample)\\n'"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean= loaded_mean, std= loaded_std)\n",
    "])\n",
    "\n",
    "train_dataset = MyDataset(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test,  transform = transform)\n",
    "\n",
    "'''\n",
    "#verify that the normalization has worked\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "total_n_batches = len(train_loader)/batch_size\n",
    "\n",
    "#iter all the training set by batches and normalize the value of each tensor\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(\"batch\",batch_idx,\"/\",total_n_batches)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "   \n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\n",
    "total_n_batches = len(validation_loader)/batch_size\n",
    "\n",
    "#iter all the training set by batches and normalize the value of each tensor\n",
    "for batch_idx, batch in enumerate(validation_loader):\n",
    "    print(\"batch\",batch_idx,\"/\",total_n_batches)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a52dc",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "83f745b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.bn4 = nn.BatchNorm1d(300)\n",
    "        self.dense2 = nn.Linear(300,150)\n",
    "        self.bn5 = nn.BatchNorm1d(150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f8734822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet2, self).__init__()\n",
    "        self.drop=nn.Dropout(0.2)\n",
    "        # STFT spectrogram input: (batch_size, 1, 128, 513)\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513))\n",
    "        self.batch1=nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128,128, kernel_size=(4, 1),padding=1)\n",
    "        self.batch2=nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=(4, 1),padding=2)\n",
    "        self.batch3=nn.BatchNorm2d(128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(128,1))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(128,1))\n",
    "        self.fc1 = nn.Linear(250,128)\n",
    "        self.bn1=nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.bn2=nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 8)  # 8 classes for genre predictions\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x=self.batch1(x)\n",
    "        x = torch.relu(x)\n",
    "        y=x\n",
    "        x = self.conv2(x)\n",
    "        x=self.batch2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x=self.batch3(x)\n",
    "        x = torch.relu(x)\n",
    "        # Sum between the first and third conv layers\n",
    "        x = x[:, :, :, 0] + y[:, :, :, 0]\n",
    "        \n",
    "        x = torch.relu(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x=self.drop(x)\n",
    "        x=self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)        \n",
    "        x=self.drop(x)\n",
    "        x=self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845a53e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0d034d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "LEARNING_RATE=0.0001\n",
    "\n",
    "learning_rate_list = [0.01, 0.001, 0.0001]\n",
    "batch_size_list = [128,256,512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd576a3",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "223742e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model, validation_dataset, Y_validation):\n",
    "    # Stop parameters learning\n",
    "    model.eval()\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confusion_matrix = np.zeros((8, 8), dtype=int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample, label in validation_loader:\n",
    "            \n",
    "            sample = sample.unsqueeze(1)\n",
    "\n",
    "            # Predict label\n",
    "            output = model(sample)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            max_index = torch.argmax(output).item()  # The index with maximum probability\n",
    "\n",
    "            confusion_matrix[label][max_index] += 1\n",
    "\n",
    "            correct += (max_index == label)\n",
    "\n",
    "    cm = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "    cm.plot()\n",
    "    print(confusion_matrix)\n",
    "    accuracy = 100 * correct / len(Y_validation)\n",
    "    average_loss = total_loss / len(Y_validation)\n",
    "\n",
    "    model.train()\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a0ca4e98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, num_epochs, learning_rate, verbose = False, RGB=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    counted_labels=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", dtype:\",inputs.dtype,\" content: \",inputs)\n",
    "                print(\"min value:\",torch.min(inputs))\n",
    "                print(\"max value:\",torch.max(inputs))\n",
    "                print(\"\\nlabels shape:\",labels.size(),\",dtype:\",labels.dtype,\", content: \",labels)\n",
    "            if(RGB==False):\n",
    "                inputs = inputs.unsqueeze(1) #add a dimension if input is to be considered just grayscale\n",
    "                #if input is RGB, there are already 3 channels\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if(verbose == True):\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "                print(\"List of labels until now:\",counted_labels)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                counted_labels[labels[index].item()]+=1\n",
    "                if(labels[index].item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "        \n",
    "        #calculate train loss and accuracy\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        train_loss_list.append(average_loss)\n",
    "        train_acc_list.append(average_accuracy)\n",
    "        \n",
    "        #calculate validation loss and accuracy\n",
    "        val_acc, val_loss = test(model, validation_dataset, Y_validation)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],Train Loss: {average_loss:.4f}. Train Accuracy: {average_accuracy} Val Loss: {val_loss} Val Accuracy: {val_acc}\")\n",
    "        progress_bar.close()\n",
    "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbbc7adc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NNet1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d6742b0e5792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rate_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_size_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNet1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#reinitialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_lr_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'_bs_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Traing model with batch size={bs}, lr={lr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NNet1' is not defined"
     ]
    }
   ],
   "source": [
    "#summary(model, (1, 128, 513))\n",
    "performance_list = []\n",
    "model_list = []\n",
    "directory = 'models/'\n",
    "\n",
    "for lr in learning_rate_list:   \n",
    "    for bs in batch_size_list:\n",
    "        model = NNet1() #reinitialize model\n",
    "        file_path = directory + 'model' + '_lr_' + str(lr).split('.')[0] + '-' + str(lr).split('.')[1]+ '_bs_' + str(bs)\n",
    "        print(f\"Traing model with batch size={bs}, lr={lr}\")\n",
    "        performance=train(model, train_dataset, batch_size=bs, num_epochs=EPOCHS, learning_rate=lr, verbose=True)\n",
    "        performance_list.append(performance)\n",
    "        model_list.append(model)\n",
    "        print(\"saving model in\", file_path)\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "        print(performance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58d928b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plt.plot(np.arange(1,EPOCHS+1), performance[1][:], label='Loss') \n",
    "plt.plot(np.arange(1,EPOCHS+1), performance[0][:], label='Accuracy')\n",
    "plt.legend()  # Display the legend showing the labels\n",
    "plt.show()\n",
    "print(performance[0][:])\n",
    "'''\n",
    "\n",
    "parameter_list = []\n",
    "for lr in learning_rate_list:\n",
    "    for bs in batch_size_list:\n",
    "        parameter_list.append([lr,bs])\n",
    "\n",
    "for performance, parameters in zip(performance_list, parameter_list):\n",
    "    plt.plot(np.arange(1,EPOCHS+1), performance[1][:], label='Val Loss') \n",
    "    plt.plot(np.arange(1,EPOCHS+1), performance[0][:], label='Val Accuracy')\n",
    "    plt.title(f\"Lr = {parameters[0]}, Batch size = {parameters[1]}\")\n",
    "    plt.legend()  # Display the legend showing the labels\n",
    "    plt.show()\n",
    "    print(performance[0][:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef7fab",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1 + BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b3685ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1_Small(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1_Small, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(2, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv2 = nn.Conv2d(64,128, kernel_size=(2, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv3 = nn.Conv2d(128,64, kernel_size=(4, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(2, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(256, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dense2 = nn.Linear(128,64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.dense3 = nn.Linear(64, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dense2(x)\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.bn5(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "61b169d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 125, 1]         262,784\n",
      "       BatchNorm2d-2          [-1, 128, 125, 1]             256\n",
      "            Conv2d-3          [-1, 128, 124, 3]          65,664\n",
      "       BatchNorm2d-4          [-1, 128, 124, 3]             256\n",
      "            Conv2d-5          [-1, 128, 125, 7]          65,664\n",
      "       BatchNorm2d-6          [-1, 128, 125, 7]             256\n",
      "         MaxPool2d-7               [-1, 1, 125]               0\n",
      "         AvgPool2d-8               [-1, 1, 125]               0\n",
      "            Linear-9                  [-1, 128]          32,128\n",
      "          Dropout-10                  [-1, 128]               0\n",
      "      BatchNorm1d-11                  [-1, 128]             256\n",
      "           Linear-12                   [-1, 64]           8,256\n",
      "          Dropout-13                   [-1, 64]               0\n",
      "      BatchNorm1d-14                   [-1, 64]             128\n",
      "           Linear-15                    [-1, 8]             520\n",
      "          Softmax-16                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 436,168\n",
      "Trainable params: 436,168\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 2.69\n",
      "Params size (MB): 1.66\n",
      "Estimated Total Size (MB): 4.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_NNet1_Small = NNet2()\n",
    "summary(model_NNet1_Small, (1, 128, 513))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model_NNet1_Small, train_dataset, batch_size=128, num_epochs=20, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343371e",
   "metadata": {},
   "source": [
    "# Raw audio (1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabb348",
   "metadata": {},
   "source": [
    "## Load raw audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8c527e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#returns file paths list for train, validation and test\n",
    "def get_file_paths():\n",
    "    folder_raw = './data/fma_small'\n",
    "    file_paths_raw = []\n",
    "\n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "    \n",
    "    #--------------TRAIN----------------------\n",
    "    print(\"Creating train dataset...\")\n",
    "    file_paths_train = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_train = np.array(small_training.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_train)} samples. Here's the first ones: {track_ids_train[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_train:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_train.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_train = np.array(Tcl().call('lsort', '-dict', file_paths_train)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    #--------------VALIDATION----------------------\n",
    "    print(\"Creating validation dataset...\")\n",
    "    file_paths_validation = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_validation = np.array(small_validation.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_validation)} samples. Here's the first ones: {track_ids_validation[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_validation:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_validation.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_validation = np.array(Tcl().call('lsort', '-dict', file_paths_validation)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    #--------------TEST----------------------\n",
    "    print(\"Creating test dataset...\")\n",
    "    file_paths_test = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_test = np.array(small_test.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_test)} samples. Here's the first ones: {track_ids_test[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_test:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_test.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_test = np.array(Tcl().call('lsort', '-dict', file_paths_test)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    \n",
    "    #create the datasets\n",
    "    \n",
    "    return file_paths_train, file_paths_validation, file_paths_test\n",
    "\n",
    "#returns file paths list for train, validation and test for each 3s clip\n",
    "def get_file_paths_split():\n",
    "    folder_raw = './data/fma_small_raw_array_22050_overlapped'\n",
    "    folder_train = folder_raw + '/train'\n",
    "    folder_validation = folder_raw + '/validation'\n",
    "    folder_test = folder_raw + '/test'\n",
    "    file_paths_raw = []\n",
    "\n",
    "    #--------------TRAIN----------------------\n",
    "    print(\"Creating train dataset...\")\n",
    "    file_paths_train = os.listdir(folder_train)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_train = np.array(Tcl().call('lsort', '-dict', file_paths_train)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_train = []\n",
    "    for file in file_paths_train:\n",
    "        complete_file_path_train.append(folder_train + '/' + file)\n",
    "\n",
    "    #--------------VALIDATION----------------------\n",
    "    print(\"Creating validation dataset...\")\n",
    "    file_paths_validation = os.listdir(folder_validation)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_validation = np.array(Tcl().call('lsort', '-dict', file_paths_validation)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_validation = []\n",
    "    for file in file_paths_validation:\n",
    "        complete_file_path_validation.append(folder_validation + '/' + file)\n",
    "\n",
    "    #--------------TEST----------------------\n",
    "    print(\"Creating test dataset...\")\n",
    "    file_paths_test = os.listdir(folder_test)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_test = np.array(Tcl().call('lsort', '-dict', file_paths_test)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_test = []\n",
    "    for file in file_paths_test:\n",
    "        complete_file_path_test.append(folder_test + '/' + file)\n",
    "    \n",
    "    return complete_file_path_train, complete_file_path_validation, complete_file_path_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "99825d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset...\n",
      "Creating validation dataset...\n",
      "Creating test dataset...\n",
      "128000\n",
      "16000\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "file_paths_train, file_paths_validation, file_paths_test = get_file_paths_split()\n",
    "print(len(file_paths_train))\n",
    "print(len(file_paths_validation))\n",
    "print(len(file_paths_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891ab30",
   "metadata": {},
   "source": [
    "## Fix wrong sampling rates in the dataset (to be run just once)\n",
    "\n",
    "Some songs in the fma_small dataset has a different sampling rate from 44100. Let's fix them by using librosa resampling tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f82d99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "#fix sampling rate\n",
    "target_sr = 44100\n",
    "wrong_sr_list = []\n",
    "\n",
    "\"\"\"\n",
    "print(\"\\n******Checking sample rates to be\",target_sr,\"in train set\")\n",
    "for file in file_paths_train:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "   \"\"\"     \n",
    "print(\"\\n*****Checking sample rates to be\",target_sr,\"in validation set\")\n",
    "for file in file_paths_validation:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "\n",
    "        \n",
    "print(\"\\n*****Checking sample rates to be\",target_sr,\"in test set\")\n",
    "for file in file_paths_test:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "        \n",
    "print(\"There were a total of\",len(wrong_sr_list),\"files with wrong sampling rate. Now they have been corrected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac83619",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2456f5f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyDatasetRaw(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose=False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        raw_vector = np.load(file_path).astype(np.int16) # Ensure int16 data type\n",
    "        if(self.verbose==True):\n",
    "            print(\"raw vector shape:\",raw_vector.shape)\n",
    "        raw_vector = torch.tensor(raw_vector)\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            #convert to float64 tensor\n",
    "            raw_vector = raw_vector.double()\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.unsqueeze(raw_vector, dim=0)\n",
    "            #print(\"TRANSFORM: after first unsqueeze:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.unsqueeze(raw_vector, dim=0) #unsqueeze two times (needed for torchvision normalize method)\n",
    "            #print(\"TRANSFORM: after second unsqueeze:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = self.transform(raw_vector) #normalize the sample\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "            raw_vector = torch.squeeze(raw_vector, dim=0)\n",
    "            raw_vector = torch.squeeze(raw_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after double squeeze shape:\",raw_vector.shape,\"content:\",raw_vector)\n",
    "\n",
    "        \n",
    "        return raw_vector, label\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc60c1",
   "metadata": {},
   "source": [
    "# Normalization of raw audio\n",
    "We calculate mean and std deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "4a1bd6d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_save_filename_raw = './data/fma_small_raw_array_22050_overlapped/train_mean'\n",
    "std_save_filename_raw = './data/fma_small_raw_array_22050_overlapped/train_std_deviation'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499a8ca",
   "metadata": {},
   "source": [
    "## Calculation of mean raw (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3257908",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDatasetRaw(file_paths_train, Y_train)\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum=0\n",
    "\n",
    "#iter all the training set by batches and calculate the sum of all the sample values (513*128 values for each sample)\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if(batch_idx%1000==0):\n",
    "        print(\"batch\",batch_idx,\"/\",total_n_batches,\"(\",round((batch_idx/len(train_dataset)*100)),\"%), current_sum:\",current_sum)\n",
    "    \n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "        current_sum += torch.sum(sample)\n",
    "       \n",
    "        #print(\"type of current_sum:\",current_sum.dtype)\n",
    "        #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum:\",current_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "86a4c9c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_sum tensor(-143807621387)\n",
      "mean of training set: tensor(-16.9841)\n",
      "Saving the mean in file: ./data/fma_small_raw_array_22050_overlapped/train_mean\n"
     ]
    }
   ],
   "source": [
    "print(\"current_sum\",current_sum)\n",
    "mean_raw = current_sum/(len(train_dataset)*66150) #divide the sum for the total number of values considerated\n",
    "print(\"mean of training set:\",mean_raw)\n",
    "\n",
    "print(\"Saving the mean in file:\",mean_save_filename_raw) \n",
    "np.save(mean_save_filename_raw,mean_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2e6f0",
   "metadata": {},
   "source": [
    "## Calculation of std deviation raw (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2076439",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#now let's calculate the standard deviation (squared root of the variance)\n",
    "\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum_of_squares = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if(batch_idx%1000==0):\n",
    "        print(\"batch\",batch_idx,\"/\",total_n_batches,round((batch_idx/len(train_dataset)*100)),\"%, current_sum_of_squares:\",current_sum_of_squares)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\\n\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for elem in inputs:\n",
    "        elem = elem.double() #convert to float64 for precise calculations\n",
    "        #print(\"elem: shape\",elem.shape,\"content:\\n\",elem)\n",
    "        difference = elem - mean_raw\n",
    "        #print(\"difference: shape:\",difference.shape,\"content:\\n\", difference)\n",
    "        difference_squared = difference**2\n",
    "        #print(\"difference_squared: shape:\",difference_squared.shape,\"content:\\n\", difference_squared)\n",
    "        current_sum_of_squares += torch.sum(difference_squared)\n",
    "        #print(\"current_sum_of_squares:\",current_sum_of_squares)\n",
    "print(\"final sum of squares:\",current_sum_of_squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ce7a2107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance raw: tensor(1066987.8132, dtype=torch.float64)\n",
      "std_deviation_raw: 1032.9510216986293\n",
      "Saving the std_deviation_raw in file: ./data/fma_small_raw_array_22050_overlapped/train_std_deviation\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "variance_raw = current_sum_of_squares/((len(train_dataset)*66150)-1)\n",
    "std_deviation_raw = math.sqrt(variance)\n",
    "\n",
    "print(\"variance raw:\",variance)\n",
    "print(\"std_deviation_raw:\",std_deviation_raw)\n",
    "\n",
    "print(\"Saving the std_deviation_raw in file:\",std_save_filename_raw) \n",
    "np.save(std_save_filename_raw,std_deviation_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f270c0",
   "metadata": {},
   "source": [
    "# Load mean and std from file (fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "0501d1ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean: -16.984083\n",
      "loaded std: 1032.9510216986293\n"
     ]
    }
   ],
   "source": [
    "loaded_mean_raw = np.load(mean_save_filename_raw+'.npy')\n",
    "print(\"loaded mean:\",loaded_mean_raw)\n",
    "\n",
    "loaded_std_raw = np.load(std_save_filename_raw+'.npy')\n",
    "print(\"loaded std:\",loaded_std_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7112c8",
   "metadata": {},
   "source": [
    "## Create dataset for raw audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c5180347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#verify that the normalization has worked\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\\ntotal_n_batches = len(train_loader)/batch_size\\n\\n#iter all the training set by batches and normalize the value of each tensor\\nfor batch_idx, batch in enumerate(train_loader):\\n    print(\"batch\",batch_idx,\"/\",total_n_batches)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\\n    print(\"labels:\",labels)\\n    for sample in inputs:\\n        print(\"sample: shape\",sample.shape,\"content:\",sample)\\n'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean= loaded_mean_raw, std= loaded_std_raw)\n",
    "])\n",
    "\n",
    "train_dataset = MyDatasetRaw(file_paths_train, Y_train, transform=transform)\n",
    "validation_dataset = MyDatasetRaw(file_paths_validation, Y_validation, transform=transform)\n",
    "test_dataset = MyDatasetRaw(file_paths_test, Y_test, transform=transform)\n",
    "\n",
    "\n",
    "'''\n",
    "#verify that the normalization has worked\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "total_n_batches = len(train_loader)/batch_size\n",
    "\n",
    "#iter all the training set by batches and normalize the value of each tensor\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(\"batch\",batch_idx,\"/\",total_n_batches)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c432ac",
   "metadata": {},
   "source": [
    "# Networks for raw audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "68b4a333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet_Raw(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(NNet_Raw, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=16)\n",
    "        self.conv2 = nn.Conv1d(16, 8, kernel_size=16)\n",
    "        self.conv3 = nn.Conv1d(25,25, kernel_size=16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=128)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=4)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(8)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(24)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(16)\n",
    "        self.inBN = nn.BatchNorm1d(1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(256, 24)\n",
    "        #self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(24, 8)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.inBN(x.float())\n",
    "        x=self.maxpool1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x=self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm2(x)        \n",
    "        #x = self.conv3(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.batchnorm4(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.batchnorm4(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "0c90b296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1             [-1, 1, 66150]               2\n",
      "         MaxPool1d-2             [-1, 1, 16537]               0\n",
      "            Conv1d-3            [-1, 16, 16522]             272\n",
      "              ReLU-4            [-1, 16, 16522]               0\n",
      "       BatchNorm1d-5            [-1, 16, 16522]              32\n",
      "         MaxPool1d-6             [-1, 16, 4130]               0\n",
      "            Conv1d-7              [-1, 8, 4115]           2,056\n",
      "              ReLU-8              [-1, 8, 4115]               0\n",
      "       BatchNorm1d-9              [-1, 8, 4115]              16\n",
      "        MaxPool1d-10                [-1, 8, 32]               0\n",
      "           Linear-11                   [-1, 24]           6,168\n",
      "             ReLU-12                   [-1, 24]               0\n",
      "      BatchNorm1d-13                   [-1, 24]              48\n",
      "          Dropout-14                   [-1, 24]               0\n",
      "           Linear-15                    [-1, 8]             200\n",
      "          Softmax-16                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 8,794\n",
      "Trainable params: 8,794\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 7.94\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 8.23\n",
      "----------------------------------------------------------------\n",
      "NNet_Raw(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(16,), stride=(1,))\n",
      "  (conv2): Conv1d(16, 8, kernel_size=(16,), stride=(1,))\n",
      "  (conv3): Conv1d(25, 25, kernel_size=(16,), stride=(1,))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool1d(kernel_size=128, stride=128, padding=0, dilation=1, ceil_mode=False)\n",
      "  (maxpool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (batchnorm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (inBN): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=24, bias=True)\n",
      "  (fc3): Linear(in_features=24, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MyModel=NNet_Raw()\n",
    "summary(MyModel, (1,66150))\n",
    "print(MyModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ab3f0168",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fc84d9cb1144599a95be0daec3928d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-310-292f0d42fcc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-289-36aa03fb708a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose, RGB)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#labels need to be a vector of class indexes (0-7) of dim (batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fma3/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fma3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(MyModel, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08cc6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "641166af",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# NN on echonest features\n",
    "\n",
    "As you can see below, unfortunately we cannot perform a train using echonest features (danceability, tempo, acousticness, ...) because there are only 1300 tracks with echonest features which are not NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25e87d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening csvs...\n",
      "small dataset shape: (8000, 52)\n",
      "echonest csv shape (only audio features): (13129, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"opening csvs...\")\n",
    "\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "echonest = utils.load('data/fma_metadata/echonest.csv')\n",
    "echonest = echonest['echonest', 'audio_features']\n",
    "small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "\n",
    "print(\"small dataset shape:\",small.shape)\n",
    "print(\"echonest csv shape (only audio features):\",echonest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8b18347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track ids shape: 8000 content: [2, 5, 10, 140, 141, 148, 182, 190, 193, 194] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.416675</td>\n",
       "      <td>0.675894</td>\n",
       "      <td>0.634476</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.177647</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>165.922</td>\n",
       "      <td>0.576661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.745566</td>\n",
       "      <td>0.701470</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.124595</td>\n",
       "      <td>100.260</td>\n",
       "      <td>0.621661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.951670</td>\n",
       "      <td>0.658179</td>\n",
       "      <td>0.924525</td>\n",
       "      <td>0.965427</td>\n",
       "      <td>0.115474</td>\n",
       "      <td>0.032985</td>\n",
       "      <td>111.562</td>\n",
       "      <td>0.963590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.376312</td>\n",
       "      <td>0.734079</td>\n",
       "      <td>0.265685</td>\n",
       "      <td>0.669581</td>\n",
       "      <td>0.085995</td>\n",
       "      <td>0.039068</td>\n",
       "      <td>107.952</td>\n",
       "      <td>0.609991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.963657</td>\n",
       "      <td>0.435933</td>\n",
       "      <td>0.075632</td>\n",
       "      <td>0.345493</td>\n",
       "      <td>0.105686</td>\n",
       "      <td>0.026658</td>\n",
       "      <td>33.477</td>\n",
       "      <td>0.163950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154413</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154414</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155066</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        acousticness  danceability    energy  instrumentalness  liveness  \\\n",
       "2           0.416675      0.675894  0.634476          0.010628  0.177647   \n",
       "5           0.043567      0.745566  0.701470          0.000697  0.373143   \n",
       "10          0.951670      0.658179  0.924525          0.965427  0.115474   \n",
       "140         0.376312      0.734079  0.265685          0.669581  0.085995   \n",
       "141         0.963657      0.435933  0.075632          0.345493  0.105686   \n",
       "...              ...           ...       ...               ...       ...   \n",
       "154308           NaN           NaN       NaN               NaN       NaN   \n",
       "154309           NaN           NaN       NaN               NaN       NaN   \n",
       "154413           NaN           NaN       NaN               NaN       NaN   \n",
       "154414           NaN           NaN       NaN               NaN       NaN   \n",
       "155066           NaN           NaN       NaN               NaN       NaN   \n",
       "\n",
       "        speechiness    tempo   valence  \n",
       "2          0.159310  165.922  0.576661  \n",
       "5          0.124595  100.260  0.621661  \n",
       "10         0.032985  111.562  0.963590  \n",
       "140        0.039068  107.952  0.609991  \n",
       "141        0.026658   33.477  0.163950  \n",
       "...             ...      ...       ...  \n",
       "154308          NaN      NaN       NaN  \n",
       "154309          NaN      NaN       NaN  \n",
       "154413          NaN      NaN       NaN  \n",
       "154414          NaN      NaN       NaN  \n",
       "155066          NaN      NaN       NaN  \n",
       "\n",
       "[8000 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select small dataset from echonest csv\n",
    "\n",
    "track_ids = small.index.values.tolist()\n",
    "print(\"Track ids shape:\",len(track_ids),\"content:\",track_ids[:10],\"...\")\n",
    "echonest_small = pd.DataFrame(echonest,index=track_ids)\n",
    "\n",
    "ipd.display(echonest_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12714b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_echonest: shape: (8000, 8)\n",
      "There are 6706 rows containing NaN only as data\n"
     ]
    }
   ],
   "source": [
    "X_train_echonest = echonest_small.to_numpy(dtype=np.float16)\n",
    "print(\"X_train_echonest: shape:\",X_train_echonest.shape)\n",
    "\n",
    "nan_rows = np.argwhere(np.isnan(X_train_echonest).all(axis=1))\n",
    "\n",
    "print(\"There are\",len(nan_rows),\"rows containing NaN only as data\")\n",
    "#nan_rows = nan_rows.squeeze()\n",
    "#for i in nan_rows:\n",
    "#    print(\"row:\",i,\":\",X_train_echonest[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917bc8e",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "We try to use transfer learning using a ResNet18 by torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "497e31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 24, 64, 257]             648\n",
      "       BatchNorm2d-2          [-1, 24, 64, 257]              48\n",
      "              ReLU-3          [-1, 24, 64, 257]               0\n",
      "         MaxPool2d-4          [-1, 24, 32, 129]               0\n",
      "            Conv2d-5           [-1, 24, 16, 65]             216\n",
      "       BatchNorm2d-6           [-1, 24, 16, 65]              48\n",
      "            Conv2d-7           [-1, 24, 16, 65]             576\n",
      "       BatchNorm2d-8           [-1, 24, 16, 65]              48\n",
      "              ReLU-9           [-1, 24, 16, 65]               0\n",
      "           Conv2d-10          [-1, 24, 32, 129]             576\n",
      "      BatchNorm2d-11          [-1, 24, 32, 129]              48\n",
      "             ReLU-12          [-1, 24, 32, 129]               0\n",
      "           Conv2d-13           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-14           [-1, 24, 16, 65]              48\n",
      "           Conv2d-15           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-16           [-1, 24, 16, 65]              48\n",
      "             ReLU-17           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-18           [-1, 48, 16, 65]               0\n",
      "           Conv2d-19           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-20           [-1, 24, 16, 65]              48\n",
      "             ReLU-21           [-1, 24, 16, 65]               0\n",
      "           Conv2d-22           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-23           [-1, 24, 16, 65]              48\n",
      "           Conv2d-24           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-25           [-1, 24, 16, 65]              48\n",
      "             ReLU-26           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-27           [-1, 48, 16, 65]               0\n",
      "           Conv2d-28           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-29           [-1, 24, 16, 65]              48\n",
      "             ReLU-30           [-1, 24, 16, 65]               0\n",
      "           Conv2d-31           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-32           [-1, 24, 16, 65]              48\n",
      "           Conv2d-33           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-34           [-1, 24, 16, 65]              48\n",
      "             ReLU-35           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-36           [-1, 48, 16, 65]               0\n",
      "           Conv2d-37           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-38           [-1, 24, 16, 65]              48\n",
      "             ReLU-39           [-1, 24, 16, 65]               0\n",
      "           Conv2d-40           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-41           [-1, 24, 16, 65]              48\n",
      "           Conv2d-42           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-43           [-1, 24, 16, 65]              48\n",
      "             ReLU-44           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-45           [-1, 48, 16, 65]               0\n",
      "           Conv2d-46            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-47            [-1, 48, 8, 33]              96\n",
      "           Conv2d-48            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-49            [-1, 48, 8, 33]              96\n",
      "             ReLU-50            [-1, 48, 8, 33]               0\n",
      "           Conv2d-51           [-1, 48, 16, 65]           2,304\n",
      "      BatchNorm2d-52           [-1, 48, 16, 65]              96\n",
      "             ReLU-53           [-1, 48, 16, 65]               0\n",
      "           Conv2d-54            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-55            [-1, 48, 8, 33]              96\n",
      "           Conv2d-56            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-57            [-1, 48, 8, 33]              96\n",
      "             ReLU-58            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-59            [-1, 96, 8, 33]               0\n",
      "           Conv2d-60            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-61            [-1, 48, 8, 33]              96\n",
      "             ReLU-62            [-1, 48, 8, 33]               0\n",
      "           Conv2d-63            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-64            [-1, 48, 8, 33]              96\n",
      "           Conv2d-65            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-66            [-1, 48, 8, 33]              96\n",
      "             ReLU-67            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-68            [-1, 96, 8, 33]               0\n",
      "           Conv2d-69            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-70            [-1, 48, 8, 33]              96\n",
      "             ReLU-71            [-1, 48, 8, 33]               0\n",
      "           Conv2d-72            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-73            [-1, 48, 8, 33]              96\n",
      "           Conv2d-74            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-75            [-1, 48, 8, 33]              96\n",
      "             ReLU-76            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-77            [-1, 96, 8, 33]               0\n",
      "           Conv2d-78            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-79            [-1, 48, 8, 33]              96\n",
      "             ReLU-80            [-1, 48, 8, 33]               0\n",
      "           Conv2d-81            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-82            [-1, 48, 8, 33]              96\n",
      "           Conv2d-83            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-84            [-1, 48, 8, 33]              96\n",
      "             ReLU-85            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-86            [-1, 96, 8, 33]               0\n",
      "           Conv2d-87            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-88            [-1, 48, 8, 33]              96\n",
      "             ReLU-89            [-1, 48, 8, 33]               0\n",
      "           Conv2d-90            [-1, 48, 8, 33]             432\n",
      "      BatchNorm2d-91            [-1, 48, 8, 33]              96\n",
      "           Conv2d-92            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-93            [-1, 48, 8, 33]              96\n",
      "             ReLU-94            [-1, 48, 8, 33]               0\n",
      " InvertedResidual-95            [-1, 96, 8, 33]               0\n",
      "           Conv2d-96            [-1, 48, 8, 33]           2,304\n",
      "      BatchNorm2d-97            [-1, 48, 8, 33]              96\n",
      "             ReLU-98            [-1, 48, 8, 33]               0\n",
      "           Conv2d-99            [-1, 48, 8, 33]             432\n",
      "     BatchNorm2d-100            [-1, 48, 8, 33]              96\n",
      "          Conv2d-101            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-102            [-1, 48, 8, 33]              96\n",
      "            ReLU-103            [-1, 48, 8, 33]               0\n",
      "InvertedResidual-104            [-1, 96, 8, 33]               0\n",
      "          Conv2d-105            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-106            [-1, 48, 8, 33]              96\n",
      "            ReLU-107            [-1, 48, 8, 33]               0\n",
      "          Conv2d-108            [-1, 48, 8, 33]             432\n",
      "     BatchNorm2d-109            [-1, 48, 8, 33]              96\n",
      "          Conv2d-110            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-111            [-1, 48, 8, 33]              96\n",
      "            ReLU-112            [-1, 48, 8, 33]               0\n",
      "InvertedResidual-113            [-1, 96, 8, 33]               0\n",
      "          Conv2d-114            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-115            [-1, 48, 8, 33]              96\n",
      "            ReLU-116            [-1, 48, 8, 33]               0\n",
      "          Conv2d-117            [-1, 48, 8, 33]             432\n",
      "     BatchNorm2d-118            [-1, 48, 8, 33]              96\n",
      "          Conv2d-119            [-1, 48, 8, 33]           2,304\n",
      "     BatchNorm2d-120            [-1, 48, 8, 33]              96\n",
      "            ReLU-121            [-1, 48, 8, 33]               0\n",
      "InvertedResidual-122            [-1, 96, 8, 33]               0\n",
      "          Conv2d-123            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-124            [-1, 96, 4, 17]             192\n",
      "          Conv2d-125            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-126            [-1, 96, 4, 17]             192\n",
      "            ReLU-127            [-1, 96, 4, 17]               0\n",
      "          Conv2d-128            [-1, 96, 8, 33]           9,216\n",
      "     BatchNorm2d-129            [-1, 96, 8, 33]             192\n",
      "            ReLU-130            [-1, 96, 8, 33]               0\n",
      "          Conv2d-131            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-132            [-1, 96, 4, 17]             192\n",
      "          Conv2d-133            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-134            [-1, 96, 4, 17]             192\n",
      "            ReLU-135            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-136           [-1, 192, 4, 17]               0\n",
      "          Conv2d-137            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-138            [-1, 96, 4, 17]             192\n",
      "            ReLU-139            [-1, 96, 4, 17]               0\n",
      "          Conv2d-140            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-141            [-1, 96, 4, 17]             192\n",
      "          Conv2d-142            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-143            [-1, 96, 4, 17]             192\n",
      "            ReLU-144            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-145           [-1, 192, 4, 17]               0\n",
      "          Conv2d-146            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-147            [-1, 96, 4, 17]             192\n",
      "            ReLU-148            [-1, 96, 4, 17]               0\n",
      "          Conv2d-149            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-150            [-1, 96, 4, 17]             192\n",
      "          Conv2d-151            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-152            [-1, 96, 4, 17]             192\n",
      "            ReLU-153            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-154           [-1, 192, 4, 17]               0\n",
      "          Conv2d-155            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-156            [-1, 96, 4, 17]             192\n",
      "            ReLU-157            [-1, 96, 4, 17]               0\n",
      "          Conv2d-158            [-1, 96, 4, 17]             864\n",
      "     BatchNorm2d-159            [-1, 96, 4, 17]             192\n",
      "          Conv2d-160            [-1, 96, 4, 17]           9,216\n",
      "     BatchNorm2d-161            [-1, 96, 4, 17]             192\n",
      "            ReLU-162            [-1, 96, 4, 17]               0\n",
      "InvertedResidual-163           [-1, 192, 4, 17]               0\n",
      "          Conv2d-164          [-1, 1024, 4, 17]         196,608\n",
      "     BatchNorm2d-165          [-1, 1024, 4, 17]           2,048\n",
      "            ReLU-166          [-1, 1024, 4, 17]               0\n",
      "          Linear-167                 [-1, 1000]       1,025,000\n",
      "================================================================\n",
      "Total params: 1,366,792\n",
      "Trainable params: 1,366,792\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 33.40\n",
      "Params size (MB): 5.21\n",
      "Estimated Total Size (MB): 39.37\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/riccardo/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "model_ResNet18 = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x0_5', pretrained=True)\n",
    "summary(model_ResNet18, (3,128,513))\n",
    "\n",
    "\n",
    "# Remove layers you want to discard\n",
    "\n",
    "class MyModel_ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel_ResNet18, self).__init__()\n",
    "        pretrained_model = torchvision.models.shufflenet_v2_x0_5(pretrained=True)\n",
    "        layers=list(pretrained_model.children())[:-4]\n",
    "        #print(layers)\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "        #self.conv1=nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "        #self.relu1=nn.ReLU(inplace=True)\n",
    "        #self.mp1=nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        #self.conv2=nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        '''self.relu2=nn.ReLU(inplace=True)\n",
    "        self.mp2=nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv3=nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3=nn.ReLU(inplace=True)\n",
    "        self.conv4=nn.Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4=nn.ReLU(inplace=True)\n",
    "        self.conv5=nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5=nn.ReLU(inplace=True)\n",
    "        self.mp5=nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)'''\n",
    "        \n",
    "        \n",
    "        self.pool= nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten=nn.Flatten() \n",
    "        self.last_layer = nn.Linear(49920, 32)\n",
    "        self.fc= nn.Linear(32, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x=self.conv1(x)\n",
    "        #x=self.relu1(x)\n",
    "        #x=self.mp1(x)\n",
    "        '''x=self.conv2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.mp2(x)    \n",
    "        x=self.conv4(x)\n",
    "        x=self.relu4(x)\n",
    "        x=self.conv5(x)\n",
    "        x=self.relu5(x)\n",
    "        x=self.mp5(x)'''\n",
    "        \n",
    "        x = self.features(x)\n",
    "        #x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.last_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8b7bc56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 24, 64, 257]             648\n",
      "       BatchNorm2d-2          [-1, 24, 64, 257]              48\n",
      "              ReLU-3          [-1, 24, 64, 257]               0\n",
      "         MaxPool2d-4          [-1, 24, 32, 129]               0\n",
      "            Conv2d-5           [-1, 24, 16, 65]             216\n",
      "       BatchNorm2d-6           [-1, 24, 16, 65]              48\n",
      "            Conv2d-7           [-1, 24, 16, 65]             576\n",
      "       BatchNorm2d-8           [-1, 24, 16, 65]              48\n",
      "              ReLU-9           [-1, 24, 16, 65]               0\n",
      "           Conv2d-10          [-1, 24, 32, 129]             576\n",
      "      BatchNorm2d-11          [-1, 24, 32, 129]              48\n",
      "             ReLU-12          [-1, 24, 32, 129]               0\n",
      "           Conv2d-13           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-14           [-1, 24, 16, 65]              48\n",
      "           Conv2d-15           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-16           [-1, 24, 16, 65]              48\n",
      "             ReLU-17           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-18           [-1, 48, 16, 65]               0\n",
      "           Conv2d-19           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-20           [-1, 24, 16, 65]              48\n",
      "             ReLU-21           [-1, 24, 16, 65]               0\n",
      "           Conv2d-22           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-23           [-1, 24, 16, 65]              48\n",
      "           Conv2d-24           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-25           [-1, 24, 16, 65]              48\n",
      "             ReLU-26           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-27           [-1, 48, 16, 65]               0\n",
      "           Conv2d-28           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-29           [-1, 24, 16, 65]              48\n",
      "             ReLU-30           [-1, 24, 16, 65]               0\n",
      "           Conv2d-31           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-32           [-1, 24, 16, 65]              48\n",
      "           Conv2d-33           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-34           [-1, 24, 16, 65]              48\n",
      "             ReLU-35           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-36           [-1, 48, 16, 65]               0\n",
      "           Conv2d-37           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-38           [-1, 24, 16, 65]              48\n",
      "             ReLU-39           [-1, 24, 16, 65]               0\n",
      "           Conv2d-40           [-1, 24, 16, 65]             216\n",
      "      BatchNorm2d-41           [-1, 24, 16, 65]              48\n",
      "           Conv2d-42           [-1, 24, 16, 65]             576\n",
      "      BatchNorm2d-43           [-1, 24, 16, 65]              48\n",
      "             ReLU-44           [-1, 24, 16, 65]               0\n",
      " InvertedResidual-45           [-1, 48, 16, 65]               0\n",
      "          Flatten-46                [-1, 49920]               0\n",
      "           Linear-47                    [-1, 8]         399,368\n",
      "          Softmax-48                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 407,000\n",
      "Trainable params: 407,000\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 20.44\n",
      "Params size (MB): 1.55\n",
      "Estimated Total Size (MB): 22.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "model = MyModel_ResNet18()\n",
    "summary(model, (3,128,513))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b4b15",
   "metadata": {},
   "source": [
    "## RGB Dataset\n",
    "Resnet18 expects RGB input images, so our dataset need to be converted from a one to a three-channel. We'll do dat by copying three times the same image in the three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5e94008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDatasetRGB(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "                \n",
    "        #do ResNet18 normalization:\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        #copy the channel 3 times (need to unsqueeze to create a new dimension first)\n",
    "        #print(\"DATASET*  sample shape is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        stft_vector = stft_vector.unsqueeze(0).repeat(3,1,1)\n",
    "        stft_vector = stft_vector.to(torch.float32) #float32 needed for ResNet18 model (downcast from float64)\n",
    "        #print(\"DATASET* sample shape after repeat is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        #print(\"stft_vector dtype:\",stft_vector.dtype)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c24605b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor batch_idx, batch  in enumerate(train_data_loader):\\n    print (\"batch index:\",batch_idx)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    \\n    for idx, sample in enumerate(inputs):\\n        label = labels[idx]\\n        print(\"inputs: shape:\",inputs.shape)\\n        print(\"sample: shape:\",sample.shape)\\n'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform_ResNet18 = transforms.Compose([\n",
    "    transforms.Normalize(mean= [loaded_mean,loaded_mean,loaded_mean], std=[loaded_std,loaded_std,loaded_std]) #our values\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #ResNet18 specific values\n",
    "])\n",
    "\n",
    "train_dataset = MyDatasetRGB(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDatasetRGB(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDatasetRGB(test_file_paths, Y_test,  transform = transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size = 10, shuffle=True)\n",
    "\n",
    "'''\n",
    "for batch_idx, batch  in enumerate(train_data_loader):\n",
    "    print (\"batch index:\",batch_idx)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    \n",
    "    for idx, sample in enumerate(inputs):\n",
    "        label = labels[idx]\n",
    "        print(\"inputs: shape:\",inputs.shape)\n",
    "        print(\"sample: shape:\",sample.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5f3747da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2d031c3ccc4949aed77c2afbc91a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-cf08808a9749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRGB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-4e326a5b92a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose, RGB)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Extract the inputs and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-2d7d4f15fcf3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         x=self.mp5(x)'''\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    163\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "#todo: convert input tensor from float64 to double\n",
    "\n",
    "#todo normalize again using resnet18 suggested mean and std\n",
    "#TODO: FIX TEST FUNCTION USING RGB (maybe already works)\n",
    "\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.001,verbose=False, RGB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0f8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
