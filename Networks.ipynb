{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15945d26",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6eab989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tqdm.notebook as tq\n",
    "import utils\n",
    "from pydub import AudioSegment\n",
    "from tkinter import Tcl # file sorting by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7763c",
   "metadata": {},
   "source": [
    "# Creation of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8ded2",
   "metadata": {},
   "source": [
    "### Dictionary creation for the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c4013",
   "metadata": {},
   "source": [
    "We want a dictionary indicating a numbeer for each genre:\n",
    "\n",
    "{0: 'Hip-Hop', 1: 'Pop', 2: 'Folk', 3: 'Rock', 4: 'Experimental', 5: 'International', 6: 'Electronic', 7: 'Instrumental'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e985c",
   "metadata": {},
   "source": [
    "### Creation of the labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0be9951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_folder: data/fma_small_stft_transposed/train\n",
      "validation_folder: data/fma_small_stft_transposed/validation\n",
      "test_folder: data/fma_small_stft_transposed/test \n",
      "\n",
      "audio directory:  ./data/fma_small/\n",
      "Loading tracks.csv...\n",
      "small dataset shape: (8000, 52)\n",
      "Track.csv: 6400 training samples, 800 validation samples, 800 test samples\n",
      "\n",
      "there are 8 unique genres\n",
      "Dictionary of genres created: {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Rock': 3, 'Experimental': 4, 'International': 5, 'Electronic': 6, 'Instrumental': 7}\n",
      "labels length: 63970\n",
      "labels length: 8000\n",
      "labels length: 8000\n"
     ]
    }
   ],
   "source": [
    "def create_single_dataset(folder_path, tracks_dataframe, genre_dictionary):    \n",
    "    labels = []\n",
    "   \n",
    "    _, file_list = get_sorted_file_paths(folder_path)\n",
    "    \n",
    "    for i,file in enumerate(file_list):\n",
    "        #print(\"considering file:\",file, \"({}/{})\".format(i,len(file_list)))\n",
    "        track_id_clip_id = file.split('.')[0]\n",
    "        track_id = track_id_clip_id.split('_')[0]\n",
    "        #print(\"track id with clip: {}, track id: {}\".format(track_id_clip_id, track_id))\n",
    "        genre = tracks_dataframe.loc[int(track_id)]\n",
    "        #print(\"genre from dataframe: \", genre)\n",
    "        label = genre_dictionary[genre]\n",
    "        #print(\"label from dictionary:\",label)\n",
    "        labels.append(label)\n",
    "    print(\"labels length: {}\".format(len(labels)))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "#create the train,validation and test vectors using the files in the train/validation/test folders\n",
    "def create_dataset_splitted(folder_path):\n",
    "    train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "    validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "    test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "    \n",
    "    print(\"train_folder:\",train_folder)\n",
    "    print(\"validation_folder:\",validation_folder)\n",
    "    print(\"test_folder:\",test_folder,\"\\n\")\n",
    "    \n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "    \n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "\n",
    "    small_training_top_genres = small_training['genre_top']\n",
    "    small_validation_top_genres = small_validation['genre_top']\n",
    "    small_test_top_genres = small_test['genre_top']\n",
    "    \n",
    "    #create dictionary of genre classes:\n",
    "    unique_genres = small_training_top_genres.unique()\n",
    "    unique_genres = np.array(unique_genres)\n",
    "    print(\"there are {} unique genres\".format(len(unique_genres)))\n",
    "    genre_dictionary = {}\n",
    "    for i,genre in enumerate(unique_genres):\n",
    "        genre_dictionary[genre] = i\n",
    "    print(\"Dictionary of genres created:\",genre_dictionary)\n",
    "    \n",
    "    \n",
    "    Y_train = create_single_dataset(train_folder, small_training_top_genres, genre_dictionary)\n",
    "    Y_validation = create_single_dataset(validation_folder, small_validation_top_genres, genre_dictionary)\n",
    "    Y_test = create_single_dataset(test_folder, small_test_top_genres, genre_dictionary)\n",
    "    \n",
    "    return Y_train, Y_validation, Y_test\n",
    " \n",
    "def get_sorted_file_paths(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #sort the dataset files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_list = Tcl().call('lsort', '-dict', file_list) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...\n",
    "    file_paths = [os.path.join(folder_path, file_name) for file_name in file_list] #join filename with folder path\n",
    "    #print(\"There are {} in the folder: {}\".format(len(file_list),file_list))\n",
    "    return file_paths, file_list\n",
    "    \n",
    "    \n",
    "folder_path=\"data/fma_small_stft_transposed\"\n",
    "Y_train, Y_validation, Y_test = create_dataset_splitted(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20175a8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "187ddbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list, labels):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cae70891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  63970\n",
      "len of validation dataset:  8000\n",
      "len of test dataset:  8000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_stft_transposed\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDataset(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef67f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_dimension_errors(filepaths):\n",
    "    error_indexes = []\n",
    "    progress = 0\n",
    "    for file in filepaths:\n",
    "        progress+=1\n",
    "        print(\"checked {}/{} files\".format(progress,len(filepaths)))\n",
    "        x = np.load(file)\n",
    "        if(x.shape != (128,513)):\n",
    "            error_indexes.append(x)\n",
    "            print(\"error\")\n",
    "    print(\"{} errors found in files: {}\".format(len(error_indexes),error_indexes))\n",
    "    for idx,error in enumerate(error_indexes):\n",
    "        print(\"index: {}, shape: {}\".format(idx,error.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a52dc",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f745b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513), stride=(1,513))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1), stride=(1,513))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1), stride=(1,513))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.dense2 = nn.Linear(300, 150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x=self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845a53e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d034d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "LEARNING_RATE=0.0001\n",
    "\n",
    "learning_rate_list = [0.01, 0.001, 0.0001]\n",
    "batch_size_list = [128,256,512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd576a3",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "223742e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, validation_dataset, Y_validation):\n",
    "    #Stop parameters learning\n",
    "    model.eval()\n",
    "    \n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=10)\n",
    "    \n",
    "    # Definisci i pesi delle classi in base alla loro importanza (puoi sperimentare con questi valori)\n",
    "    class_weights = torch.tensor([1.5, 1.5, 1.2, 1.0, 0.5, 1.0, 1.2, 1.5])\n",
    "\n",
    "    # Crea una funzione di perdita con pesi\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confusion_matrix = np.zeros((8,8 ), dtype=int)\n",
    "\n",
    "    correct_maj=0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            \n",
    "            \n",
    "            #print(\"Inputs:\",inputs,\"size:\",inputs.size())\n",
    "            #print(\"Labels:\",labels,\"size:\",labels.size())\n",
    "            inputs=inputs.unsqueeze(1)\n",
    "            #predict label\n",
    "            outputs = model(inputs)\n",
    "            #print(\"Outputs:\",outputs,\"size:\",outputs.size())\n",
    "            #compute loss\n",
    "            #TO DO:adapt it to the voting\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            voting=outputs.sum(dim=0)\n",
    "            #print(\"Voting:\",voting,\"size:\",voting.size())\n",
    "            predicted= torch.argmax(voting)\n",
    "            #print(\"winning class\",predicted)\n",
    "            correct += (predicted == labels[0])\n",
    "            confusion_matrix[predicted][labels[0]]+=1\n",
    "            \n",
    "            votes=[0,0,0,0,0,0,0,0]\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                votes[max_index]+=1\n",
    "            majority_prediction=votes.index(max(votes)) \n",
    "            correct_maj += (majority_prediction == labels[0])\n",
    "            \n",
    "            \n",
    "    maj_acc=correct_maj*100/800        \n",
    "            \n",
    "    cm=ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "    cm.plot()\n",
    "    print(confusion_matrix)\n",
    "    accuracy = 100*correct / 800 \n",
    "    average_loss = total_loss / 800\n",
    "    \n",
    "    print(\"Majority:\",maj_acc,\"\\t Probability:\",accuracy)\n",
    "    model.train()\n",
    "    return accuracy, average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0ca4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, num_epochs, learning_rate, verbose = False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    counted_labels=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", content: \",inputs)\n",
    "                print(\"\\nlabels shape:\",labels.size(),\", content: \",labels)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if(verbose == True):\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "                print(\"List of labels until now:\",counted_labels)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                counted_labels[labels[index].item()]+=1\n",
    "                if(labels[index].item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "        \n",
    "        #calculate train loss and accuracy\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        train_loss_list.append(average_loss)\n",
    "        train_acc_list.append(average_accuracy)\n",
    "        \n",
    "        #calculate validation loss and accuracy\n",
    "        val_acc, val_loss = test(model, validation_dataset, Y_validation)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],Train Loss: {average_loss:.4f}. Train Accuracy: {average_accuracy} Val Loss: {val_loss} Val Accuracy: {val_acc}\")\n",
    "        progress_bar.close()\n",
    "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbbc7adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing model with batch size=128, lr=0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d27d5a19f74dedb5f955d6aebbb7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [100 100 100 100 100 100 100 100]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]]\n",
      "Epoch [1/10],Train Loss: 2.1489. Train Accuracy: 12.4859375 Val Loss: 2.149008959531784 Val Accuracy: 12.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5090e0725ced4d92b51cfb9cddfd2cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e522e69697f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_lr_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'_bs_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Traing model with batch size={bs}, lr={lr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mperformance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mperformance_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0b7ef43db2fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#calculate validation loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mval_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mval_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d6457c5ef5be>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, validation_dataset, Y_validation)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;31m#print(\"Inputs:\",inputs,\"size:\",inputs.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m#print(\"Labels:\",labels,\"size:\",labels.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1f58f3fb8878>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mstft_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#load from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstft_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 440\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0m_check_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfortran_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_array_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0mheader_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhlength_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhlength_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;31m# The header is a pretty-printed string representation of a literal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summary(model, (1, 128, 513))\n",
    "performance_list = []\n",
    "model_list = []\n",
    "directory = 'models/'\n",
    "\n",
    "for lr in learning_rate_list:   \n",
    "    for bs in batch_size_list:\n",
    "        model = NNet1() #reinitialize model\n",
    "        file_path = directory + 'model' + '_lr_' + str(lr).split('.')[0] + '-' + str(lr).split('.')[1]+ '_bs_' + str(bs)\n",
    "        print(f\"Traing model with batch size={bs}, lr={lr}\")\n",
    "        performance=train(model, train_dataset, batch_size=bs, num_epochs=EPOCHS, learning_rate=lr)\n",
    "        performance_list.append(performance)\n",
    "        model_list.append(model)\n",
    "        print(\"saving model in\", file_path)\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "        print(performance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d928b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'performance_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-feab07013d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mparameter_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'performance_list' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "plt.plot(np.arange(1,EPOCHS+1), performance[1][:], label='Loss') \n",
    "plt.plot(np.arange(1,EPOCHS+1), performance[0][:], label='Accuracy')\n",
    "plt.legend()  # Display the legend showing the labels\n",
    "plt.show()\n",
    "print(performance[0][:])\n",
    "'''\n",
    "\n",
    "parameter_list = []\n",
    "for lr in learning_rate_list:\n",
    "    for bs in batch_size_list:\n",
    "        parameter_list.append([lr,bs])\n",
    "\n",
    "for performance, parameters in zip(performance_list, parameter_list):\n",
    "    plt.plot(np.arange(1,EPOCHS+1), performance[1][:], label='Val Loss') \n",
    "    plt.plot(np.arange(1,EPOCHS+1), performance[0][:], label='Val Accuracy')\n",
    "    plt.title(f\"Lr = {parameters[0]}, Batch size = {parameters[1]}\")\n",
    "    plt.legend()  # Display the legend showing the labels\n",
    "    plt.show()\n",
    "    print(performance[0][:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef7fab",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1 + BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3685ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet1_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1_BN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513), stride=(1, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1), stride=(1, 513))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1), stride=(1, 513))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.bn4 = nn.BatchNorm1d(300)\n",
    "        self.dense2 = nn.Linear(300, 150)\n",
    "        self.bn5 = nn.BatchNorm1d(150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61b169d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb9f0f052a14a9db783d581158aa765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  3  0  0  0  3  1  0]\n",
      " [ 6  1  0  1  0  0  0  1]\n",
      " [ 0  2 23  0  5  1  0  3]\n",
      " [ 5 11  6 43  2  5  0  7]\n",
      " [22 69 70 56 85 68 49 81]\n",
      " [ 1  0  1  0  0 13  0  0]\n",
      " [57 13  0  0  3 10 50  1]\n",
      " [ 0  1  0  0  5  0  0  7]]\n",
      "Majority: tensor(28.6250) \t Probability: tensor(28.8750)\n",
      "Epoch [1/10],Train Loss: 1.8356. Train Accuracy: 43.4515625 Val Loss: 1.9695085182785987 Val Accuracy: 28.875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6acc0b781747d6b512ef3c60a5104c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-cbe1ea5fdbf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_NNet1_BN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNet1_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_NNet1_BN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-cc4eca7f517a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# reset train accuracy each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-1f58f3fb8878>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mstft_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#load from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstft_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 440\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_NNet1_BN = NNet1_BN()\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model_NNet1_BN, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc08332",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd36488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenreNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MusicGenreNet, self).__init__()\n",
    "        \n",
    "        # STFT spectrogram input: (batch_size, 1, 128, 513)\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=(4, 513))\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=(4, 1))\n",
    "        self.conv3 = nn.Conv2d(256, 256, kernel_size=(4, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(125, 1))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(125, 1))\n",
    "        self.fc1 = nn.Linear(256, 10)\n",
    "        self.fc2 = nn.Linear(10, 150)\n",
    "        self.fc3 = nn.Linear(150, 300)\n",
    "        self.fc4 = nn.Linear(300, 8)  # 8 classes for genre predictions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Sum between the first and third conv layers\n",
    "        x = x[:, :, :, 0] + x[:, :, :, 2]\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.maxpool(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dda60597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicGenreNet(\n",
      "  (conv1): Conv2d(1, 256, kernel_size=(4, 513), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1))\n",
      "  (maxpool): MaxPool2d(kernel_size=(125, 1), stride=(125, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgpool): AvgPool2d(kernel_size=(125, 1), stride=(125, 1), padding=0)\n",
      "  (fc1): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=150, bias=True)\n",
      "  (fc3): Linear(in_features=150, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=8, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393cd2fe8f604ff48ff824c50e227ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inputs shape: torch.Size([32, 128, 513]) , content:  tensor([[[1.4618e+01, 1.9920e+01, 3.2567e+01,  ..., 1.6582e-01,\n",
      "          1.6580e-01, 1.6580e-01],\n",
      "         [1.6860e+00, 8.4427e+00, 4.9806e+01,  ..., 5.6450e-05,\n",
      "          8.9599e-05, 7.0072e-05],\n",
      "         [3.8417e-01, 1.7521e+01, 4.2537e+01,  ..., 1.4457e-04,\n",
      "          2.1742e-04, 2.6711e-04],\n",
      "         ...,\n",
      "         [2.6286e+00, 1.7414e+01, 2.9946e+01,  ..., 1.0302e-04,\n",
      "          9.6688e-05, 5.8366e-05],\n",
      "         [6.4445e-01, 2.2088e+01, 2.4809e+01,  ..., 1.5084e-04,\n",
      "          7.7643e-05, 4.8997e-05],\n",
      "         [3.7857e+00, 9.0765e+00, 1.2529e+01,  ..., 5.5393e-05,\n",
      "          8.5444e-05, 3.8940e-05]],\n",
      "\n",
      "        [[5.8274e+00, 1.3948e+01, 2.1273e+01,  ..., 8.6421e-02,\n",
      "          8.6466e-02, 8.6468e-02],\n",
      "         [4.6153e+01, 6.5156e+01, 3.4670e+01,  ..., 9.8506e-05,\n",
      "          1.9205e-04, 5.9769e-05],\n",
      "         [3.7399e+01, 3.6822e+01, 1.1841e+01,  ..., 6.7457e-05,\n",
      "          1.0641e-04, 1.6221e-04],\n",
      "         ...,\n",
      "         [4.4067e+01, 4.6278e+01, 3.7503e+01,  ..., 3.7313e-03,\n",
      "          3.8036e-03, 3.7508e-03],\n",
      "         [6.7020e+01, 5.6015e+01, 1.8357e+01,  ..., 3.4104e-05,\n",
      "          8.0334e-05, 3.6803e-05],\n",
      "         [2.8871e+01, 3.1417e+01, 1.2919e+01,  ..., 7.5606e-05,\n",
      "          9.3104e-05, 9.5949e-06]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [3.5667e-01, 3.4165e-01, 3.0073e-01,  ..., 4.7176e-05,\n",
      "          3.8233e-05, 3.2968e-05],\n",
      "         [1.5929e+01, 2.6886e+01, 1.3163e+02,  ..., 3.7898e-03,\n",
      "          3.7204e-03, 3.5074e-03],\n",
      "         ...,\n",
      "         [1.6006e+01, 3.8124e+01, 7.1925e+01,  ..., 2.3269e-04,\n",
      "          1.5240e-04, 4.0326e-05],\n",
      "         [1.7574e+01, 3.5808e+01, 6.1416e+01,  ..., 1.7382e-04,\n",
      "          1.0367e-04, 4.6769e-05],\n",
      "         [9.8786e+00, 1.4210e+01, 4.3808e+01,  ..., 1.5183e-04,\n",
      "          3.7526e-05, 8.8071e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.2419e+00, 1.0060e+00, 3.5187e+00,  ..., 3.7207e-02,\n",
      "          3.7174e-02, 3.7175e-02],\n",
      "         [1.3176e+00, 2.9200e+00, 1.1540e+01,  ..., 8.8191e-05,\n",
      "          1.4739e-04, 1.1675e-04],\n",
      "         [7.4869e-01, 1.7309e+00, 8.4359e+00,  ..., 1.3173e-04,\n",
      "          1.7014e-04, 1.0736e-04],\n",
      "         ...,\n",
      "         [6.5802e-01, 3.9477e+00, 3.5995e+00,  ..., 1.0020e-04,\n",
      "          1.1252e-04, 7.7687e-05],\n",
      "         [5.5697e-01, 4.9912e+00, 9.4579e+00,  ..., 1.3739e-04,\n",
      "          1.0770e-04, 7.6369e-05],\n",
      "         [8.0019e-02, 3.2133e+00, 5.5450e+00,  ..., 1.2085e-04,\n",
      "          1.9533e-05, 4.3710e-05]],\n",
      "\n",
      "        [[9.9965e+00, 1.1274e+01, 9.1159e+00,  ..., 7.0122e-02,\n",
      "          7.0198e-02, 7.0251e-02],\n",
      "         [2.3882e+00, 8.4614e+00, 2.1186e+01,  ..., 4.4465e-05,\n",
      "          1.2789e-04, 4.9664e-05],\n",
      "         [1.2239e+00, 6.5343e+01, 4.5134e+01,  ..., 1.6393e-04,\n",
      "          3.5741e-04, 4.1399e-04],\n",
      "         ...,\n",
      "         [7.9675e+00, 1.1287e+01, 2.1282e+01,  ..., 1.1181e-04,\n",
      "          1.5290e-04, 7.5836e-05],\n",
      "         [4.9183e+00, 1.1437e+00, 1.9404e+01,  ..., 2.2744e-04,\n",
      "          1.6343e-04, 1.6305e-05],\n",
      "         [2.3763e+00, 1.1168e+01, 3.5512e+01,  ..., 9.2905e-05,\n",
      "          1.3498e-04, 2.1207e-04]],\n",
      "\n",
      "        [[3.3311e+01, 3.6915e+01, 4.1631e+01,  ..., 3.6871e-02,\n",
      "          3.6822e-02, 3.6743e-02],\n",
      "         [9.3699e+00, 3.7494e+01, 4.0477e+01,  ..., 2.2761e-04,\n",
      "          2.1937e-04, 9.9553e-05],\n",
      "         [1.3595e+00, 3.3935e+01, 4.7257e+01,  ..., 1.6370e-04,\n",
      "          6.1149e-05, 2.7610e-04],\n",
      "         ...,\n",
      "         [1.2157e+01, 5.7757e+01, 4.3091e+01,  ..., 1.4709e-04,\n",
      "          1.5703e-04, 2.7325e-05],\n",
      "         [2.3550e+01, 3.9451e+01, 2.6868e+01,  ..., 4.6592e-05,\n",
      "          6.2058e-05, 1.4034e-05],\n",
      "         [2.7179e+01, 2.9157e+01, 2.8575e+01,  ..., 1.8120e-04,\n",
      "          1.6936e-04, 1.4368e-04]]])\n",
      "\n",
      "labels shape: torch.Size([32]) , content:  tensor([7, 7, 6, 2, 7, 5, 5, 1, 1, 4, 5, 4, 6, 2, 6, 6, 4, 7, 6, 6, 1, 6, 3, 1,\n",
      "        7, 7, 0, 7, 1, 3, 6, 2])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 3 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d4763f9ec774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#summary(model, (1, 128, 513))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0b7ef43db2fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Extract the inputs and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nndl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-560e60635b3f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Sum between the first and third conv layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 3 with size 1"
     ]
    }
   ],
   "source": [
    "# Creating an instance of the network\n",
    "model = MusicGenreNet()\n",
    "\n",
    "# Printing the model architecture\n",
    "print(model)\n",
    "#summary(model, (1, 128, 513))\n",
    "train(model, train_dataset, batch_size=32, num_epochs=50, learning_rate=0.001, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c443ec",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGenreClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(AudioGenreClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Time-distributed layer\n",
    "        self.time_distributed = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        self.bilstm = nn.LSTM(256, hidden_size, num_layers = num_layers, bidirectional=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Time-distributed layer\n",
    "        x = self.time_distributed(x)\n",
    "\n",
    "        # Bidirectional LSTM layers\n",
    "        output, _ = self.bilstm(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.softmax(self.attention(output), dim=1)\n",
    "        attended_output = torch.sum(attention_weights * output, dim=1)\n",
    "\n",
    "        # Pooling layer\n",
    "        pooled_output = self.pooling(attended_output.permute(0, 1).unsqueeze(2))\n",
    "        \n",
    "        # Reshape and pass through the output layer\n",
    "        pooled_output = pooled_output.squeeze(2)\n",
    "        output = self.fc(pooled_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRNN(model, dataset, batch_size, num_epochs, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \",device)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            #print(\"inputs shape:\",inputs.shape,\", content: \",inputs)\n",
    "            #print(\"labels shape:\",labels.shape,\", content: \",labels)\n",
    "            #inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            #print(\"\\noutputs type:\",type(outputs),\"content:\",outputs)\n",
    "            #print(\"\\nlabels type:\",type(labels),\"content:\",labels)\n",
    "\n",
    "            loss = criterion(outputs, labels.float()) #labels need to be a vector of float, not Long\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #print(\"outputs shape:\",outputs.size(),\"content:\",outputs)\n",
    "            #print(\"labels shape:\",labels.size(),\"content:\",labels)\n",
    "\n",
    "            #calculate train accuracy\n",
    "            for idx, predicted_label in enumerate(outputs):\n",
    "                #print(\"predicted_label size:\",predicted_label.size(),\"content:\",predicted_label)\n",
    "                #print(\"labels[idx] size:\",labels[idx].size(),\"content:\",labels[idx])\n",
    "                max_idx = torch.argmax(predicted_label).item() #index with max argument in the one hot predicted label vector\n",
    "                #print(\"max_idx content:\",max_idx)\n",
    "\n",
    "                if(labels[idx][max_idx].item() == 1):\n",
    "                    correct += 1\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            #print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"Batch accuracy\": accuracy, \"Average accuracy\": average_accuracy})\n",
    "\n",
    "\n",
    "\n",
    "            #bps = (batch_idx + 1) / (time.time() - start_time)\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "       \n",
    "\n",
    "\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}. Accuracy: {average_accuracy}\")\n",
    "        progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRNN = AudioGenreClassifier(input_size=513, hidden_size=256, num_layers=5, num_classes=8)\n",
    "trainRNN(modelRNN, train_dataset, batch_size=32, num_epochs=8, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343371e",
   "metadata": {},
   "source": [
    "# Raw audio (1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabb348",
   "metadata": {},
   "source": [
    "## Raw audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30092ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset of way audio\n",
    "class MyDatasetRaw(Dataset):\n",
    "    def __init__(self, file_list, labels):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        #print(\"opening file:\",file_path)\n",
    "        audio_file = AudioSegment.from_file(file_path)\n",
    "\n",
    "        data = audio_file._data\n",
    "        pcm16_signed_integers = []\n",
    "\n",
    "        # This loop decodes the bytestring into PCM samples.\n",
    "        # The bytestring is a stream of little-endian encoded signed integers.\n",
    "        # This basically just cuts each two-byte sample out of the bytestring, converts\n",
    "        # it to an integer, and appends it to the list of samples.\n",
    "        for sample_index in range(len(data)//2):\n",
    "            sample = int.from_bytes(data[sample_index*2:sample_index*2+2], 'big', signed=True)\n",
    "            pcm16_signed_integers.append(sample)\n",
    "\n",
    "\n",
    "        #load audio in ram\n",
    "        #x = np.load(file_path,allow_pickle=True) #load the MONO audio file from the data/fma_small directory\n",
    "        x=torch.tensor(pcm16_signed_integers)\n",
    "        x = x.type(torch.FloatTensor)\n",
    "        #print(x.shape)\n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c527e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns file paths list for train, validation and test\n",
    "def get_file_paths():\n",
    "    folder_raw = './data/fma_small'\n",
    "    file_paths_raw = []\n",
    "\n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "    \n",
    "    #--------------TRAIN----------------------\n",
    "    print(\"Creating train dataset...\")\n",
    "    file_paths_train = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_train = np.array(small_training.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_train)} samples. Here's the first ones: {track_ids_train[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_train:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_train.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_train = np.array(Tcl().call('lsort', '-dict', file_paths_train)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    #--------------VALIDATION----------------------\n",
    "    print(\"Creating validation dataset...\")\n",
    "    file_paths_validation = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_validation = np.array(small_validation.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_validation)} samples. Here's the first ones: {track_ids_validation[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_validation:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_validation.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_validation = np.array(Tcl().call('lsort', '-dict', file_paths_validation)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    #--------------TEST----------------------\n",
    "    print(\"Creating test dataset...\")\n",
    "    file_paths_test = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_test = np.array(small_test.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_test)} samples. Here's the first ones: {track_ids_test[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_test:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_test.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_test = np.array(Tcl().call('lsort', '-dict', file_paths_test)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    \n",
    "    #create the datasets\n",
    "    \n",
    "    return file_paths_train, file_paths_validation, file_paths_test\n",
    "\n",
    "#returns file paths list for train, validation and test for each 3s clip\n",
    "def get_file_paths_split():\n",
    "    folder_raw = './data/fma_small_raw_split'\n",
    "    folder_train = folder_raw + '/train'\n",
    "    folder_validation = folder_raw + '/validation'\n",
    "    folder_test = folder_raw + '/test'\n",
    "    file_paths_raw = []\n",
    "\n",
    "    #--------------TRAIN----------------------\n",
    "    print(\"Creating train dataset...\")\n",
    "    file_paths_train = os.listdir(folder_train)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_train = np.array(Tcl().call('lsort', '-dict', file_paths_train)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_train = []\n",
    "    for file in file_paths_train:\n",
    "        complete_file_path_train.append(folder_train + '/' + file)\n",
    "\n",
    "    #--------------VALIDATION----------------------\n",
    "    print(\"Creating validation dataset...\")\n",
    "    file_paths_validation = os.listdir(folder_validation)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_validation = np.array(Tcl().call('lsort', '-dict', file_paths_validation)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_validation = []\n",
    "    for file in file_paths_validation:\n",
    "        complete_file_path_validation.append(folder_train + '/' + file)\n",
    "\n",
    "    #--------------TEST----------------------\n",
    "    print(\"Creating test dataset...\")\n",
    "    file_paths_test = os.listdir(folder_test)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_test = np.array(Tcl().call('lsort', '-dict', file_paths_test)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_test = []\n",
    "    for file in file_paths_test:\n",
    "        complete_file_path_test.append(folder_train + '/' + file)\n",
    "    \n",
    "    return complete_file_path_train, complete_file_path_validation, complete_file_path_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99825d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_train, file_paths_validation, file_paths_test = get_file_paths_split()\n",
    "print(len(file_paths_train))\n",
    "print(len(file_paths_validation))\n",
    "print(len(file_paths_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891ab30",
   "metadata": {},
   "source": [
    "## Fix wrong sampling rates in the dataset\n",
    "\n",
    "Some songs in the fma_small dataset has a different sampling rate from 44100. Let's fix them by using librosa resampling tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f82d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "#fix sampling rate\n",
    "target_sr = 44100\n",
    "wrong_sr_list = []\n",
    "\n",
    "\"\"\"\n",
    "print(\"\\n******Checking sample rates to be\",target_sr,\"in train set\")\n",
    "for file in file_paths_train:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "   \"\"\"     \n",
    "print(\"\\n*****Checking sample rates to be\",target_sr,\"in validation set\")\n",
    "for file in file_paths_validation:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "\n",
    "        \n",
    "print(\"\\n*****Checking sample rates to be\",target_sr,\"in test set\")\n",
    "for file in file_paths_test:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "        \n",
    "print(\"There were a total of\",len(wrong_sr_list),\"files with wrong sampling rate. Now they have been corrected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029105b",
   "metadata": {},
   "source": [
    "## Create labels for raw audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ea1db",
   "metadata": {},
   "source": [
    "We have to take one label each ten from Y_label. Now we are not using 10 clips for each audio but just the whole audio itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a element every ten (not using 10 clips per audio anymore)\n",
    "Y_train_reshaped = [elem for elem in Y_train[0::10]] \n",
    "print(\"Y_train reshaped:\",len(Y_train_reshaped))\n",
    "Y_validation_reshaped = [elem for elem in Y_validation[0::10]] \n",
    "print(\"Y_validation reshaped:\",len(Y_validation_reshaped))\n",
    "Y_test_reshaped = [elem for elem in Y_test[0::10]] \n",
    "print(\"Y_test reshaped:\",len(Y_test_reshaped))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7112c8",
   "metadata": {},
   "source": [
    "## Create dataset for raw audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDatasetRaw(file_paths_train, Y_train)\n",
    "validation_dataset = MyDatasetRaw(file_paths_validation, Y_validation)\n",
    "test_dataset = MyDatasetRaw(file_paths_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89626be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom model class\n",
    "class NNet_Raw(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet_Raw, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 33073, 64)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(8)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa495fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet_Raw2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet_Raw2, self).__init__()\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.layer1 = nn.Conv1d(1, 8, 7, 3)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(8)\n",
    "        self.layer2 = nn.Conv1d(8, 16, 5, 2)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(16)\n",
    "        self.layer3 = nn.Conv1d(16, 16, 3, 2)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(16)\n",
    "        self.layer4 = nn.Conv1d(16, 16, 3, 2)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(16)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(112, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.dropout1(self.lrelu(self.layer1(x)))\n",
    "        y = self.dropout1(self.lrelu(self.batchnorm1(self.layer2(y))))\n",
    "        y = self.dropout1(self.lrelu(self.batchnorm2(self.layer3(y))))\n",
    "        y = self.dropout2(self.lrelu(self.batchnorm3(self.layer4(y))))\n",
    "        y = self.flatten(y)\n",
    "        y = self.fc(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyModel=NNet_Raw()\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(MyModel, train_dataset, batch_size=BATCH_SIZE, num_epochs=EPOCHS, learning_rate=LEARNING_RATE, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96525097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9257b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
