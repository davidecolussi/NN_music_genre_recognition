{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15945d26",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6eab989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tqdm.notebook as tq\n",
    "import utils\n",
    "from pydub import AudioSegment\n",
    "from tkinter import Tcl # file sorting by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7763c",
   "metadata": {},
   "source": [
    "# Creation of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8ded2",
   "metadata": {},
   "source": [
    "### Dictionary creation for the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c4013",
   "metadata": {},
   "source": [
    "We want a dictionary indicating a numbeer for each genre:\n",
    "\n",
    "{0: 'Hip-Hop', 1: 'Pop', 2: 'Folk', 3: 'Rock', 4: 'Experimental', 5: 'International', 6: 'Electronic', 7: 'Instrumental'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e985c",
   "metadata": {},
   "source": [
    "### Creation of the labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be9951a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_folder: data/fma_small_stft_transposed_22050_overlapped/train\n",
      "validation_folder: data/fma_small_stft_transposed_22050_overlapped/validation\n",
      "test_folder: data/fma_small_stft_transposed_22050_overlapped/test \n",
      "\n",
      "audio directory:  ./data/fma_small/\n",
      "Loading tracks.csv...\n",
      "small dataset shape: (8000, 52)\n",
      "Track.csv: 6400 training samples, 800 validation samples, 800 test samples\n",
      "\n",
      "there are 8 unique genres\n",
      "Dictionary of genres created: {'Hip-Hop': 0, 'Pop': 1, 'Folk': 2, 'Rock': 3, 'Experimental': 4, 'International': 5, 'Electronic': 6, 'Instrumental': 7}\n",
      "labels length: 128000\n",
      "labels length: 16000\n",
      "labels length: 16000\n"
     ]
    }
   ],
   "source": [
    "def create_single_dataset(folder_path, tracks_dataframe, genre_dictionary):    \n",
    "    labels = []\n",
    "   \n",
    "    _, file_list = get_sorted_file_paths(folder_path)\n",
    "    \n",
    "    for i,file in enumerate(file_list):\n",
    "        #print(\"considering file:\",file, \"({}/{})\".format(i,len(file_list)))\n",
    "        track_id_clip_id = file.split('.')[0]\n",
    "        track_id = track_id_clip_id.split('_')[0]\n",
    "        #print(\"track id with clip: {}, track id: {}\".format(track_id_clip_id, track_id))\n",
    "        genre = tracks_dataframe.loc[int(track_id)]\n",
    "        #print(\"genre from dataframe: \", genre)\n",
    "        label = genre_dictionary[genre]\n",
    "        #print(\"label from dictionary:\",label)\n",
    "        labels.append(label)\n",
    "    print(\"labels length: {}\".format(len(labels)))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "#create the train,validation and test vectors using the files in the train/validation/test folders\n",
    "def create_dataset_splitted(folder_path):\n",
    "    train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "    validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "    test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "    \n",
    "    print(\"train_folder:\",train_folder)\n",
    "    print(\"validation_folder:\",validation_folder)\n",
    "    print(\"test_folder:\",test_folder,\"\\n\")\n",
    "    \n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "    \n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "\n",
    "    small_training_top_genres = small_training['genre_top']\n",
    "    small_validation_top_genres = small_validation['genre_top']\n",
    "    small_test_top_genres = small_test['genre_top']\n",
    "    \n",
    "    #create dictionary of genre classes:\n",
    "    unique_genres = small_training_top_genres.unique()\n",
    "    unique_genres = np.array(unique_genres)\n",
    "    print(\"there are {} unique genres\".format(len(unique_genres)))\n",
    "    genre_dictionary = {}\n",
    "    for i,genre in enumerate(unique_genres):\n",
    "        genre_dictionary[genre] = i\n",
    "    print(\"Dictionary of genres created:\",genre_dictionary)\n",
    "    \n",
    "    \n",
    "    Y_train = create_single_dataset(train_folder, small_training_top_genres, genre_dictionary)\n",
    "    Y_validation = create_single_dataset(validation_folder, small_validation_top_genres, genre_dictionary)\n",
    "    Y_test = create_single_dataset(test_folder, small_test_top_genres, genre_dictionary)\n",
    "    \n",
    "    return Y_train, Y_validation, Y_test\n",
    " \n",
    "def get_sorted_file_paths(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #sort the dataset files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_list = Tcl().call('lsort', '-dict', file_list) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...\n",
    "    file_paths = [os.path.join(folder_path, file_name) for file_name in file_list] #join filename with folder path\n",
    "    #print(\"There are {} in the folder: {}\".format(len(file_list),file_list))\n",
    "    return file_paths, file_list\n",
    "    \n",
    "    \n",
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "Y_train, Y_validation, Y_test = create_dataset_splitted(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20175a8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187ddbe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae70891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train dataset:  128000\n",
      "len of validation dataset:  16000\n",
      "len of test dataset:  16000\n"
     ]
    }
   ],
   "source": [
    "folder_path=\"data/fma_small_stft_transposed_22050_overlapped\"\n",
    "\n",
    "train_folder = os.path.join(folder_path,'train') # concatenate train folder to path\n",
    "validation_folder = os.path.join(folder_path,'validation') # concatenate train folder to path\n",
    "test_folder = os.path.join(folder_path,'test') # concatenate train folder to path\n",
    "\n",
    "train_file_paths, _ = get_sorted_file_paths(train_folder)\n",
    "train_dataset = MyDataset(train_file_paths, Y_train)\n",
    "print(\"len of train dataset: \",len(train_dataset))\n",
    "\n",
    "validation_file_paths, _ = get_sorted_file_paths(validation_folder)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation)\n",
    "print(\"len of validation dataset: \",len(validation_file_paths))\n",
    "\n",
    "test_file_paths, _ = get_sorted_file_paths(test_folder)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test)\n",
    "print(\"len of test dataset: \",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef67f394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_for_dimension_errors(filepaths):\n",
    "    error_indexes = []\n",
    "    progress = 0\n",
    "    for file in filepaths:\n",
    "        progress+=1\n",
    "        print(\"checked {}/{} files\".format(progress,len(filepaths)))\n",
    "        x = np.load(file)\n",
    "        if(x.shape != (128,513)):\n",
    "            error_indexes.append(x)\n",
    "            print(\"error\")\n",
    "    print(\"{} errors found in files: {}\".format(len(error_indexes),error_indexes))\n",
    "    for idx,error in enumerate(error_indexes):\n",
    "        print(\"index: {}, shape: {}\".format(idx,error.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd9c68",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "We will use Z-Score to normalize the training, validation and test set by calculating the mean and the std deviation on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f571ada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_mean'\n",
    "std_save_filename = './data/fma_small_stft_transposed_22050_overlapped/train_std_deviation'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677263cf",
   "metadata": {},
   "source": [
    "## Calculation of mean and standard deviation (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5795abf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum: tensor(8.9296e+09)\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum=0\n",
    "\n",
    "#iter all the training set by batches and calculate the sum of all the sample values (513*128 values for each sample)\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum:\",current_sum)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "        current_sum += torch.sum(sample)\n",
    "        #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum:\",current_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c794bf2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of training set: tensor(1.0629)\n",
      "Saving the mean in file: ./data/fma_small_stft_transposed_22050_overlapped/train_mean\n"
     ]
    }
   ],
   "source": [
    "mean = current_sum/(len(train_dataset)*513*128) #divide the sum for the total number of values considerated\n",
    "print(\"mean of training set:\",mean)\n",
    "\n",
    "print(\"Saving the mean in file:\",save_filename) \n",
    "np.save(save_filename,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec52eb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sum of squares: tensor(7.8299e+10)\n"
     ]
    }
   ],
   "source": [
    "#now let's calculate the standard deviation (squared root of the variance)\n",
    "\n",
    "batch_size=1\n",
    "total_n_batches = len(train_dataset)/batch_size\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "current_sum_of_squares = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    #print(\"batch\",batch_idx,\"/\",total_n_batches,\"current_sum_of_squares:\",current_sum_of_squares)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    #print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        #print(\"sample shape\",sample.shape)\n",
    "        for row in sample:\n",
    "            #print(\"row shape:\",row.shape)\n",
    "            for elem in row:\n",
    "                #print(\"elem: shape\",elem.shape,\"content:\",elem)\n",
    "                difference = elem - mean\n",
    "                difference_squared = difference**2\n",
    "                current_sum_of_squares += difference_squared\n",
    "                #print(\"current_sum:\",current_sum)\n",
    "print(\"final sum of squares:\",current_sum_of_squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cecfb0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance: tensor(9.3202)\n",
      "std_deviation: 3.0528938510507846\n",
      "Saving the std_deviation in file: ./data/fma_small_stft_transposed_22050_overlapped/train_std_deviation\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "variance = current_sum_of_squares/((len(train_dataset) * 513 * 128)-1)\n",
    "std_deviation = math.sqrt(variance)\n",
    "\n",
    "print(\"variance:\",variance)\n",
    "print(\"std_deviation:\",std_deviation)\n",
    "\n",
    "print(\"Saving the std_deviation in file:\",std_save_filename) \n",
    "np.save(std_save_filename,std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fc2ab",
   "metadata": {},
   "source": [
    "## Load calculated mean and std deviation from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977c1ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean: 1.0629134\n",
      "loaded std: 3.0528938510507846\n"
     ]
    }
   ],
   "source": [
    "loaded_mean = np.load(save_filename+'.npy')\n",
    "print(\"loaded mean:\",loaded_mean)\n",
    "\n",
    "loaded_std = np.load(std_save_filename+'.npy')\n",
    "print(\"loaded std:\",loaded_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d4804",
   "metadata": {},
   "source": [
    "## Create the normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092efbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#verify that the normalization has worked\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\\ntotal_n_batches = len(train_loader)/batch_size\\n\\n#iter all the training set by batches and normalize the value of each tensor\\nfor batch_idx, batch in enumerate(train_loader):\\n    print(\"batch\",batch_idx,\"/\",total_n_batches)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\\n    print(\"labels:\",labels)\\n    for sample in inputs:\\n        print(\"sample: shape\",sample.shape,\"content:\",sample)\\n   \\nvalidation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\\ntotal_n_batches = len(validation_loader)/batch_size\\n\\n#iter all the training set by batches and normalize the value of each tensor\\nfor batch_idx, batch in enumerate(validation_loader):\\n    print(\"batch\",batch_idx,\"/\",total_n_batches)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\\n    print(\"labels:\",labels)\\n    for sample in inputs:\\n        print(\"sample: shape\",sample.shape,\"content:\",sample)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean= loaded_mean, std= loaded_std)\n",
    "])\n",
    "\n",
    "train_dataset = MyDataset(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDataset(test_file_paths, Y_test,  transform = transform)\n",
    "\n",
    "'''\n",
    "#verify that the normalization has worked\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "total_n_batches = len(train_loader)/batch_size\n",
    "\n",
    "#iter all the training set by batches and normalize the value of each tensor\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(\"batch\",batch_idx,\"/\",total_n_batches)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "   \n",
    "validation_dataset = MyDataset(validation_file_paths, Y_validation,  transform = transform)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\n",
    "total_n_batches = len(validation_loader)/batch_size\n",
    "\n",
    "#iter all the training set by batches and normalize the value of each tensor\n",
    "for batch_idx, batch in enumerate(validation_loader):\n",
    "    print(\"batch\",batch_idx,\"/\",total_n_batches)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    print(\"inputs: shape:\",inputs.shape,\"content:\",inputs)\n",
    "    print(\"labels:\",labels)\n",
    "    for sample in inputs:\n",
    "        print(\"sample: shape\",sample.shape,\"content:\",sample)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a52dc",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f745b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(4, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=(4, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(512, 300)\n",
    "        self.bn4 = nn.BatchNorm1d(300)\n",
    "        self.dense2 = nn.Linear(300,150)\n",
    "        self.bn5 = nn.BatchNorm1d(150)\n",
    "        self.dense3 = nn.Linear(150, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845a53e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d034d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EPOCHS=10\n",
    "LEARNING_RATE=0.0001\n",
    "\n",
    "learning_rate_list = [0.01, 0.001, 0.0001]\n",
    "batch_size_list = [128,256,512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd576a3",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "223742e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model, validation_dataset, Y_validation):\n",
    "    # Stop parameters learning\n",
    "    model.eval()\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confusion_matrix = np.zeros((8, 8), dtype=int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample, label in validation_loader:\n",
    "            \n",
    "            sample = sample.unsqueeze(1)\n",
    "\n",
    "            # Predict label\n",
    "            output = model(sample)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            max_index = torch.argmax(output).item()  # The index with maximum probability\n",
    "\n",
    "            confusion_matrix[label][max_index] += 1\n",
    "\n",
    "            correct += (max_index == label)\n",
    "\n",
    "    cm = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "    cm.plot()\n",
    "    print(confusion_matrix)\n",
    "    accuracy = 100 * correct / len(Y_validation)\n",
    "    average_loss = total_loss / len(Y_validation)\n",
    "\n",
    "    model.train()\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a0ca4e98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, num_epochs, learning_rate, verbose = False, RGB=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    counted_labels=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise ValueError(\"The dataset parameter should be an instance of torch.utils.data.Dataset.\")\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        running_accuracy = 0.0\n",
    "        #initialize correctly predicted samples\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tq.tqdm(total=num_batches, unit=\"batch\")\n",
    "    \n",
    "        # Initialize the progress bar description\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            correct = 0 # reset train accuracy each batch\n",
    "            \n",
    "            inputs,labels = batch[0],batch[1]\n",
    "            if(verbose == True):\n",
    "                print(\"\\ninputs shape:\",inputs.size(),\", dtype:\",inputs.dtype,\" content: \",inputs)\n",
    "                print(\"min value:\",torch.min(inputs))\n",
    "                print(\"max value:\",torch.max(inputs))\n",
    "                print(\"\\nlabels shape:\",labels.size(),\",dtype:\",labels.dtype,\", content: \",labels)\n",
    "            if(RGB==False):\n",
    "                inputs = inputs.unsqueeze(1) #add a dimension if input is to be considered just grayscale\n",
    "                #if input is RGB, there are already 3 channels\n",
    "            \n",
    "            # Extract the inputs and targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if(verbose == True):\n",
    "                print(\"\\noutputs size:\",outputs.size(),\"content:\",outputs)\n",
    "                print(\"List of labels until now:\",counted_labels)\n",
    "\n",
    "            loss = criterion(outputs, labels) #labels need to be a vector of class indexes (0-7) of dim (batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate train accuracy\n",
    "            for index, output in enumerate(outputs):\n",
    "                max_index = torch.argmax(output).item() #the index with maximum probability\n",
    "                counted_labels[labels[index].item()]+=1\n",
    "                if(labels[index].item() == max_index):\n",
    "                    correct += 1\n",
    "            \n",
    "                if(verbose==True):\n",
    "                    print(\"considering output at index {}:\".format(index,output))\n",
    "                    print(\"max output index = {}\",max_index)\n",
    "                    if(labels[index].item() == max_index):\n",
    "                        print(\"correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "                    else:\n",
    "                        print(\"NOT correct! in fact labels[index] = {}, max_index = {}\".format(labels[index].item(),max_index))\n",
    "\n",
    "            \n",
    "            accuracy = 100 * correct / batch_size\n",
    "            running_accuracy += accuracy #epoch running_accuracy\n",
    "            \n",
    "            # Update the progress bar description and calculate bps\n",
    "            #progress_bar.set_postfix({\"Loss\": running_loss / (batch_idx + 1)})\n",
    "            average_accuracy = running_accuracy / (batch_idx + 1)\n",
    "            average_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": average_loss, \"acc\": accuracy, \"avg_acc\": average_accuracy})\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "            # Evaluate the model on the validation dataset\n",
    "        \n",
    "        #calculate train loss and accuracy\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        average_accuracy = running_accuracy / len(data_loader)\n",
    "        train_loss_list.append(average_loss)\n",
    "        train_acc_list.append(average_accuracy)\n",
    "        \n",
    "        #calculate validation loss and accuracy\n",
    "        val_acc, val_loss = test(model, validation_dataset, Y_validation)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],Train Loss: {average_loss:.4f}. Train Accuracy: {average_accuracy} Val Loss: {val_loss} Val Accuracy: {val_acc}\")\n",
    "        progress_bar.close()\n",
    "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbbc7adc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NNet1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d6742b0e5792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rate_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_size_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNet1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#reinitialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_lr_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'_bs_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Traing model with batch size={bs}, lr={lr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NNet1' is not defined"
     ]
    }
   ],
   "source": [
    "#summary(model, (1, 128, 513))\n",
    "performance_list = []\n",
    "model_list = []\n",
    "directory = 'models/'\n",
    "\n",
    "for lr in learning_rate_list:   \n",
    "    for bs in batch_size_list:\n",
    "        model = NNet1() #reinitialize model\n",
    "        file_path = directory + 'model' + '_lr_' + str(lr).split('.')[0] + '-' + str(lr).split('.')[1]+ '_bs_' + str(bs)\n",
    "        print(f\"Traing model with batch size={bs}, lr={lr}\")\n",
    "        performance=train(model, train_dataset, batch_size=bs, num_epochs=EPOCHS, learning_rate=lr, verbose=True)\n",
    "        performance_list.append(performance)\n",
    "        model_list.append(model)\n",
    "        print(\"saving model in\", file_path)\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "        print(performance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58d928b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plt.plot(np.arange(1,EPOCHS+1), performance[1][:], label='Loss') \n",
    "plt.plot(np.arange(1,EPOCHS+1), performance[0][:], label='Accuracy')\n",
    "plt.legend()  # Display the legend showing the labels\n",
    "plt.show()\n",
    "print(performance[0][:])\n",
    "'''\n",
    "\n",
    "parameter_list = []\n",
    "for lr in learning_rate_list:\n",
    "    for bs in batch_size_list:\n",
    "        parameter_list.append([lr,bs])\n",
    "\n",
    "for performance, parameters in zip(performance_list, parameter_list):\n",
    "    plt.plot(np.arange(1,EPOCHS+1), performance[1][:], label='Val Loss') \n",
    "    plt.plot(np.arange(1,EPOCHS+1), performance[0][:], label='Val Accuracy')\n",
    "    plt.title(f\"Lr = {parameters[0]}, Batch size = {parameters[1]}\")\n",
    "    plt.legend()  # Display the legend showing the labels\n",
    "    plt.show()\n",
    "    print(performance[0][:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef7fab",
   "metadata": {},
   "source": [
    "# Network Architecture Definition (nnet1 + BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3685ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet1_Small(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNet1_Small, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(2, 513))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(2, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(4, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=(2, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(3, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(3, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.dense2 = nn.Linear(128,32)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.dense3 = nn.Linear(32, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_avg = self.avgpool(x)\n",
    "        x_max = self.maxpool(x)\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61b169d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 127, 1]          32,864\n",
      "       BatchNorm2d-2           [-1, 32, 127, 1]              64\n",
      "              ReLU-3           [-1, 32, 127, 1]               0\n",
      "         MaxPool2d-4            [-1, 32, 31, 1]               0\n",
      "            Conv2d-5            [-1, 64, 30, 1]           4,160\n",
      "       BatchNorm2d-6            [-1, 64, 30, 1]             128\n",
      "              ReLU-7            [-1, 64, 30, 1]               0\n",
      "         MaxPool2d-8             [-1, 64, 7, 1]               0\n",
      "            Conv2d-9             [-1, 64, 6, 1]           8,256\n",
      "      BatchNorm2d-10             [-1, 64, 6, 1]             128\n",
      "        AvgPool2d-11             [-1, 64, 2, 1]               0\n",
      "        MaxPool2d-12             [-1, 64, 2, 1]               0\n",
      "          Flatten-13                  [-1, 256]               0\n",
      "           Linear-14                  [-1, 128]          32,896\n",
      "          Dropout-15                  [-1, 128]               0\n",
      "      BatchNorm1d-16                  [-1, 128]             256\n",
      "             ReLU-17                  [-1, 128]               0\n",
      "           Linear-18                   [-1, 32]           4,128\n",
      "          Dropout-19                   [-1, 32]               0\n",
      "      BatchNorm1d-20                   [-1, 32]              64\n",
      "             ReLU-21                   [-1, 32]               0\n",
      "           Linear-22                    [-1, 8]             264\n",
      "          Softmax-23                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 83,208\n",
      "Trainable params: 83,208\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 0.16\n",
      "Params size (MB): 0.32\n",
      "Estimated Total Size (MB): 0.73\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_NNet1_Small = NNet1_Small()\n",
    "summary(model_NNet1_Small, (1, 128, 513))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac93d69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222c0ac3ef5e47a3840ea8647ec2f1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-99339914b4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_NNet1_Small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-897426d8a859>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, batch_size, num_epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#labels need to be a vector of class indexes (0-7) of dim (batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fma3/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fma3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model_NNet1_Small, train_dataset, batch_size=128, num_epochs=20, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343371e",
   "metadata": {},
   "source": [
    "# Raw audio (1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabb348",
   "metadata": {},
   "source": [
    "## Raw audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30092ba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset of way audio\n",
    "#class MyDatasetRaw(Dataset):\n",
    "    #def __init__(self, file_list, labels):\n",
    "        #self.file_list = file_list\n",
    "        #self.labels=labels\n",
    "\n",
    "    #def __len__(self):\n",
    "        #return len(self.file_list)\n",
    "\n",
    "    #def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        #file_path = self.file_list[idx]\n",
    "        #label = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        #print(\"opening file:\",file_path)\n",
    "        #audio_file = AudioSegment.from_file(file_path)\n",
    "\n",
    "        #data = audio_file._data\n",
    "        #pcm16_signed_integers = []\n",
    "\n",
    "        # This loop decodes the bytestring into PCM samples.\n",
    "        # The bytestring is a stream of little-endian encoded signed integers.\n",
    "        # This basically just cuts each two-byte sample out of the bytestring, converts\n",
    "        # it to an integer, and appends it to the list of samples.\n",
    "        #for sample_index in range(len(data)//2):\n",
    "            #sample = int.from_bytes(data[sample_index*2:sample_index*2+2], 'big', signed=True)\n",
    "            #pcm16_signed_integers.append(sample)\n",
    "\n",
    "\n",
    "        #load audio in ram\n",
    "        #x = np.load(file_path,allow_pickle=True) #load the MONO audio file from the data/fma_small directory\n",
    "        #x=torch.tensor(pcm16_signed_integers)\n",
    "        #x = x.type(torch.FloatTensor)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Load the MP3 file\n",
    "        #audio = AudioSegment.from_mp3(file_path)\n",
    "\n",
    "        # Set the desired new sampling rate\n",
    "        #new_sampling_rate = 8000  # for example, 16000 Hz\n",
    "\n",
    "        # Resample the audio to the new sampling rate\n",
    "        #resampled_audio = audio.set_frame_rate(new_sampling_rate)\n",
    "\n",
    "        # Convert the resampled audio to a raw PCM array\n",
    "        #resampled_pcm_audio = resampled_audio.raw_data\n",
    "        #resampled_pcm_array = np.frombuffer(resampled_pcm_audio, dtype=np.int16)\n",
    "        \n",
    "        #print(\"Versione 2:\",resampled_pcm_array.size,\"\\tVersione 1:\",x.shape)\n",
    "        #x=torch.tensor(resampled_pcm_array)\n",
    "        #x = x.type(torch.FloatTensor)\n",
    "        #Ã return x, label\n",
    "    \n",
    "# Define the custom class for accessing our dataset\n",
    "class MyDatasetRaw(Dataset):\n",
    "    def __init__(self, file_list, labels):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        raw_vector = np.load(file_path).astype(np.int16)  # Ensure int16 data type\n",
    "        raw_vector = torch.tensor(raw_vector)\n",
    "        return raw_vector[:200000], label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c527e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#returns file paths list for train, validation and test\n",
    "def get_file_paths():\n",
    "    folder_raw = './data/fma_small'\n",
    "    file_paths_raw = []\n",
    "\n",
    "    AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    print(\"audio directory: \",AUDIO_DIR)\n",
    "    print(\"Loading tracks.csv...\")\n",
    "    tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "    #get only the small subset of the dataset\n",
    "    small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "    print(\"small dataset shape:\",small.shape)    \n",
    "\n",
    "    small_training = small.loc[small[('set', 'split')] == 'training']['track']\n",
    "    small_validation = small.loc[small[('set', 'split')] == 'validation']['track']\n",
    "    small_test = small.loc[small[('set', 'split')] == 'test']['track']\n",
    "\n",
    "    print(\"Track.csv: {} training samples, {} validation samples, {} test samples\\n\".format(len(small_training), len(small_validation), len(small_test)))\n",
    "    \n",
    "    #--------------TRAIN----------------------\n",
    "    print(\"Creating train dataset...\")\n",
    "    file_paths_train = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_train = np.array(small_training.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_train)} samples. Here's the first ones: {track_ids_train[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_train:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_train.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_train = np.array(Tcl().call('lsort', '-dict', file_paths_train)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    #--------------VALIDATION----------------------\n",
    "    print(\"Creating validation dataset...\")\n",
    "    file_paths_validation = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_validation = np.array(small_validation.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_validation)} samples. Here's the first ones: {track_ids_validation[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_validation:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_validation.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_validation = np.array(Tcl().call('lsort', '-dict', file_paths_validation)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    #--------------TEST----------------------\n",
    "    print(\"Creating test dataset...\")\n",
    "    file_paths_test = []\n",
    "\n",
    "    #we have to get train track_ids from dataframe\n",
    "    track_ids_test = np.array(small_test.index) #get indexes and convert to numpy array\n",
    "    print(f\"There are {len(track_ids_test)} samples. Here's the first ones: {track_ids_test[:10]}\")\n",
    "\n",
    "    for track_id in track_ids_test:\n",
    "        file_path = utils.get_audio_path(AUDIO_DIR,track_id)\n",
    "        file_paths_test.append(file_path)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_test = np.array(Tcl().call('lsort', '-dict', file_paths_test)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    \n",
    "    #create the datasets\n",
    "    \n",
    "    return file_paths_train, file_paths_validation, file_paths_test\n",
    "\n",
    "#returns file paths list for train, validation and test for each 3s clip\n",
    "def get_file_paths_split():\n",
    "    folder_raw = './data/fma_small_raw_array_22050_overlapped'\n",
    "    folder_train = folder_raw + '/train'\n",
    "    folder_validation = folder_raw + '/validation'\n",
    "    folder_test = folder_raw + '/test'\n",
    "    file_paths_raw = []\n",
    "\n",
    "    #--------------TRAIN----------------------\n",
    "    print(\"Creating train dataset...\")\n",
    "    file_paths_train = os.listdir(folder_train)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_train = np.array(Tcl().call('lsort', '-dict', file_paths_train)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_train = []\n",
    "    for file in file_paths_train:\n",
    "        complete_file_path_train.append(folder_train + '/' + file)\n",
    "\n",
    "    #--------------VALIDATION----------------------\n",
    "    print(\"Creating validation dataset...\")\n",
    "    file_paths_validation = os.listdir(folder_validation)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_validation = np.array(Tcl().call('lsort', '-dict', file_paths_validation)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_validation = []\n",
    "    for file in file_paths_validation:\n",
    "        complete_file_path_validation.append(folder_validation + '/' + file)\n",
    "\n",
    "    #--------------TEST----------------------\n",
    "    print(\"Creating test dataset...\")\n",
    "    file_paths_test = os.listdir(folder_test)\n",
    "    #sort the files in alphabetical order (important to associate correct labels created using track_id in track.csv)\n",
    "    file_paths_test = np.array(Tcl().call('lsort', '-dict', file_paths_test)) # sort file by name: 2_0,2_1, ... 2_9,3_0, ... 400_0,400_1, ...    \n",
    "    \n",
    "    complete_file_path_test = []\n",
    "    for file in file_paths_test:\n",
    "        complete_file_path_test.append(folder_test + '/' + file)\n",
    "    \n",
    "    return complete_file_path_train, complete_file_path_validation, complete_file_path_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99825d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset...\n",
      "Creating validation dataset...\n",
      "Creating test dataset...\n",
      "128000\n",
      "16000\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "file_paths_train, file_paths_validation, file_paths_test = get_file_paths_split()\n",
    "print(len(file_paths_train))\n",
    "print(len(file_paths_validation))\n",
    "print(len(file_paths_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891ab30",
   "metadata": {},
   "source": [
    "## Fix wrong sampling rates in the dataset\n",
    "\n",
    "Some songs in the fma_small dataset has a different sampling rate from 44100. Let's fix them by using librosa resampling tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f82d99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "#fix sampling rate\n",
    "target_sr = 44100\n",
    "wrong_sr_list = []\n",
    "\n",
    "\"\"\"\n",
    "print(\"\\n******Checking sample rates to be\",target_sr,\"in train set\")\n",
    "for file in file_paths_train:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "   \"\"\"     \n",
    "print(\"\\n*****Checking sample rates to be\",target_sr,\"in validation set\")\n",
    "for file in file_paths_validation:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "\n",
    "        \n",
    "print(\"\\n*****Checking sample rates to be\",target_sr,\"in test set\")\n",
    "for file in file_paths_test:\n",
    "    x, sr = librosa.load(file,sr=None)\n",
    "    if(sr!=target_sr):\n",
    "        print(\"wrong sr found! value:\",sr,\"resampling to\",target_sr,\"...\")\n",
    "        x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)\n",
    "        print(\"saving new resampled file in\",file,\"...\")\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(file, x, target_sr, format='mp3')    \n",
    "        wrong_sr_list.append(file)\n",
    "        \n",
    "print(\"There were a total of\",len(wrong_sr_list),\"files with wrong sampling rate. Now they have been corrected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029105b",
   "metadata": {},
   "source": [
    "## Create labels for raw audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ea1db",
   "metadata": {},
   "source": [
    "We have to take one label each ten from Y_label. Now we are not using 10 clips for each audio but just the whole audio itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7112c8",
   "metadata": {},
   "source": [
    "## Create dataset for raw audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45f5f209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDatasetRaw(file_paths_train, Y_train)\n",
    "validation_dataset = MyDatasetRaw(file_paths_validation, Y_validation)\n",
    "test_dataset = MyDatasetRaw(file_paths_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68b4a333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NNet_Raw3(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(NNet_Raw3, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=8, stride=16)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=8, stride=8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128) \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(24960, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c90b296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 32, 12500]             288\n",
      "              ReLU-2            [-1, 32, 12500]               0\n",
      "         MaxPool1d-3             [-1, 32, 6250]               0\n",
      "       BatchNorm1d-4             [-1, 32, 6250]              64\n",
      "           Dropout-5             [-1, 32, 6250]               0\n",
      "            Conv1d-6              [-1, 64, 781]          16,448\n",
      "              ReLU-7              [-1, 64, 781]               0\n",
      "         MaxPool1d-8              [-1, 64, 390]               0\n",
      "       BatchNorm1d-9              [-1, 64, 390]             128\n",
      "          Dropout-10              [-1, 64, 390]               0\n",
      "           Linear-11                  [-1, 128]       3,195,008\n",
      "             ReLU-12                  [-1, 128]               0\n",
      "      BatchNorm1d-13                  [-1, 128]             256\n",
      "          Dropout-14                  [-1, 128]               0\n",
      "           Linear-15                   [-1, 32]           4,128\n",
      "             ReLU-16                   [-1, 32]               0\n",
      "      BatchNorm1d-17                   [-1, 32]              64\n",
      "          Dropout-18                   [-1, 32]               0\n",
      "           Linear-19                    [-1, 8]             264\n",
      "================================================================\n",
      "Total params: 3,216,648\n",
      "Trainable params: 3,216,648\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.76\n",
      "Forward/backward pass size (MB): 12.02\n",
      "Params size (MB): 12.27\n",
      "Estimated Total Size (MB): 25.05\n",
      "----------------------------------------------------------------\n",
      "NNet_Raw3(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(8,), stride=(16,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(8,), stride=(8,))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (batchnorm1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=24960, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MyModel=NNet_Raw3()\n",
    "summary(MyModel, (1,200000))\n",
    "print(MyModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f0168",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train_raw(MyModel, train_dataset, batch_size=128, num_epochs=10, learning_rate=0.1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08cc6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "641166af",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# NN on echonest featuresÂ \n",
    "\n",
    "As you can see below, unfortunately we cannot perform a train using echonest features (danceability, tempo, acousticness, ...) because there are only âº1300 tracks with echonest features which are not NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25e87d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening csvs...\n",
      "small dataset shape: (8000, 52)\n",
      "echonest csv shape (only audio features): (13129, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"opening csvs...\")\n",
    "\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "echonest = utils.load('data/fma_metadata/echonest.csv')\n",
    "echonest = echonest['echonest', 'audio_features']\n",
    "small = tracks[tracks['set', 'subset'] <= 'small']\n",
    "\n",
    "print(\"small dataset shape:\",small.shape)\n",
    "print(\"echonest csv shape (only audio features):\",echonest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8b18347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track ids shape: 8000 content: [2, 5, 10, 140, 141, 148, 182, 190, 193, 194] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.416675</td>\n",
       "      <td>0.675894</td>\n",
       "      <td>0.634476</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.177647</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>165.922</td>\n",
       "      <td>0.576661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.745566</td>\n",
       "      <td>0.701470</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.124595</td>\n",
       "      <td>100.260</td>\n",
       "      <td>0.621661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.951670</td>\n",
       "      <td>0.658179</td>\n",
       "      <td>0.924525</td>\n",
       "      <td>0.965427</td>\n",
       "      <td>0.115474</td>\n",
       "      <td>0.032985</td>\n",
       "      <td>111.562</td>\n",
       "      <td>0.963590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.376312</td>\n",
       "      <td>0.734079</td>\n",
       "      <td>0.265685</td>\n",
       "      <td>0.669581</td>\n",
       "      <td>0.085995</td>\n",
       "      <td>0.039068</td>\n",
       "      <td>107.952</td>\n",
       "      <td>0.609991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.963657</td>\n",
       "      <td>0.435933</td>\n",
       "      <td>0.075632</td>\n",
       "      <td>0.345493</td>\n",
       "      <td>0.105686</td>\n",
       "      <td>0.026658</td>\n",
       "      <td>33.477</td>\n",
       "      <td>0.163950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154413</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154414</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155066</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        acousticness  danceability    energy  instrumentalness  liveness  \\\n",
       "2           0.416675      0.675894  0.634476          0.010628  0.177647   \n",
       "5           0.043567      0.745566  0.701470          0.000697  0.373143   \n",
       "10          0.951670      0.658179  0.924525          0.965427  0.115474   \n",
       "140         0.376312      0.734079  0.265685          0.669581  0.085995   \n",
       "141         0.963657      0.435933  0.075632          0.345493  0.105686   \n",
       "...              ...           ...       ...               ...       ...   \n",
       "154308           NaN           NaN       NaN               NaN       NaN   \n",
       "154309           NaN           NaN       NaN               NaN       NaN   \n",
       "154413           NaN           NaN       NaN               NaN       NaN   \n",
       "154414           NaN           NaN       NaN               NaN       NaN   \n",
       "155066           NaN           NaN       NaN               NaN       NaN   \n",
       "\n",
       "        speechiness    tempo   valence  \n",
       "2          0.159310  165.922  0.576661  \n",
       "5          0.124595  100.260  0.621661  \n",
       "10         0.032985  111.562  0.963590  \n",
       "140        0.039068  107.952  0.609991  \n",
       "141        0.026658   33.477  0.163950  \n",
       "...             ...      ...       ...  \n",
       "154308          NaN      NaN       NaN  \n",
       "154309          NaN      NaN       NaN  \n",
       "154413          NaN      NaN       NaN  \n",
       "154414          NaN      NaN       NaN  \n",
       "155066          NaN      NaN       NaN  \n",
       "\n",
       "[8000 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select small dataset from echonest csv\n",
    "\n",
    "track_ids = small.index.values.tolist()\n",
    "print(\"Track ids shape:\",len(track_ids),\"content:\",track_ids[:10],\"...\")\n",
    "echonest_small = pd.DataFrame(echonest,index=track_ids)\n",
    "\n",
    "ipd.display(echonest_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12714b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_echonest: shape: (8000, 8)\n",
      "There are 6706 rows containing NaN only as data\n"
     ]
    }
   ],
   "source": [
    "X_train_echonest = echonest_small.to_numpy(dtype=np.float16)\n",
    "print(\"X_train_echonest: shape:\",X_train_echonest.shape)\n",
    "\n",
    "nan_rows = np.argwhere(np.isnan(X_train_echonest).all(axis=1))\n",
    "\n",
    "print(\"There are\",len(nan_rows),\"rows containing NaN only as data\")\n",
    "#nan_rows = nan_rows.squeeze()\n",
    "#for i in nan_rows:\n",
    "#    print(\"row:\",i,\":\",X_train_echonest[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917bc8e",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "We try to use transfer learning using a ResNet18 by torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "497e31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/puzzerella/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 64, 64, 257]           9,408\n",
      "       BatchNorm2d-2          [-1, 64, 64, 257]             128\n",
      "              ReLU-3          [-1, 64, 64, 257]               0\n",
      "         MaxPool2d-4          [-1, 64, 32, 129]               0\n",
      "            Conv2d-5          [-1, 64, 32, 129]          36,864\n",
      "       BatchNorm2d-6          [-1, 64, 32, 129]             128\n",
      "              ReLU-7          [-1, 64, 32, 129]               0\n",
      "            Conv2d-8          [-1, 64, 32, 129]          36,864\n",
      "       BatchNorm2d-9          [-1, 64, 32, 129]             128\n",
      "             ReLU-10          [-1, 64, 32, 129]               0\n",
      "       BasicBlock-11          [-1, 64, 32, 129]               0\n",
      "           Conv2d-12          [-1, 64, 32, 129]          36,864\n",
      "      BatchNorm2d-13          [-1, 64, 32, 129]             128\n",
      "             ReLU-14          [-1, 64, 32, 129]               0\n",
      "           Conv2d-15          [-1, 64, 32, 129]          36,864\n",
      "      BatchNorm2d-16          [-1, 64, 32, 129]             128\n",
      "             ReLU-17          [-1, 64, 32, 129]               0\n",
      "       BasicBlock-18          [-1, 64, 32, 129]               0\n",
      "           Conv2d-19          [-1, 128, 16, 65]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 16, 65]             256\n",
      "             ReLU-21          [-1, 128, 16, 65]               0\n",
      "           Conv2d-22          [-1, 128, 16, 65]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 16, 65]             256\n",
      "           Conv2d-24          [-1, 128, 16, 65]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 16, 65]             256\n",
      "             ReLU-26          [-1, 128, 16, 65]               0\n",
      "       BasicBlock-27          [-1, 128, 16, 65]               0\n",
      "           Conv2d-28          [-1, 128, 16, 65]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 16, 65]             256\n",
      "             ReLU-30          [-1, 128, 16, 65]               0\n",
      "           Conv2d-31          [-1, 128, 16, 65]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 16, 65]             256\n",
      "             ReLU-33          [-1, 128, 16, 65]               0\n",
      "       BasicBlock-34          [-1, 128, 16, 65]               0\n",
      "           Conv2d-35           [-1, 256, 8, 33]         294,912\n",
      "      BatchNorm2d-36           [-1, 256, 8, 33]             512\n",
      "             ReLU-37           [-1, 256, 8, 33]               0\n",
      "           Conv2d-38           [-1, 256, 8, 33]         589,824\n",
      "      BatchNorm2d-39           [-1, 256, 8, 33]             512\n",
      "           Conv2d-40           [-1, 256, 8, 33]          32,768\n",
      "      BatchNorm2d-41           [-1, 256, 8, 33]             512\n",
      "             ReLU-42           [-1, 256, 8, 33]               0\n",
      "       BasicBlock-43           [-1, 256, 8, 33]               0\n",
      "           Conv2d-44           [-1, 256, 8, 33]         589,824\n",
      "      BatchNorm2d-45           [-1, 256, 8, 33]             512\n",
      "             ReLU-46           [-1, 256, 8, 33]               0\n",
      "           Conv2d-47           [-1, 256, 8, 33]         589,824\n",
      "      BatchNorm2d-48           [-1, 256, 8, 33]             512\n",
      "             ReLU-49           [-1, 256, 8, 33]               0\n",
      "       BasicBlock-50           [-1, 256, 8, 33]               0\n",
      "           Conv2d-51           [-1, 512, 4, 17]       1,179,648\n",
      "      BatchNorm2d-52           [-1, 512, 4, 17]           1,024\n",
      "             ReLU-53           [-1, 512, 4, 17]               0\n",
      "           Conv2d-54           [-1, 512, 4, 17]       2,359,296\n",
      "      BatchNorm2d-55           [-1, 512, 4, 17]           1,024\n",
      "           Conv2d-56           [-1, 512, 4, 17]         131,072\n",
      "      BatchNorm2d-57           [-1, 512, 4, 17]           1,024\n",
      "             ReLU-58           [-1, 512, 4, 17]               0\n",
      "       BasicBlock-59           [-1, 512, 4, 17]               0\n",
      "           Conv2d-60           [-1, 512, 4, 17]       2,359,296\n",
      "      BatchNorm2d-61           [-1, 512, 4, 17]           1,024\n",
      "             ReLU-62           [-1, 512, 4, 17]               0\n",
      "           Conv2d-63           [-1, 512, 4, 17]       2,359,296\n",
      "      BatchNorm2d-64           [-1, 512, 4, 17]           1,024\n",
      "             ReLU-65           [-1, 512, 4, 17]               0\n",
      "       BasicBlock-66           [-1, 512, 4, 17]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 83.09\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 128.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_ResNet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "summary(model, (3,128,513))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b4b15",
   "metadata": {},
   "source": [
    "## RGB Dataset\n",
    "Resnet18 expects RGB input images, so our dataset need to be converted from a one to a three-channel. We'll do dat by copying three times the same image in the three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5e94008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom class for accessing our dataset\n",
    "class MyDatasetRGB(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, verbose = False):\n",
    "        self.file_list = file_list\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns a training sample and its label\n",
    "        file_path = self.file_list[idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        stft_vector = torch.tensor(np.load(file_path)) #load from file\n",
    "        \n",
    "        # Normalize your data here\n",
    "        if self.transform:\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: applying transform to tensor shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = self.transform(torch.unsqueeze(stft_vector, dim=0)) #unsqueeze needed for the torchvision normalize method\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after transform shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "            stft_vector = torch.squeeze(stft_vector, dim=0)\n",
    "            if(self.verbose==True):\n",
    "                print(\"TRANSFORM: after squeeze shape:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "                \n",
    "        #do ResNet18 normalization:\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        #copy the channel 3 times (need to unsqueeze to create a new dimension first)\n",
    "        #print(\"DATASET*  sample shape is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        stft_vector = stft_vector.unsqueeze(0).repeat(3,1,1)\n",
    "        stft_vector = stft_vector.to(torch.float32) #float32 needed for ResNet18 model (downcast from float64)\n",
    "        #print(\"DATASET* sample shape after repeat is:\",stft_vector.shape,\"content:\",stft_vector)\n",
    "        #print(\"stft_vector dtype:\",stft_vector.dtype)\n",
    "\n",
    "        \n",
    "        return stft_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c24605b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor batch_idx, batch  in enumerate(train_data_loader):\\n    print (\"batch index:\",batch_idx)\\n    inputs = batch[0]\\n    labels = batch[1]\\n    \\n    for idx, sample in enumerate(inputs):\\n        label = labels[idx]\\n        print(\"inputs: shape:\",inputs.shape)\\n        print(\"sample: shape:\",sample.shape)\\n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform_ResNet18 = transforms.Compose([\n",
    "    #transforms.Normalize(mean= loaded_mean, std= loaded_std) #our values\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #ResNet18 specific values\n",
    "])\n",
    "\n",
    "train_dataset = MyDatasetRGB(train_file_paths, Y_train,  transform = transform)\n",
    "validation_dataset = MyDatasetRGB(validation_file_paths, Y_validation,  transform = transform)\n",
    "test_dataset = MyDatasetRGB(test_file_paths, Y_test,  transform = transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size = 10, shuffle=True)\n",
    "\n",
    "'''\n",
    "for batch_idx, batch  in enumerate(train_data_loader):\n",
    "    print (\"batch index:\",batch_idx)\n",
    "    inputs = batch[0]\n",
    "    labels = batch[1]\n",
    "    \n",
    "    for idx, sample in enumerate(inputs):\n",
    "        label = labels[idx]\n",
    "        print(\"inputs: shape:\",inputs.shape)\n",
    "        print(\"sample: shape:\",sample.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3747da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2061487a950a40959a7c57a195328607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "#todo: convert input tensor from float64 to double\n",
    "\n",
    "#todo normalize again using resnet18 suggested mean and std\n",
    "#TODO: FIX TEST FUNCTION USING RGB (maybe already works)\n",
    "\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list =train(model_ResNet18, train_dataset, batch_size=32, num_epochs=20, learning_rate=0.001,verbose=False, RGB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0f8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
